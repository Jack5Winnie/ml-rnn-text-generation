SENTENCE_START Generating Text with Recurrent Neural Networks Ilya Sutskever ILYA@CS UTORONTO CA James Martens JMARTENS@CS TORONTO EDU Geoffrey Hinton HINTON@CS TORONTO EDU University of Toronto, 6 King's College Rd , Toronto, ON M5S 3G4 CANADA Abstract Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly  SENTENCE_END
SENTENCE_START Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems  SENTENCE_END
SENTENCE_START In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks  SENTENCE_END
SENTENCE_START The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or "gated") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next  SENTENCE_END
SENTENCE_START After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling  a hierarchical non-parametric sequence model  SENTENCE_END
SENTENCE_START To our knowledge this represents the largest recurrent neural network application to date  SENTENCE_END
SENTENCE_START Introduction Recurrent Neural Networks (RNNs) form an expressive model family for sequence tasks  SENTENCE_END
SENTENCE_START They are powerful because they have a high-dimensional hidden state with non-linear dynamics that enable them to remember and process past information  SENTENCE_END
SENTENCE_START Furthermore, the gradients of the RNN are cheap to compute with backpropagation through time  SENTENCE_END
SENTENCE_START Despite their attractive qualities, RNNs failed to become a Appearing in Proceedings of the 28th International Conference on Machine Learning, Bellevue, WA, USA, 2011  SENTENCE_END
SENTENCE_START Copyright 2011 by the author(s)/owner(s)  SENTENCE_END
SENTENCE_START mainstream tool in machine learning due to the difficulty of training them effectively  SENTENCE_END
SENTENCE_START The cause of this difficulty is the very unstable relationship between the parameters and the dynamics of the hidden states, which manifests itself in the "vanishing/exploding gradients problem" (Bengio et al , 1994)  SENTENCE_END
SENTENCE_START As a result, there has been surprisingly little research on standard RNNs in the last 20 years, and only a few successful applications using large RNNs (Robinson, 2002; Pollastri et al , 2002), including a recent notable application of RNNs as a word-level language model (Mikolov et al , 2010)  SENTENCE_END
SENTENCE_START Recently, Martens (2010) developed a greatly improved variant of Hessian-Free optimization (HF) which was powerful enough to train very deep neural networks from random initializations  SENTENCE_END
SENTENCE_START Since an RNN can be viewed as an extremely deep neural network with weight sharing across time, the same HF optimizer should be able to train RNNs  SENTENCE_END
SENTENCE_START Fortunately, Martens & Sutskever (2011) were able to show that this is indeed the case, and that this form of non-diagonal, 2nd-order optimization provides a principled solution to the vanishing gradients problem in RNNs  SENTENCE_END
SENTENCE_START Moreover, with the addition of a novel damping mechanism, Martens & Sutskever (2011) showed that the HF optimizer is robust enough to train RNNs, both on pathological synthetic datasets known to be impossible to learn with gradient descent, and on complex and diverse realworld sequence datasets  SENTENCE_END
SENTENCE_START The goal of the paper is to demonstrate the power of large RNNs trained with the new Hessian-Free optimizer by applying them to the task of predicting the next character in a stream of text  SENTENCE_END
SENTENCE_START This is an important problem because a better character-level language model could improve compression of text files (Rissanen & Langdon, 1979) and make it easier for people with physical disabilities to interact with computers (Ward et al , 2000)  SENTENCE_END
SENTENCE_START More speculatively, achieving the asymptotic limit in text compression requires an understanding that is "equivalent to intelligence" (Hutter, 2006)  SENTENCE_END
SENTENCE_START Good compression can be achieved by exploiting simple regularities such as the vocabulary and the syntax of the relevant languages and the shallow associations exemplified by the fact that the word "milk" often occurs soon after the word "cow", but beyond a certain point any improvement in performance must result from a deeper understanding of the text's meaning  SENTENCE_END
SENTENCE_START Although standard RNNs are very expressive, we found that achieving competitive results on character-level language modeling required the development of a different type of RNN that was better suited to our application  SENTENCE_END
SENTENCE_START This new "MRNN" architecture uses multiplicative connections to allow the current input character to determine the hidden-to-hidden weight matrix  SENTENCE_END
SENTENCE_START We trained MRNNs on over a hundred of megabytes of text for several days using 8 Graphics Processing Units in parallel to perform significantly better than one of the best word-agnostic single character-level language models: the sequence memoizer (Wood et al , 2009; Gasthaus et al , 2010), which is a hierarchical nonparametric Bayesian method  SENTENCE_END
SENTENCE_START It defines a prior process on the set of predictions at every conceivable context, with judiciously chosen details that make approximate inference computationally tractable  SENTENCE_END
SENTENCE_START The memoizer induces dependencies between its predictions by making similar predictions at similar contexts  SENTENCE_END
SENTENCE_START Although intelligent marginalization techniques are able to eliminate all but a relatively small number of the random variables (so the datastructures used scale linearly with the amount of data), its memory requirements are still prohibitively expensive for large datasets, which is a direct consequence of its nonparametric nature  SENTENCE_END
SENTENCE_START While our method performs at the state of the art for pure character-level models, its compression performance falls short of the best models which have explicit knowledge of words, the most powerful of these being PAQ8hp12 (Mahoney, 2005)  SENTENCE_END
SENTENCE_START PAQ is a mixture model of a large number of well-chosen context models whose mixing proportions are computed by a neural network whose weights are a function of the current context, and whose predictions are further combined with a neural-network like model  SENTENCE_END
SENTENCE_START Unlike standard compression techniques, some of PAQ's context models not only consider contiguous contexts but also contexts with "gaps", allowing it to capture some types of longer range structures cheaply  SENTENCE_END
SENTENCE_START More significantly, PAQ is not word-agnostic, because it uses a combination of character-level and word-level models  SENTENCE_END
SENTENCE_START PAQ also preprocesses the data with a dictionary of common English words which we disabled, because it gave PAQ an unfair advantage over models that do not use such task-specific (and indeed, English-specific) explicit prior knowledge  SENTENCE_END
SENTENCE_START The numerous mixture components of PAQ were chosen because they improved performance on a development set, so in this respect PAQ is similar in model complexity to the winning entry of the netflix prize (Bell et al , 2007)  SENTENCE_END
SENTENCE_STARTFigure 1. A Recurrent Neural Network is a very deep feedforward neural network whose weights are shared across time  SENTENCE_END
SENTENCE_START The nonlinear activation function used by the hidden units is the source of the RNN's rich dynamics  SENTENCE_END
SENTENCE_START Finally, language models can be used to "generate"  language, and to our surprise, the text generated by the MRNNs we trained exhibited a significant amount of interesting and high-level linguistic structure, featuring a large vocabulary, a considerable amount of grammatical structure, and a wide variety of highly plausible proper names that were not in the training set  SENTENCE_END
SENTENCE_START Mastering the vocabulary of English did not seem to be a problem for the MRNN: it generated very few uncapitalized non-words, and those that it did generate were often very plausible, like "homosomalist" or "unameliary"  SENTENCE_END
SENTENCE_START Of particular interest was the fact that the MRNN learned to balance parentheses and quotes over long distances (e g , 30 characters)  SENTENCE_END
SENTENCE_START A character-level N-gram language model could only do this by modeling 31-grams, and neither Memoizer nor PAQ are representationally capable of balancing parentheses because of their need for exact context matches  SENTENCE_END
SENTENCE_START In contrast, the MRNN's nonlinear dynamics enables it to extract higher level "knowledge" from the text, and there are no obvious limits to its representational power because of the ability of its hidden states to perform general computation  SENTENCE_END
SENTENCE_START Recurrent Neural Networks A Recurrent Neural Network is a straightforward adaptation of the standard feed-forward neural network to allow it to model sequential data  SENTENCE_END
SENTENCE_START At each timestep, the RNN receives an input, updates its hidden state, and makes a prediction (fig  SENTENCE_END
SENTENCE_START 1)  SENTENCE_END
SENTENCE_START The RNN's high dimensional hidden state and nonlinear evolution endow it with great expressive power, enabling the hidden state of the RNN to integrate information over many timesteps and use it to make accurate predictions  SENTENCE_END
SENTENCE_START Even if the non-linearity used by each unit is quite simple, iterating it over time leads to very rich dynamics  SENTENCE_END
SENTENCE_START The standard RNN is formalized as follows: Given a sequence of input vectors (x1, xT ), the RNN computes a sequence of hidden states (h1, hT ) and a sequence of outputs (o1,oT ) by iterating the following equations for t = 1 to T: ht = tanh(Whxxt + Whhht-1 + bh) (1) ot = Wohht + bo (2) In these equations, Whx is the input-to-hidden weight matrix, Whh is the hidden-to-hidden (or recurrent) weight matrix, Woh is the hidden-to-output weight matrix, and the vectors bh and bo are the biases  SENTENCE_END
SENTENCE_START The undefined expression Whhht-1 at time t = 1 is replaced with a special initial bias vector, hinit, and the tanh nonlinearity is applied coordinate-wise  SENTENCE_END
SENTENCE_START The gradients of the RNN are easy to compute via backpropagation through time (Rumelhart et al , 1986; Werbos, 1990)1 , so it may seem that RNNs are easy to train with gradient descent  SENTENCE_END
SENTENCE_START In reality, the relationship between the parameters and the dynamics of the RNN is highly unstable which makes gradient descent ineffective  SENTENCE_END
SENTENCE_START This intuition was formalized by Hochreiter (1991) and Bengio et al  SENTENCE_END
SENTENCE_START (1994) who proved that the gradient decays (or, less frequently, blows up) exponentially as it is backpropagated through time, and used this result to argue that RNNs cannot learn long-range temporal dependencies when gradient descent is used for training  SENTENCE_END
SENTENCE_START In addition, the occasional tendency of the backpropagated gradient to exponentially blow-up greatly increases the variance of the gradients and makes learning very unstable  SENTENCE_END
SENTENCE_START As gradient descent was the main algorithm used for training neural networks at the time, these theoretical results and the empirical difficulty of training RNNs led to the near abandonment of RNN research  SENTENCE_END
SENTENCE_START One way to deal with the inability of gradient descent to learn long-range temporal structure in a standard RNN is to modify the model to include "memory" units that are specially designed to store information over long time periods  SENTENCE_END
SENTENCE_START This approach is known as "Long-Short Term Memory" (Hochreiter & Schmidhuber, 1997) and has been successfully applied to complex real-world sequence modeling tasks (e g , Graves & Schmidhuber, 2009)  SENTENCE_END
SENTENCE_START Long Short Term Memory makes it possible to handle datasets which require long-term memorization and recall but even on these datasets it is outperformed by using a standard RNN trained with the HF optimizer (Martens & Sutskever, 2011)  SENTENCE_END
SENTENCE_START Another way to avoid the problems associated with backpropagation through time is the Echo State Network (Jaeger & Haas, 2004) which forgoes learning the recurrent connections altogether and only trains the non-recurrent output weights  SENTENCE_END
SENTENCE_START This is a much easier learning task and it works surprisingly well provided the recurrent connections 1 In contrast, Dynamic Bayes Networks (Murphy, 2002), the probabilistic analogues of RNNs, do not have an efficient algorithm for computing their gradients  SENTENCE_END
SENTENCE_START Figure 2  SENTENCE_END
SENTENCE_START An illustration of the significance of the multiplicative connections (the product is depicted by a triangle)  SENTENCE_END
SENTENCE_START The presence of the multiplicative connections enables the RNN to be sensitive to conjunctions of context and character, allowing different contexts to respond in a qualitatively different manner to the same input character  SENTENCE_END
SENTENCE_START are carefully initialized so that the intrinsic dynamics of the network exhibits a rich reservoir of temporal behaviours that can be selectively coupled to the output  SENTENCE_END
SENTENCE_START The Multiplicative RNN Having applied a modestly-sized standard RNN architecture to the character-level language modeling problem (where the target output at each time step is defined as the the input character at the next time-step), we found the performance somewhat unsatisfactory, and that while increasing the dimensionality of the hidden state did help, the per-parameter gain in test performance was not sufficient to allow the method to be both practical and competitive with state-of-the-art approaches  SENTENCE_END
SENTENCE_START We address this problem by proposing a new temporal architecture called the Multiplicative RNN (MRNN) which we will argue is better suited to the language modeling task  SENTENCE_END
SENTENCE_START 3 1  SENTENCE_END
SENTENCE_START The Tensor RNN The dynamics of the RNN's hidden states depend on the hidden-to-hidden matrix and on the inputs  SENTENCE_END
SENTENCE_START In a standard RNN (as defined by eqs  SENTENCE_END
SENTENCE_START 1-2), the current input xt is first transformed via the visible-to-hidden weight matrix Whx and then contributes additively to the input for the current hidden state  SENTENCE_END
SENTENCE_START A more powerful way for the current input character to affect the hidden state dynamics would be to determine the entire hidden-to-hidden matrix (which defines the non-linear dynamics) in addition to providing an additive bias  SENTENCE_END
SENTENCE_START One motivation for this approach came from viewing an RNN as a model of an unbounded tree in which each node is a hidden state vector and each edge is labelled by a character that determines how the parent node gives rise to the child node  SENTENCE_END
SENTENCE_START This view emphasizes the resemblance of an RNN to a Markov model that stores familiar strings of characters in a tree, and it also makes it clear that the RNN tree is potentially much more powerful than the Markov model because the distributed representation of a node allows different nodes to share knowledge  SENTENCE_END
SENTENCE_START For example, the character string "ing" is quite probable after "fix" and also quite probable after "break"  SENTENCE_END
SENTENCE_START If the hidden state vectors that represent the two histories "fix" and "break" share a common representation of the fact that this could be the stem of a verb, then this common representation can be acted upon by the character "i" to produce a hidden state that predicts an "n"  SENTENCE_END
SENTENCE_START For this to be a good prediction we require the conjunction of the verb-stem representation in the previous hidden state and the character "i"  SENTENCE_END
SENTENCE_START One or other of these alone does not provide half as much evidence for predicting an "n": It is their conjunction that is important  SENTENCE_END
SENTENCE_START This strongly suggests that we need a multiplicative interaction  SENTENCE_END
SENTENCE_START To achieve this goal we modify the RNN so that its hidden-to-hidden weight matrix is a (learned) function of the current input xt: ht = tanh Whxxt + W (xt) hh ht-1 + bh (3) ot = Wohht + bo (4) These are identical to eqs  SENTENCE_END
SENTENCE_START 1 and 2, except that Whh is replaced with W (xt) hh , allowing each character to specify a different hidden-to-hidden weight matrix  SENTENCE_END
SENTENCE_START It is natural to define W (xt) hh using a tensor  SENTENCE_END
SENTENCE_START If we store M matrices, W (1) hh ,   SENTENCE_END
SENTENCE_START , W (M) hh , where M is the number of dimensions of xt, we could define W (xt) hh by the equation W (xt) hh = M m=1 x (m) t W (m) hh (5) where x (m) t is the m-th coordinate of xt  SENTENCE_END
SENTENCE_START When the input xt is a 1-of-M encoding of a character, it is easily seen that every character has an associated weight matrix and W (xt) hh is the matrix assigned to the character represented by xt  SENTENCE_END
SENTENCE_START 2 3 2  SENTENCE_END
SENTENCE_START The Multiplicative RNN The above scheme, while appealing, has a major drawback: Fully general 3-way tensors are not practical because of their size  SENTENCE_END
SENTENCE_START In particular, if we want to use RNNs with a large number of hidden units (say, 1000) and if the dimensionality of xt is even moderately large, then the storage required for the tensor W (xt) hh becomes prohibitive  SENTENCE_END
SENTENCE_START It turns out we can remedy the above problem by factoring the tensor W (x) hh (e g , Taylor & Hinton, 2009)  SENTENCE_END
SENTENCE_START This is done by introducing the three matrices Wfx, Whf , and Wfh, and reparameterizing the matrix W (xt) hh by the equation W (xt) hh = Whf  diag(Wfxxt)  Wfh (6) 2 The above model, applied to discrete inputs represented with their 1-of-M encodings, is the nonlinear version of the Observable Operator Model (OOM; Jaeger, 2000) whose linear nature makes it closely related to an HMM in terms of expressive power  SENTENCE_END
SENTENCE_START Figure 3  SENTENCE_END
SENTENCE_START The Multiplicative Recurrent Neural Network "gates" the recurrent weight matrix with the input symbol  SENTENCE_END
SENTENCE_START Each triangle symbol represents a factor that applies a learned linear filter at each of its two input vertices  SENTENCE_END
SENTENCE_START The product of the outputs of these two linear filters is then sent, via weighted connections, to all the units connected to the third vertex of the triangle  SENTENCE_END
SENTENCE_START Consequently every input can synthesize its own hidden-to-hidden weight matrix by determining the gains on all of the factors, each of which represents a rank one hidden-to-hidden weight matrix defined by the outer-product of its incoming and outgoing weight-vectors to the hidden units  SENTENCE_END
SENTENCE_START The synthesized weight matrices share "structure" because they are all formed by blending the same set of rank one matrices  SENTENCE_END
SENTENCE_START In contrast, an unconstrained tensor model ensures that each input has a completely separate weight matrix  SENTENCE_END
SENTENCE_START If the dimensionality of the vector Wfxxt, denoted by F, is sufficiently large, then the factorization is as expressive as the original tensor  SENTENCE_END
SENTENCE_START Smaller values of F require fewer parameters while hopefully retaining a significant fraction of the tensor's expressive power  SENTENCE_END
SENTENCE_START The Multiplicative RNN (MRNN) is the result of factorizing the Tensor RNN by expanding eq  SENTENCE_END
SENTENCE_START 6 within eq  SENTENCE_END
SENTENCE_START The MRNN computes the hidden state sequence (h1,   SENTENCE_END
SENTENCE_START , hT ), an additional "factor state sequence" (f1,   SENTENCE_END
SENTENCE_START , fT ), and the output sequence (o1,   SENTENCE_END
SENTENCE_START , oT ) by iterating the equations ft = diag(Wfxxt)  Wfhht-1 (7) ht = tanh(Whf ft + Whxxt) (8) ot = Wohht + bo (9) which implement the neural network in fig  SENTENCE_END
SENTENCE_START The tensor factorization of eq  SENTENCE_END
SENTENCE_START 6 has the interpretation of an additional layer of multiplicative units between each pair of consecutive layers (i e , the triangles in fig  SENTENCE_END
SENTENCE_START 3), so the MRNN actually has two steps of nonlinear processing in its hidden states for every input timestep  SENTENCE_END
SENTENCE_START Each of the multiplicative units outputs the value ft of eq  SENTENCE_END
SENTENCE_START 7 which is the product of the outputs of the two linear filters connecting the multiplicative unit to the previous hidden states and to the inputs  SENTENCE_END
SENTENCE_START We experimentally verified the advantage of the MRNN over the RNN when the two have the same number of parameters  SENTENCE_END
SENTENCE_START We trained an RNN with 500 hidden units and an MRNN with 350 hidden units and 350 factors (so the RNN has slightly more parameters) on the "machine learning" dataset (dataset 3 in the experimental section)  SENTENCE_END
SENTENCE_START After extensive training, the MRNN achieved 1 56 bits per character and the RNN achieved 1 65 bits per character on the Generating Text with Recurrent Neural Networks test set  SENTENCE_END
SENTENCE_START 3 3  SENTENCE_END
SENTENCE_START The difficulty of learning multiplicative units In an MRNN, the effective weight W (c) ij 3 from hidden unit i to hidden unit j contributed by character c is given by: W (c) ij = f Wif WfcWfj (10) This product of parameters makes gradient descent learning difficult  SENTENCE_END
SENTENCE_START If, for example, Wif is very small and Wfj is very large we get a very large deriviative for the very small weight and a very small derivative for the very large weight  SENTENCE_END
SENTENCE_START Fortunately, this type of difficulty is exactly what second-order methods are good at handling, so multiplicative units should be better handled by a 2nd-order approach like the HF optimizer  SENTENCE_END
SENTENCE_START The RNN as a Generative Model The goal of character-level language modeling is to predict the next character in a sequence  SENTENCE_END
SENTENCE_START More formally, given a training sequence (x1,   SENTENCE_END
SENTENCE_START , xT ), the RNN uses the sequence of its output vectors (o1,   SENTENCE_END
SENTENCE_START , oT ) to obtain a sequence of predictive distributions P(xt+1|xt) = softmax(ot), where the softmax distribution is defined by P(softmax(ot) = j) = exp(o (j) t )/ k exp(o (k) t )  SENTENCE_END
SENTENCE_START The language modeling objective is to maximize the total log probability of the training sequence T-1 t=0 log P(xt+1|xt), which implies that the RNN learns a probability distribution over sequences  SENTENCE_END
SENTENCE_START Even though the hidden units are deterministic, we can sample from an MRNN stochastically because the states of its output units define the conditional distribution P(xt+1|xt)  SENTENCE_END
SENTENCE_START We can sample from this conditional distribution to get the next character in a generated string and provide it as the next input to the RNN  SENTENCE_END
SENTENCE_START This means that the RNN is a directed non-Markov model and, in this respect, it resembles the sequence memoizer (Wood et al , 2009)  SENTENCE_END
SENTENCE_START The experiments The goal of our experiments is to demonstrate that the MRNN, when trained by HF, learns high-quality language models  SENTENCE_END
SENTENCE_START We demonstrate this by comparing the MRNN to the sequence memoizer and to PAQ on three real-world language datasets  SENTENCE_END
SENTENCE_START After splitting each dataset into a training and test set, we trained a large MRNN, a sequence memoizer4 , and PAQ, and report the bits per character (bpc) each model achieves on the test set  SENTENCE_END
SENTENCE_START 3 We slightly abuse notation, using W (c) ij to stand for W (c) hh ij   SENTENCE_END
SENTENCE_START 4 Which has no hyper-parameters and strictly speaking isn't 'trained' but rather conditioned the training set  SENTENCE_END
SENTENCE_START Owing to its nonparametric nature and the nature of the data-structures it uses, the sequence memoizer is very memory intensive, so it can only be applied to training datasets of roughly 130MB on a machine with 32GB of RAM  SENTENCE_END
SENTENCE_START In contrast, the MRNN can be applied to datasets of unlimited size although it typically requires considerably more total FLOPS to achieve good performance (but, unlike the memoizer, it is easily parallelized)  SENTENCE_END
SENTENCE_START However, to make the experimental comparison fair, we train the MRNN, the memoizer, and PAQ on datasets of the same size  SENTENCE_END
SENTENCE_START 5 1  SENTENCE_END
SENTENCE_START The datasets We now describe the datasets  SENTENCE_END
SENTENCE_START Each dataset is a long string of characters from an 86-character alphabet of about 100MB that includes digits and punctuation, together with a special symbol which indicates that the character in the original text was not one of the other 85 characters in our alphabet  SENTENCE_END
SENTENCE_START The last 10 million characters of each dataset are used as a test set  SENTENCE_END
SENTENCE_START The first dataset is a sequence of characters from the English Wikipedia  SENTENCE_END
SENTENCE_START We removed the XML and the Wikipedia markup to clean the dataset  SENTENCE_END
SENTENCE_START Since Wikipedia is extremely nonuniform, we randomly permuted its articles before partitioning it into a train and a test set  SENTENCE_END
SENTENCE_START The second dataset is a collection of articles from the New York Times (Sandhaus, 2008)  SENTENCE_END
SENTENCE_START The third dataset is a corpus of machine learning papers  SENTENCE_END
SENTENCE_START We constructed this dataset by downloading every NIPS and JMLR paper, and converting them to plain text using the pdftotext utility  SENTENCE_END
SENTENCE_START We then translated a large number of special characters to their ascii equivalents (including non-ascii punctuation, greek letters, and the "fi" and "if" symbol) to clean the dataset, and removed most of the unstructured text by using only sentences consisting of at least 70% alphanumeric characters  SENTENCE_END
SENTENCE_START Finally, we randomly permuted the papers  SENTENCE_END
SENTENCE_START The first two corpora are subsets of larger corpora (over 1GB large), but the semi-online nature of our optimizer makes it easy to train the MRNN on a dataset of any size  SENTENCE_END
SENTENCE_START 5 2  SENTENCE_END
SENTENCE_START Training details To compute the exact gradient of the log probability of the training set (eq  SENTENCE_END
SENTENCE_START 4), the MRNN needs to process the entire training set sequentially and store the hidden state sequence in order to apply backpropagation-through-time  SENTENCE_END
SENTENCE_START This is infeasible due to the size of the training set but it is also unnecessary: Training the MRNN on many shorter sequences is just as effective, provided they are several hundred characters or more long  SENTENCE_END
SENTENCE_START If the sequences are too short, we fail to utilize the ability of the HF optimizer to capture long- SENTENCE_END
SENTENCE_START This table shows the test bits per character for each ex- periment, with the training bits in brackets (where available)  SENTENCE_END
SENTENCE_START The MRNN achieves lower bits per character than the sequence memoizer but higher than PAQ on each of the three datasets  SENTENCE_END
SENTENCE_START The MRNN (full set) column refers to MRNNs trained on the larger (1GB) training corpora (except for the ML dataset which is not a subset of a larger corpus)  SENTENCE_END
SENTENCE_START Note, also, that the improvement resulting from larger dataset is modest, implying that the an MRNN with 1500 units and factors is fairly well-trained with 100MB of text  SENTENCE_END
SENTENCE_START DATA SET MEMOIZER PAQ MRNN MRNN (FULL SET) WIKI 1 66 1 51 1 60 (1 53) 1 55 (1 54) NYT 1 49 1 38 1 48 (1 44) 1 47 (1 46) ML 1 33 1 22 1 31 (1 27) term dependencies spanning hundreds of timesteps  SENTENCE_END
SENTENCE_START An advantage of using a large number of relatively short sequences over using a single long sequence is that the former is much easier to parallelize  SENTENCE_END
SENTENCE_START This is essential, since our preliminary experiments suggested that HF applied to MRNNs works best when the gradient is computed using millions of characters and the curvature-matrix vector products are computed using hundreds of thousands of characters  SENTENCE_END
SENTENCE_START Using a highly parallel system (consisting of 8 high-end GPUs with 4GB of RAM each), we computed the gradient on 160300=48000 sequences of length 250, of which 8300=2400 sequences were used to compute the curvature-matrix vector products that are needed for the HF optimizer (Martens & Sutskever, 2011) (so each GPU processes 300 sequences at a time)  SENTENCE_END
SENTENCE_START The first few characters of any sequence are much harder to predict because they do not have a sufficiently large context, so it is not beneficial to have the MRNN spend neural resources predicting these characters  SENTENCE_END
SENTENCE_START We take this effect into account by having the MRNN predict only the last 200 timesteps of the 250-long training sequences, thus providing every prediction with at least 50 characters of context  SENTENCE_END
SENTENCE_START The Hessian-Free optimizer (Martens, 2010) and its RNN-specialized variant (Martens & Sutskever, 2011) have a small number of meta-parameters that must be specified  SENTENCE_END
SENTENCE_START We set the structural damping coefficient  to 0 1, and initialized  to 10 (see Martens & Sutskever (2011) for a description of these meta-parameters)  SENTENCE_END
SENTENCE_START Our HF implementation uses a different subset of the training data at every iteration, so at a coarse temporal scale it is essentially online  SENTENCE_END
SENTENCE_START In this setup, training lasted roughly 5 days for each dataset  SENTENCE_END
SENTENCE_START We found that a total of 160150 weight updates was sufficient to adequately train an MRNN  SENTENCE_END
SENTENCE_START More specifically, we used 160 steps of HF, with each of these steps using a maximum of 150 conjugate gradient iterations to approach the minimum of the quadratic Gauss-Newton-based approximation to the objective function, which remains fixed during the conjugate gradient iterations  SENTENCE_END
SENTENCE_START The small number of weight updates, each requiring a massive amount of computation, makes the HF optimizer much easier to parallelize than stochastic gradient descent  SENTENCE_END
SENTENCE_START In all our experiments we use MRNNs with 1500 hidden units and 1500 factors (F), which have 4,900,000 parameters  SENTENCE_END
SENTENCE_START The MRNNs were initialized with sparse connections: each unit starts out with 15 nonzero connections to other units (see Martens & Sutskever, 2011)  SENTENCE_END
SENTENCE_START Note that if we unroll the MRNN in time (as in fig  SENTENCE_END
SENTENCE_START 3) we obtain a neural network with 500 layers of size 1500 if we view the multiplicative units ft as layers  SENTENCE_END
SENTENCE_START This is arguably the deepest and largest neural network ever trained  SENTENCE_END
SENTENCE_START 5 3  SENTENCE_END
SENTENCE_START The results The main experimental results are shown in table 5 2  SENTENCE_END
SENTENCE_START We see that the MRNN predicts the test set more accurately than the sequence memoizer but less accurately than the dictionary-free PAQ on the three datasets  SENTENCE_END
SENTENCE_START 5 4  SENTENCE_END
SENTENCE_START Debagging It is easy to convert a sentence into a bag of words, but it is much harder to convert a bag of words into a meaningful sentence  SENTENCE_END
SENTENCE_START We name the latter the debagging problem  SENTENCE_END
SENTENCE_START We perform an experiment where a character-level language model evaluates every possible ordering of the words in the bag, and returns and the ordering it deems best  SENTENCE_END
SENTENCE_START To make the experiment tractable, we only considered bags of 7 words, giving a search space of size 5040  SENTENCE_END
SENTENCE_START For our experiment, we used the MRNN and the memoizer5 to debag 500 bags of randomly chosen words from "Ana Karenina"  SENTENCE_END
SENTENCE_START We use 11 words for each bag, where the first two and the last two words are used as context to aid debagging the middle seven words  SENTENCE_END
SENTENCE_START We say that the model correctly debags a sentence if the correct ordering is assigned the highest log probability  SENTENCE_END
SENTENCE_START We found that the wikipedia-trained MRNN recovered the correct ordering 34% of the time, and the wikipedia-trained memoizer did so 27% of the time  SENTENCE_END
SENTENCE_START Given that the problem is "word-level", utilizing large character contexts is essential to achieve good performance  SENTENCE_END
SENTENCE_START 5 We were unable to modify the implementation of PAQ to make debagging feasible  SENTENCE_END
SENTENCE_START Generating Text with Recurrent Neural Networks 6  SENTENCE_END
SENTENCE_START Qualitative experiments In this section we qualitatively investigate the nature of the models learned by the MRNN  SENTENCE_END
SENTENCE_START 6 1  SENTENCE_END
SENTENCE_START Samples from the models The simplest qualitative experiment is to inspect the samples generated by the three MRNNs  SENTENCE_END
SENTENCE_START The most salient characteristic of the samples is the richness of their vocabularies  SENTENCE_END
SENTENCE_START Further inspection reveals that the text is mostly grammatical, and that parentheses are usually balanced over many characters  SENTENCE_END
SENTENCE_START The artifacts of the generated text, such as consecutive commas or quotes, are the result of the data preprocessing and are frequently found in the training set  SENTENCE_END
SENTENCE_START 6 1 1  SENTENCE_END
SENTENCE_START SAMPLES FROM THE WIKIPEDIA MODEL We now present a sample from the Wikipedia model  SENTENCE_END
SENTENCE_START We use ? SENTENCE_END
SENTENCE_START to indicate the "unknown" character  SENTENCE_END
SENTENCE_START The sample below was obtained by running the MRNN less than 10 times and selecting the most intriguing sample  SENTENCE_END
SENTENCE_START The beginning of the paragraph and the parentheses near the end are particularly interesting  SENTENCE_END
SENTENCE_START The MRNN was initialized with the phrase "The meaning of life is": The meaning of life is the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove her bigger  SENTENCE_END
SENTENCE_START In the show's agreement unanimously resurfaced  SENTENCE_END
SENTENCE_START The wild pasteured with consistent street forests were incorporated by the 15th century BE  SENTENCE_END
SENTENCE_START In 1996 the primary rapford undergoes an effort that the reserve conditioning, written into Jewish cities, sleepers to incorporate the  St Eurasia that activates the population  SENTENCE_END
SENTENCE_START Mar? SENTENCE_END
SENTENCE_START ?a Nationale, Kelli, Zedlat-Dukastoe, Florendon, Ptu's thought is  SENTENCE_END
SENTENCE_START To adapt in most parts of North America, the dynamic fairy Dan please believes, the free speech are much related to the 6 1 2  SENTENCE_END
SENTENCE_START SAMPLES FROM THE NYT MODEL Below is a sample from the model trained on the full NYT dataset, where the MRNN was initialized with a single space  SENTENCE_END
SENTENCE_START The spaces surrounding the punctuation are an artifact of the preprocessing  SENTENCE_END
SENTENCE_START while he was giving attention to the second advantage of school building a 2-for-2 stool killed by the Cultures saddled with a half- suit defending the Bharatiya Fernall 's office   SENTENCE_END
SENTENCE_START Ms   SENTENCE_END
SENTENCE_START Claire Parters will also have a history temple for him to raise jobs until naked Prodiena to paint baseball partners , provided people to ride both of Manhattan in 1978 , but what was largely directed to China in 1946 , focusing on the trademark period is the sailboat yesterday and comments on whom they obtain overheard within the 120th anniversary , where many civil rights defined , officials said early that forms , " said Bernard J  Marco Jr  of Pennsylvania , was monitoring New York 6 1 3  SENTENCE_END
SENTENCE_START SAMPLES FORM THE ML MODEL Finally, we generate text from an MRNN trained on the ML corpus conditioned on the string "Recurrent"  SENTENCE_END
SENTENCE_START This MRNN is also able to balance parentheses (e g , the third line of the sample): Recurrent network with the Stiefel information for logistic regression methods Along with either of the algorithms previously (two or more skewprecision) is more similar to the model with the same average mismatched graph  SENTENCE_END
SENTENCE_START Though this task is to be studied under the reward transform, such as (c) and (C) from the training set, based on target activities for articles a ? SENTENCE_END
SENTENCE_START 2(6) and (4 3)  SENTENCE_END
SENTENCE_START The PHDPic (PDB) matrix of cav'va using the three relevant information contains for tieming measurements  SENTENCE_END
SENTENCE_START Moreover, because of the therap tor, the aim is to improve the score to the best patch randomly, but for each initially four data sets  SENTENCE_END
SENTENCE_START As shown in Figure 11, it is more than 100 steps, we used ?? SENTENCE_END
SENTENCE_START \to \infty with 1000 6 2  SENTENCE_END
SENTENCE_START Structured sentence completion In this section, we investigate the MRNN's response in various situations by sampling from the MRNN's distribution conditioned on a prefix  SENTENCE_END
SENTENCE_START The goal is to see whether the MRNN is able to generate "plausible" continuations to the initial strings  SENTENCE_END
SENTENCE_START In our first experiment, we use the Wikipedia MRNN to complete the string "England, Spain, France, Germany,": England, Spain, France, Germany, and Massachusetts  SENTENCE_END
SENTENCE_START England, Spain, France, Germany, cars, and direct schools England, Spain, France, Germany, , or New Orleans and Uganda  SENTENCE_END
SENTENCE_START England, Spain, France, Germany, , Westchester, Jet State, Springfield, Athleaves and Sorvinhee In the above completions, the MRNN correctly interpreted the string to be a list of locations, so the generated text was also a part of a list  SENTENCE_END
SENTENCE_START Next, we performed a similar experiment using the ML model and the pair of strings "(ABC et al" and "ABC et al"  SENTENCE_END
SENTENCE_START The system has never seen the string "(ABC et al" in its training set (simply because there is no machine learning author named ABC, and its capitalization is particularly uncommon for a citation), so the MRNN needed to generalize over an entirely new author name: (ABC et al , 2003), ?13?, and for a supervised Mann-Whitnaguing (ABC et al , 2002), based on Lebanon and Haussler, 1995b) ABC et al  SENTENCE_END
SENTENCE_START (2003b), or Penalization of Information ABC et al  SENTENCE_END
SENTENCE_START (2008) can be evaluated and motivated by providing optimal estimate This example shows that the MRNN is sensitive to the initial bracket before "ABC", illustrating its representational power  SENTENCE_END
SENTENCE_START The above effect is extremely robust  SENTENCE_END
SENTENCE_START In contrast, both N-gram models and the sequence memoizer cannot make such predictions unless these exact strings (e g , "(ABC et al , 2003)") occur in the training set, which cannot be counted on  SENTENCE_END
SENTENCE_START In fact, any method which is based on precise context matches is fundamentally incapable of utilizing long contexts, because the probability that a long context occurs more than once is vanishingly small  SENTENCE_END
SENTENCE_START We experimentally verified that neither the sequence memoizer Generating Text with Recurrent Neural Networks nor PAQ are not sensitive to the initial bracket  SENTENCE_END
SENTENCE_START Discussion Modeling language at the character level seems unnecessarily difficult because we already know that morphemes are the appropriate units for making semantic and syntactic predictions  SENTENCE_END
SENTENCE_START Converting large databases into sequences of morphemes, however, is non-trivial compared with treating them as character strings  SENTENCE_END
SENTENCE_START Also, learning which character strings make words is a relatively easy task compared with discovering the subtleties of semantic and syntactic struc- ture  SENTENCE_END
SENTENCE_START So, given a powerful learning system like an MRNN, the convenience of using characters may outweigh the extra work of having to learn the words All our experiments show that an MRNN finds it very easy to learn words  SENTENCE_END
SENTENCE_START With the exception of proper names, the generated text contains very few non-words  SENTENCE_END
SENTENCE_START At the same time, the MRNN also assigns probability to (and occasionally generates) plausible words that do not appear in the training set (e g , "cryptoliation", "homosomalist", or "unameliary")  SENTENCE_END
SENTENCE_START This is a desirable property which enabled the MRNN to gracefully deal with real words that it nonetheless didn't see in the training set  SENTENCE_END
SENTENCE_START Predicting the next word by making a sequence of character predictions avoids having to use a huge softmax over all known words and this is so advantageous that some word-level language models actually make up binary "spellings" of words so that they can predict them one bit at a time (Mnih & Hinton, 2009)  SENTENCE_END
SENTENCE_START MRNNs already learn surprisingly good language models using only 1500 hidden units, and unlike other approaches such as the sequence memoizer and PAQ, they are easy to extend along various dimensions  SENTENCE_END
SENTENCE_START If we could train much bigger MRNNs with millions of units and billions of connections, it is possible that brute force alone would be sufficient to achieve an even higher standard of performance  SENTENCE_END
SENTENCE_START But this will of course require considerably more computational power  SENTENCE_END






SENTENCE_START GloVe: Global Vectors for Word Representation Jeffrey Pennington, Richard Socher, Christopher D  Manning Computer Science Department, Stanford University, Stanford, CA 94305 jpennin@stanford edu, richard@socher org, manning@stanford edu Abstract Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque  SENTENCE_END
SENTENCE_START We analyze and make explicit the model properties needed for such regularities to emerge in word vectors  SENTENCE_END
SENTENCE_START The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods  SENTENCE_END
SENTENCE_START Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus  SENTENCE_END
SENTENCE_START The model produces a vector space with meaningful sub- structure, as evidenced by its performance of 75% on a recent word analogy task  SENTENCE_END
SENTENCE_START It also outperforms related models on similarity tasks and named entity recognition  SENTENCE_END
SENTENCE_START 1 Introduction Semantic vector space models of language represent each word with a real-valued vector  SENTENCE_END
SENTENCE_START These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al , 2008), document classification (Sebastiani, 2002), question answering (Tellex et al , 2003), named entity recognition (Turian et al , 2010), and parsing (Socher et al , 2013)  SENTENCE_END
SENTENCE_START Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations  SENTENCE_END
SENTENCE_START Recently, Mikolov et al  SENTENCE_END
SENTENCE_START (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference  SENTENCE_END
SENTENCE_START For example, the analogy "king is to queen as man is to woman" should be encoded in the vector space by the vector equation king - queen = man - woman  SENTENCE_END
SENTENCE_START This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009)  SENTENCE_END
SENTENCE_START The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA) (Deerwester et al , 1990) and 2) local context window methods, such as the skip-gram model of Mikolov et al  SENTENCE_END
SENTENCE_START (2013c)  SENTENCE_END
SENTENCE_START Currently, both families suffer significant drawbacks  SENTENCE_END
SENTENCE_START While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure  SENTENCE_END
SENTENCE_START Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts  SENTENCE_END
SENTENCE_START In this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression models are appropriate for doing so  SENTENCE_END
SENTENCE_START We propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics  SENTENCE_END
SENTENCE_START The model produces a word vector space with meaningful sub-structure, as evidenced by its state-of-the-art performance of 75% accuracy on the word analogy dataset  SENTENCE_END
SENTENCE_START We also demonstrate that our methods outperform other current methods on several word similarity tasks, and also on a common named entity recognition (NER) benchmark  SENTENCE_END
SENTENCE_START We provide the source code for the model as well as trained word vectors at http://nlp  SENTENCE_END
SENTENCE_START stanford edu/projects/glove/  SENTENCE_END
SENTENCE_START 2 Related Work Matrix Factorization Methods  SENTENCE_END
SENTENCE_START Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as LSA  SENTENCE_END
SENTENCE_START These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus  SENTENCE_END
SENTENCE_START The particular type of information captured by such matrices varies by application  SENTENCE_END
SENTENCE_START In LSA, the matrices are of "term-document" type, i e , the rows correspond to words or terms, and the columns correspond to different documents in the corpus  SENTENCE_END
SENTENCE_START In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of "term-term" type, i e , the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word  SENTENCE_END
SENTENCE_START A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness  SENTENCE_END
SENTENCE_START A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al , 2006), in which the co-occurrence matrix is first transformed by an entropy-or correlation-based normalization  SENTENCE_END
SENTENCE_START An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval  SENTENCE_END
SENTENCE_START A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation  SENTENCE_END
SENTENCE_START More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations  SENTENCE_END
SENTENCE_START Shallow Window-Based Methods  SENTENCE_END
SENTENCE_START Another approach is to learn word representations that aid in making predictions within local context windows  SENTENCE_END
SENTENCE_START For example, Bengio et al  SENTENCE_END
SENTENCE_START (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling  SENTENCE_END
SENTENCE_START Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al  SENTENCE_END
SENTENCE_START (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models  SENTENCE_END
SENTENCE_START Recently, the importance of the full neural network structure for learning useful word representations has been called into question  SENTENCE_END
SENTENCE_START The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al  SENTENCE_END
SENTENCE_START (2013a) propose a simple single-layer architecture based on the inner product between two word vectors  SENTENCE_END
SENTENCE_START Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al  SENTENCE_END
SENTENCE_START (2014) proposed explicit word embeddings based on a PPMI metric  SENTENCE_END
SENTENCE_START In the skip-gram and ivLBL models, the objective is to predict a word's context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context  SENTENCE_END
SENTENCE_START Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors  SENTENCE_END
SENTENCE_START Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus  SENTENCE_END
SENTENCE_START Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data  SENTENCE_END
SENTENCE_START 3 The GloVe Model The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning  SENTENCE_END
SENTENCE_START In this section, we shed some light on this question  SENTENCE_END
SENTENCE_START We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model  SENTENCE_END
SENTENCE_START First we establish some notation  SENTENCE_END
SENTENCE_START Let the matrix of word-word co-occurrence counts be denoted by X, whose entries Xi j tabulate the number of times word j occurs in the context of word i  SENTENCE_END
SENTENCE_START Let Xi = k Xik be the number of times any word appears in the context of word i  SENTENCE_END
SENTENCE_START Finally, let Pi j = P(j|i) = Xi j/Xi be the probability that word j appear in the Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token corpus  SENTENCE_END
SENTENCE_START Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam  SENTENCE_END
SENTENCE_START Probability and Ratio k = solid k = gas k = water k = fashion P(k|ice) 1 9  10-4 6 6  10-5 3 0  10-3 1 7  10-5 P(k|steam) 2 2  10-5 7 8  10-4 2 2  10-3 1 8  10-5 P(k|ice)/P(k|steam) 8 9 8 5  10-2 1 36 0 96 context of word i  SENTENCE_END
SENTENCE_START We begin with a simple example that showcases how certain aspects of meaning can be extracted directly from co-occurrence probabilities  SENTENCE_END
SENTENCE_START Consider two words i and j that exhibit a particular aspect of interest; for concreteness, suppose we are interested in the concept of thermodynamic phase, for which we might take i = ice and j = steam  SENTENCE_END
SENTENCE_START The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, k  For words k related to ice but not steam, say k = solid, we expect the ratio Pik/Pjk will be large  SENTENCE_END
SENTENCE_START Similarly, for words k related to steam but not ice, say k = gas, the ratio should be small  SENTENCE_END
SENTENCE_START For words k like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one  SENTENCE_END
SENTENCE_START Table 1 shows these probabilities and their ratios for a large corpus, and the numbers confirm these expectations  SENTENCE_END
SENTENCE_START Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words  SENTENCE_END
SENTENCE_START The above argument suggests that the appropriate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves  SENTENCE_END
SENTENCE_START Noting that the ratio Pik/Pjk depends on three words i, j, and k, the most general model takes the form, F(wi,wj, ~ wk ) = Pik Pjk , (1) where w  Rd are word vectors and ~ w  Rd are separate context word vectors whose role will be discussed in Section 4 2  SENTENCE_END
SENTENCE_START In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters  SENTENCE_END
SENTENCE_START The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice  SENTENCE_END
SENTENCE_START First, we would like F to encode the information present the ratio Pik/Pjk in the word vector space  SENTENCE_END
SENTENCE_START Since vector spaces are inherently linear structures, the most natural way to do this is with vector differences  SENTENCE_END
SENTENCE_START With this aim, we can restrict our consideration to those functions F that depend only on the difference of the two target words, modifying Eqn  SENTENCE_END
SENTENCE_START (1) to, F(wi - wj, ~ wk ) = Pik Pjk   SENTENCE_END
SENTENCE_START (2) Next, we note that the arguments of F in Eqn  SENTENCE_END
SENTENCE_START (2) are vectors while the right-hand side is a scalar  SENTENCE_END
SENTENCE_START While F could be taken to be a complicated function parameterized by, e g , a neural network, doing so would obfuscate the linear structure we are trying to capture  SENTENCE_END
SENTENCE_START To avoid this issue, we can first take the dot product of the arguments, F (wi - wj )T ~ wk = Pik Pjk , (3) which prevents F from mixing the vector dimensions in undesirable ways  SENTENCE_END
SENTENCE_START Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles  SENTENCE_END
SENTENCE_START To do so consistently, we must not only exchange w  ~ w but also X  XT   SENTENCE_END
SENTENCE_START Our final model should be invariant under this relabeling, but Eqn  SENTENCE_END
SENTENCE_START (3) is not  SENTENCE_END
SENTENCE_START However, the symmetry can be restored in two steps  SENTENCE_END
SENTENCE_START First, we require that F be a homomorphism between the groups (R,+) and (R>0, ), i e , F (wi - wj )T ~ wk = F(wT i ~ wk ) F(wT j ~ wk ) , (4) which, by Eqn  SENTENCE_END
SENTENCE_START (3), is solved by, F(wT i ~ wk ) = Pik = Xik Xi   SENTENCE_END
SENTENCE_START (5) The solution to Eqn  SENTENCE_END
SENTENCE_START (4) is F = exp, or, wT i ~ wk = log(Pik ) = log(Xik ) - log(Xi )   SENTENCE_END
SENTENCE_START (6) Next, we note that Eqn  SENTENCE_END
SENTENCE_START (6) would exhibit the exchange symmetry if not for the log(Xi ) on the right-hand side  SENTENCE_END
SENTENCE_START However, this term is independent of k so it can be absorbed into a bias bi for wi  SENTENCE_END
SENTENCE_START Finally, adding an additional bias ~ bk for ~ wk restores the symmetry, wT i ~ wk + bi + ~ bk = log(Xik )   SENTENCE_END
SENTENCE_START (7) Eqn  SENTENCE_END
SENTENCE_START (7) is a drastic simplification over Eqn  SENTENCE_END
SENTENCE_START (1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero  SENTENCE_END
SENTENCE_START One resolution to this issue is to include an additive shift in the logarithm, log(Xik )  log(1 + Xik ), which maintains the sparsity of X while avoiding the divergences  SENTENCE_END
SENTENCE_START The idea of factorizing the log of the co-occurrence matrix is closely related to LSA and we will use the resulting model as a baseline in our experiments  SENTENCE_END
SENTENCE_START A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never  SENTENCE_END
SENTENCE_START Such rare co-occurrences are noisy and carry less information than the more frequent ones -- yet even just the zero entries account for 7595% of the data in X, depending on the vocabulary size and corpus  SENTENCE_END
SENTENCE_START We propose a new weighted least squares regression model that addresses these problems  SENTENCE_END
SENTENCE_START Casting Eqn  SENTENCE_END
SENTENCE_START (7) as a least squares problem and introducing a weighting function f (Xi j ) into the cost function gives us the model J = V i, j=1 f Xi j wT i ~ wj + bi + ~ bj - log Xi j 2 , (8) where V is the size of the vocabulary  SENTENCE_END
SENTENCE_START The weighting function should obey the following properties: 1  f (0) = 0  SENTENCE_END
SENTENCE_START If f is viewed as a continuous function, it should vanish as x  0 fast enough that the limx0 f (x) log2 x is finite  SENTENCE_END
SENTENCE_START 2  f (x) should be non-decreasing so that rare co-occurrences are not overweighted  SENTENCE_END
SENTENCE_START 3  f (x) should be relatively small for large values of x, so that frequent co-occurrences are not overweighted  SENTENCE_END
SENTENCE_START Of course a large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as, f (x) = (x/xmax) if x < xmax 1 otherwise   SENTENCE_END
SENTENCE_START (9) 0 2 0 4 0 6 0 8 1 0 0 0 Figure 1: Weighting function f with  = 3/4  SENTENCE_END
SENTENCE_START The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments  SENTENCE_END
SENTENCE_START We found that  = 3/4 gives a modest improvement over a linear version with  = 1  SENTENCE_END
SENTENCE_START Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al , 2013a)  SENTENCE_END
SENTENCE_START 3 1 Relationship to Other Models Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be com- monalities between the models  SENTENCE_END
SENTENCE_START Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL  SENTENCE_END
SENTENCE_START Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn  SENTENCE_END
SENTENCE_START (8)  SENTENCE_END
SENTENCE_START The starting point for the skip-gram or ivLBL methods is a model Qi j for the probability that word j appears in the context of word i  SENTENCE_END
SENTENCE_START For concreteness, let us assume that Qi j is a softmax, Qi j = exp(wT i ~ wj ) V k=1 exp(wT i ~ wk )   SENTENCE_END
SENTENCE_START (10) Most of the details of these models are irrelevant for our purposes, aside from the the fact that they attempt to maximize the log probability as a context window scans over the corpus  SENTENCE_END
SENTENCE_START Training proceeds in an online, stochastic fashion, but the implied global objective function can be written as, J = - icorpus jcontext(i) log Qi j   SENTENCE_END
SENTENCE_START (11) Evaluating the normalization factor of the softmax for each term in this sum is costly  SENTENCE_END
SENTENCE_START To allow for efficient training, the skip-gram and ivLBL models introduce approximations to Qi j  SENTENCE_END
SENTENCE_START However, the sum in Eqn  SENTENCE_END
SENTENCE_START (11) can be evaluated much more efficiently if we first group together those terms that have the same values for i and j, J = - V i=1 V j=1 Xi j log Qi j , (12) where we have used the fact that the number of like terms is given by the co-occurrence matrix X  Recalling our notation for Xi = k Xik and Pi j = Xi j/Xi, we can rewrite J as, J = - V i=1 Xi V j=1 Pi j log Qi j = V i=1 Xi H(Pi,Qi ) , (13) where H(Pi,Qi ) is the cross entropy of the distributions Pi and Qi, which we define in analogy to Xi  SENTENCE_END
SENTENCE_START As a weighted sum of cross-entropy error, this objective bears some formal resemblance to the weighted least squares objective of Eqn  SENTENCE_END
SENTENCE_START (8)  SENTENCE_END
SENTENCE_START In fact, it is possible to optimize Eqn  SENTENCE_END
SENTENCE_START (13) directly as opposed to the online training methods used in the skip-gram and ivLBL models  SENTENCE_END
SENTENCE_START One could interpret this objective as a "global skip-gram" model, and it might be interesting to investigate further  SENTENCE_END
SENTENCE_START On the other hand, Eqn  SENTENCE_END
SENTENCE_START (13) exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word vectors  SENTENCE_END
SENTENCE_START To begin, cross entropy error is just one among many possible distance measures between probability distributions, and it has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events  SENTENCE_END
SENTENCE_START Furthermore, for the measure to be bounded it requires that the model distribution Q be properly normalized  SENTENCE_END
SENTENCE_START This presents a computational bottleneck owing to the sum over the whole vocabulary in Eqn  SENTENCE_END
SENTENCE_START (10), and it would be desirable to consider a different distance measure that did not require this property of Q  SENTENCE_END
SENTENCE_START A natural choice would be a least squares objective in which normalization factors in Q and P are discarded, ^ J = i, j Xi ^ Pi j - ^ Qi j 2 (14) where ^ Pi j = Xi j and ^ Qi j = exp(wT i ~ wj ) are the unnormalized distributions  SENTENCE_END
SENTENCE_START At this stage another problem emerges, namely that Xi j often takes very large values, which can complicate the optimization  SENTENCE_END
SENTENCE_START An effective remedy is to minimize the squared error of the logarithms of ^ P and ^ Q instead, ^ J = i, j Xi log ^ Pi j - log ^ Qi j 2 = i, j Xi wT i ~ wj - log Xi j 2   SENTENCE_END
SENTENCE_START (15) Finally, we observe that while the weighting factor Xi is preordained by the online training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal  SENTENCE_END
SENTENCE_START In fact, Mikolov et al  SENTENCE_END
SENTENCE_START (2013a) observe that performance can be increased by filtering the data so as to reduce the effective value of the weighting factor for frequent words  SENTENCE_END
SENTENCE_START With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well  SENTENCE_END
SENTENCE_START The result is, ^ J = i, j f (Xi j ) wT i ~ wj - log Xi j 2 , (16) which is equivalent1 to the cost function of Eqn  SENTENCE_END
SENTENCE_START (8), which we derived previously  SENTENCE_END
SENTENCE_START 3 2 Complexity of the model As can be seen from Eqn  SENTENCE_END
SENTENCE_START (8) and the explicit form of the weighting function f (X), the computational complexity of the model depends on the number of nonzero elements in the matrix X  SENTENCE_END
SENTENCE_START As this number is always less than the total number of entries of the matrix, the model scales no worse than O(|V |2)  SENTENCE_END
SENTENCE_START At first glance this might seem like a substantial improvement over the shallow window-based approaches, which scale with the corpus size, |C|  SENTENCE_END
SENTENCE_START However, typical vocabularies have hundreds of thousands of words, so that |V |2 can be in the hundreds of billions, which is actually much larger than most corpora  SENTENCE_END
SENTENCE_START For this reason it is important to determine whether a tighter bound can be placed on the number of nonzero elements of X  SENTENCE_END
SENTENCE_START In order to make any concrete statements about the number of nonzero elements in X, it is necessary to make some assumptions about the distribution of word co-occurrences  SENTENCE_END
SENTENCE_START In particular, we will assume that the number of co-occurrences of word i with word j, Xi j, can be modeled as a power-law function of the frequency rank of that word pair, ri j: Xi j = k (ri j )   SENTENCE_END
SENTENCE_START (17) 1We could also include bias terms in Eqn  SENTENCE_END
SENTENCE_START (16)  SENTENCE_END
SENTENCE_START The total number of words in the corpus is proportional to the sum over all elements of the co- occurrence matrix X, |C| i j Xi j = |X | r=1 k r = kH|X |, , (18) where we have rewritten the last sum in terms of the generalized harmonic number Hn,m  SENTENCE_END
SENTENCE_START The upper limit of the sum, |X|, is the maximum frequency rank, which coincides with the number of nonzero elements in the matrix X  SENTENCE_END
SENTENCE_START This number is also equal to the maximum value of r in Eqn  SENTENCE_END
SENTENCE_START (17) such that Xi j  1, i e , |X| = k1/  SENTENCE_END
SENTENCE_START Therefore we can write Eqn  SENTENCE_END
SENTENCE_START (18) as, |C|  |X| H|X |,   SENTENCE_END
SENTENCE_START (19) We are interested in how |X| is related to |C| when both numbers are large; therefore we are free to expand the right hand side of the equation for large |X|  SENTENCE_END
SENTENCE_START For this purpose we use the expansion of generalized harmonic numbers (Apostol, 1976), Hx,s = x1-s 1 - s + (s) + O(x-s) if s > 0, s 1 , (20) giving, |C| |X| 1 - + () |X| + O(1) , (21) where (s) is the Riemann zeta function  SENTENCE_END
SENTENCE_START In the limit that X is large, only one of the two terms on the right hand side of Eqn  SENTENCE_END
SENTENCE_START (21) will be relevant, and which term that is depends on whether  > 1, |X| = O(|C|) if  < 1, O(|C|1/) if  > 1  SENTENCE_END
SENTENCE_START (22) For the corpora studied in this article, we observe that Xi j is well-modeled by Eqn  SENTENCE_END
SENTENCE_START (17) with  = 1 25  SENTENCE_END
SENTENCE_START In this case we have that |X| = O(|C|0 8)  SENTENCE_END
SENTENCE_START Therefore we conclude that the complexity of the model is much better than the worst case O(V2), and in fact it does somewhat better than the online window-based methods which scale like O(|C|)  SENTENCE_END
SENTENCE_START 4 Experiments 4 1 Evaluation methods We conduct experiments on the word analogy task of Mikolov et al  SENTENCE_END
SENTENCE_START (2013a), a variety of word similarity tasks, as described in (Luong et al , 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy  SENTENCE_END
SENTENCE_START Underlined scores are best within groups of similarly-sized models; bold scores are best overall  SENTENCE_END
SENTENCE_START HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al , 2013); skip-gram (SG) and CBOW results are from (Mikolov et al , 2013a,b); we trained SG and CBOW using the word2vec tool3  SENTENCE_END
SENTENCE_START See text for details and a description of the SVD models  SENTENCE_END
SENTENCE_START Model Dim  SENTENCE_END
SENTENCE_START Size Sem  SENTENCE_END
SENTENCE_START Syn  SENTENCE_END
SENTENCE_START Tot  SENTENCE_END
SENTENCE_START Word analogies  SENTENCE_END
SENTENCE_START The word analogy task consists of questions like, "a is to b as c is to ?" SENTENCE_END
SENTENCE_START The dataset contains 19,544 such questions, divided into a semantic subset and a syntactic subset  SENTENCE_END
SENTENCE_START The semantic questions are typically analogies about people or places, like "Athens is to Greece as Berlin is to ?"  SENTENCE_END
SENTENCE_START The syntactic questions are typically analogies about verb tenses or forms of adjectives, for example "dance is to dancing as fly is to ?"  SENTENCE_END
SENTENCE_START To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match  SENTENCE_END
SENTENCE_START We answer the question "a is to b as c is to ?" SENTENCE_END
SENTENCE_START by finding the word d whose representation wd is closest to wb - wa + wc according to the cosine similarity 4 2http://lebret ch/words/ 3http://code google com/p/word2vec/ 4Levy et al  SENTENCE_END
SENTENCE_START (2014) introduce a multiplicative analogy evaluation, 3COSMUL, and report an accuracy of 68 24% on 0 100 200 300 400 500 600 20 30 40 50 60 70 80 Vector Dimension Accuracy [%] Semantic Syntactic Overall (a) Symmetric context 2 4 6 8 10 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (b) Symmetric context 2 4 6 8 10 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (c) Asymmetric context Figure 2: Accuracy on the analogy task as function of vector size and window size/type  SENTENCE_END
SENTENCE_START All models are trained on the 6 billion token corpus  SENTENCE_END
SENTENCE_START In (a), the window size is 10  SENTENCE_END
SENTENCE_START In (b) and (c), the vector size is 100  SENTENCE_END
SENTENCE_START Word similarity  SENTENCE_END
SENTENCE_START While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3  SENTENCE_END
SENTENCE_START These include WordSim-353 (Finkelstein et al , 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al , 2012), and RW (Luong et al , 2013)  SENTENCE_END
SENTENCE_START Named entity recognition  SENTENCE_END
SENTENCE_START The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous  SENTENCE_END
SENTENCE_START We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set  SENTENCE_END
SENTENCE_START We adopt the BIO2 annota- tion standard, as well as all the preprocessing steps described in (Wang and Manning, 2013)  SENTENCE_END
SENTENCE_START We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al , 2005)  SENTENCE_END
SENTENCE_START A total of 437,905 discrete features were generated for the CoNLL-2003 training dataset  SENTENCE_END
SENTENCE_START In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features  SENTENCE_END
SENTENCE_START With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013)  SENTENCE_END
SENTENCE_START 4 2 Corpora and training details We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1 6 billion tokens; Gigaword 5 which has 4 3 billion tokens; the combination Gigaword5 + Wikipedia2014, which the analogy task  SENTENCE_END
SENTENCE_START This number is evaluated on a subset of the dataset so it is not included in Table 2  SENTENCE_END
SENTENCE_START 3COSMUL performed worse than cosine similarity in almost all of our experiments  SENTENCE_END
SENTENCE_START has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl5  SENTENCE_END
SENTENCE_START We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words6, and then construct a matrix of co-occurrence counts X  SENTENCE_END
SENTENCE_START In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context  SENTENCE_END
SENTENCE_START We explore the effect of these choices below  SENTENCE_END
SENTENCE_START In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count  SENTENCE_END
SENTENCE_START This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words' relationship to one another  SENTENCE_END
SENTENCE_START For all our experiments, we set xmax = 100, = 3/4, and train the model using AdaGrad (Duchi et al , 2011), stochastically sampling non-zero elements from X, with initial learning rate of 0 05  SENTENCE_END
SENTENCE_START We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4 6 for more details about the convergence rate)  SENTENCE_END
SENTENCE_START Unless otherwise noted, we use a context of ten words to the left and ten words to the right  SENTENCE_END
SENTENCE_START The model generates two sets of word vectors, W and ~ W  When X is symmetric, W and ~ W are equivalent and differ only as a result of their random initializations; the two sets of vectors should perform equivalently  SENTENCE_END
SENTENCE_START On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results (Ciresan et al , 2012)  SENTENCE_END
SENTENCE_START With this in mind, we choose to use 5To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable  SENTENCE_END
SENTENCE_START 6For the model trained on Common Crawl data, we use a larger vocabulary of about 2 million words  SENTENCE_END
SENTENCE_START the sum W + ~ W as our word vectors  SENTENCE_END
SENTENCE_START Doing so typically gives a small boost in performance, with the biggest increase in the semantic analogy task  SENTENCE_END
SENTENCE_START We compare with the published results of a variety of state-of-the-art models, as well as with our own results produced using the word2vec tool and with several baselines using SVDs  SENTENCE_END
SENTENCE_START With word2vec, we train the skip-gram (SG) and continuous bag-of-words (CBOW) models on the 6 billion token corpus (Wikipedia 2014 + Gigaword 5) with a vocabulary of the top 400,000 most frequent words and a context window size of 10  SENTENCE_END
SENTENCE_START We used 10 negative samples, which we show in Section 4 6 to be a good choice for this corpus  SENTENCE_END
SENTENCE_START For the SVD baselines, we generate a truncated matrix Xtrunc which retains the information of how frequently each word occurs with only the top 10,000 most frequent words  SENTENCE_END
SENTENCE_START This step is typical of many matrix-factorization-based methods as the extra columns can contribute a disproportionate number of zero entries and the methods are otherwise computationally expensive  SENTENCE_END
SENTENCE_START The singular vectors of this matrix constitute the baseline "SVD"  SENTENCE_END
SENTENCE_START We also evaluate two related baselines: "SVD-S" in which we take the SVD of Xtrunc, and "SVD-L" in which we take the SVD of log(1+Xtrunc)  SENTENCE_END
SENTENCE_START Both methods help compress the otherwise large range of values in X 7 4 3 Results We present results on the word analogy task in Table 2  SENTENCE_END
SENTENCE_START The GloVe model performs significantly better than the other baselines, often with smaller vector sizes and smaller corpora  SENTENCE_END
SENTENCE_START Our results using the word2vec tool are somewhat better than most of the previously published results  SENTENCE_END
SENTENCE_START This is due to a number of factors, including our choice to use negative sampling (which typically works better than the hierarchical softmax), the number of negative samples, and the choice of the corpus  SENTENCE_END
SENTENCE_START We demonstrate that the model can easily be trained on a large 42 billion token corpus, with a substantial corresponding performance boost  SENTENCE_END
SENTENCE_START We note that increasing the corpus size does not guarantee improved results for other models, as can be seen by the decreased performance of the SVD- 7We also investigated several other weighting schemes for transforming X; what we report here performed best  SENTENCE_END
SENTENCE_START Many weighting schemes like PPMI destroy the sparsity of X and therefore cannot feasibly be used with large vocabularies  SENTENCE_END
SENTENCE_START With smaller vocabularies, these information-theoretic transformations do indeed work well on word similarity measures, but they perform very poorly on the word analogy task  SENTENCE_END
SENTENCE_START Table 3: Spearman rank correlation on word similarity tasks  SENTENCE_END
SENTENCE_START All vectors are 300-dimensional  SENTENCE_END
SENTENCE_START The CBOW vectors are from the word2vec website and differ in that they contain phrase vectors  SENTENCE_END
SENTENCE_START Model Size WS353 MC RG SCWS RW SVD 6B 35 3 35 1 42 5 38 3 25 6 SVD-S 6B 56 5 71 5 71 0 53 6 34 7 SVD-L 6B 65 7 72 7 75 1 56 5 37 0 CBOW 6B 57 2 65 6 68 2 57 0 32 5 SG 6B 62 8 65 2 69 7 58 1 37 2 GloVe 6B 65 8 72 7 77 8 53 9 38 1 SVD-L 42B 74 0 76 4 74 1 58 3 39 9 GloVe 42B 75 9 83 6 82 9 59 6 47 8 CBOW 100B 68 4 79 6 75 4 59 4 45 5 L model on this larger corpus  SENTENCE_END
SENTENCE_START The fact that this basic SVD model does not scale well to large corpora lends further evidence to the necessity of the type of weighting scheme proposed in our model  SENTENCE_END
SENTENCE_START Table 3 shows results on five different word similarity datasets  SENTENCE_END
SENTENCE_START A similarity score is obtained from the word vectors by first normalizing each feature across the vocabulary and then calculating the cosine similarity  SENTENCE_END
SENTENCE_START We compute Spearman's rank correlation coefficient between this score and the human judgments  SENTENCE_END
SENTENCE_START CBOW denotes the vectors available on the word2vec website that are trained with word and phrase vectors on 100B words of news data  SENTENCE_END
SENTENCE_START GloVe outperforms it while using a corpus less than half the size  SENTENCE_END
SENTENCE_START Table 4 shows results on the NER task with the CRF-based model  SENTENCE_END
SENTENCE_START The L-BFGS training terminates when no improvement has been achieved on the dev set for 25 iterations  SENTENCE_END
SENTENCE_START Otherwise all configurations are identical to those used by Wang and Manning (2013)  SENTENCE_END
SENTENCE_START The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vector features  SENTENCE_END
SENTENCE_START In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al  SENTENCE_END
SENTENCE_START (2012) (HSMN) and Collobert and Weston (2008) (CW)  SENTENCE_END
SENTENCE_START We trained the CBOW model using the word2vec tool8  SENTENCE_END
SENTENCE_START The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better  SENTENCE_END
SENTENCE_START We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10  SENTENCE_END
SENTENCE_START Table 4: F1 score on NER task with 50d vectors  SENTENCE_END
SENTENCE_START Discrete is the baseline without word vectors  SENTENCE_END
SENTENCE_START We use publicly-available vectors for HPCA, HSMN, and CW  SENTENCE_END
SENTENCE_START See text for details  SENTENCE_END
SENTENCE_START Model Dev Test ACE MUC7 Discrete 91 0 85 4 77 4 73 4 SVD 90 8 85 7 77 3 73 7 SVD-S 91 0 85 5 77 6 74 3 SVD-L 90 5 84 8 73 6 71 5 HPCA 92 6 88 7 81 7 80 7 HSMN 90 5 85 7 78 7 74 7 CW 92 2 87 4 81 7 80 2 CBOW 93 1 88 2 82 2 81 1 GloVe 93 2 88 3 82 9 82 2 shown for neural vectors in (Turian et al , 2010)  SENTENCE_END
SENTENCE_START 4 4 Model Analysis: Vector Length and Context Size In Fig  SENTENCE_END
SENTENCE_START 2, we show the results of experiments that vary vector length and context window  SENTENCE_END
SENTENCE_START A context window that extends to the left and right of a target word will be called symmetric, and one which extends only to the left will be called asymmetric  SENTENCE_END
SENTENCE_START In (a), we observe diminishing returns for vectors larger than about 200 dimensions  SENTENCE_END
SENTENCE_START In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context windows  SENTENCE_END
SENTENCE_START Performance is better on the syntactic subtask for small and asymmetric context windows, which aligns with the intuition that syntactic information is mostly drawn from the immediate context and can depend strongly on word order  SENTENCE_END
SENTENCE_START Semantic information, on the other hand, is more frequently non-local, and more of it is captured with larger window sizes  SENTENCE_END
SENTENCE_START 4 5 Model Analysis: Corpus Size In Fig  SENTENCE_END
SENTENCE_START 3, we show performance on the word analogy task for 300-dimensional vectors trained on different corpora  SENTENCE_END
SENTENCE_START On the syntactic subtask, there is a monotonic increase in performance as the corpus size increases  SENTENCE_END
SENTENCE_START This is to be expected since larger corpora typically produce better statistics  SENTENCE_END
SENTENCE_START Interestingly, the same trend is not true for the semantic subtask, where the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus  SENTENCE_END
SENTENCE_START This is likely due to the large number of city- and country- based analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations  SENTENCE_END
SENTENCE_START Moreover, Wikipedia's 50 55 60 65 70 75 80 85 Overall Syntactic Semantic Wiki2010 1B tokens Accuracy [%] Wiki2014 1 6B tokens Gigaword5 4 3B tokens Gigaword5 + Wiki2014 6B tokens Common Crawl 42B tokens Figure 3: Accuracy on the analogy task for 300-dimensional vectors trained on different corpora  SENTENCE_END
SENTENCE_START entries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information  SENTENCE_END
SENTENCE_START 4 6 Model Analysis: Runtime The total runtime is split between populating X and training the model  SENTENCE_END
SENTENCE_START The former depends on many factors, including window size, vocabulary size, and corpus size  SENTENCE_END
SENTENCE_START Though we did not do so, this step could easily be parallelized across multiple machines (see, e g , Lebret and Collobert (2014) for some benchmarks)  SENTENCE_END
SENTENCE_START Using a single thread of a dual 2 1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric context window, a 400,000 word vocabulary, and a 6 billion token corpus takes about 85 minutes  SENTENCE_END
SENTENCE_START Given X, the time it takes to train the model depends on the vector size and the number of iterations  SENTENCE_END
SENTENCE_START For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes  SENTENCE_END
SENTENCE_START See Fig  SENTENCE_END
SENTENCE_START 4 for a plot of the learning curve  SENTENCE_END
SENTENCE_START 4 7 Model Analysis: Comparison with word2vec A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on performance  SENTENCE_END
SENTENCE_START We control for the main sources of variation that we identified in Sections 4 4 and 4 5 by setting the vector length, context window size, corpus, and vocabulary size to the configuration mentioned in the previous subsection  SENTENCE_END
SENTENCE_START The most important remaining variable to control for is training time  SENTENCE_END
SENTENCE_START For GloVe, the relevant parameter is the number of training iterations  SENTENCE_END
SENTENCE_START For word2vec, the obvious choice would be the number of training epochs  SENTENCE_END
SENTENCE_START Unfortunately, the code is currently designed for only a single epoch: GloVe Skip-Gram Accuracy [%] Iterations (GloVe) Negative Samples (Skip-Gram) Training Time (hrs) (b) GloVe vs Skip-Gram Figure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram (b)  SENTENCE_END
SENTENCE_START In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 + Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10  it specifies a learning schedule specific to a single pass through the data, making a modification for multiple passes a non-trivial task  SENTENCE_END
SENTENCE_START Another choice is to vary the number of negative samples  SENTENCE_END
SENTENCE_START Adding negative samples effectively increases the number of training words seen by the model, so in some ways it is analogous to extra epochs  SENTENCE_END
SENTENCE_START We set any unspecified parameters to their default values, assuming that they are close to opti- mal, though we acknowledge that this simplification should be relaxed in a more thorough analysis  SENTENCE_END
SENTENCE_START In Fig  SENTENCE_END
SENTENCE_START 4, we plot the overall performance on the analogy task as a function of training time  SENTENCE_END
SENTENCE_START The two x-axes at the bottom indicate the corresponding number of training iterations for GloVe and negative samples for word2vec  SENTENCE_END
SENTENCE_START We note that word2vec's performance actually decreases if the number of negative samples increases beyond about 10  SENTENCE_END
SENTENCE_START Presumably this is because the negative sampling method does not approximate the target probability distribution well 9 For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec  SENTENCE_END
SENTENCE_START It achieves better results faster, and also obtains the best results irrespective of speed  SENTENCE_END
SENTENCE_START 5 Conclusion Recently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based 9In contrast, noise-contrastive estimation is an approximation which improves with more negative samples  SENTENCE_END
SENTENCE_START In Table 1 of (Mnih et al , 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples  SENTENCE_END
SENTENCE_START methods or from prediction-based methods  SENTENCE_END
SENTENCE_START Currently, prediction-based models garner substantial support; for example, Baroni et al  SENTENCE_END
SENTENCE_START (2014) argue that these models perform better across a range of tasks  SENTENCE_END
SENTENCE_START In this work we argue that the two classes of methods are not dramatically different at a fundamental level since they both probe the underlying co-occurrence statistics of the corpus, but the efficiency with which the count-based methods capture global statistics can be advantageous  SENTENCE_END
SENTENCE_START We construct a model that utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec  SENTENCE_END
SENTENCE_START The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks  SENTENCE_END
SENTENCE_START Acknowledgments We thank the anonymous reviewers for their valuable comments  SENTENCE_END
SENTENCE_START Stanford University gratefully acknowledges the support of the Defense Threat Reduction Agency (DTRA) under Air Force Research Laboratory (AFRL) contract no  SENTENCE_END
SENTENCE_START FA8650-10-C-7020 and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under AFRL contract no  SENTENCE_END
SENTENCE_START FA8750-13-2-0040  SENTENCE_END
SENTENCE_START Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DTRA, AFRL, DEFT, or the US government  SENTENCE_END







SENTENCE_START A Critical Review of Recurrent Neural Networks for Sequence Learning Zachary C  Lipton zlipton@cs ucsd edu John Berkowitz jaberkow@physics ucsd edu Charles Elkan elkan@cs ucsd edu June 5th, 2015 Abstract Countless learning tasks require dealing with sequential data  SENTENCE_END
SENTENCE_START Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences  SENTENCE_END
SENTENCE_START In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences  SENTENCE_END
SENTENCE_START Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities  SENTENCE_END
SENTENCE_START Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes  SENTENCE_END
SENTENCE_START Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window  SENTENCE_END
SENTENCE_START Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them  SENTENCE_END
SENTENCE_START In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition  SENTENCE_END
SENTENCE_START In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models  SENTENCE_END
SENTENCE_START When appropriate, we reconcile conflicting notation and nomenclature  SENTENCE_END
SENTENCE_START Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research  SENTENCE_END
SENTENCE_START 1 Introduction Neural networks are powerful learning models that achieve state-of-the-art reults in a wide range of supervised and unsupervised machine learning tasks  SENTENCE_END
SENTENCE_START 1 arXiv:1506 00019v4 [cs LG] 17 Oct 2015 They are suited especially well for machine perception tasks, where the raw underlying features are not individually interpretable  SENTENCE_END
SENTENCE_START This success is attributed to their ability to learn hierarchical representations, unlike traditional methods that rely upon hand-engineered features [Farabet et al , 2013]  SENTENCE_END
SENTENCE_START Over the past several years, storage has become more affordable, datasets have grown far larger, and the field of parallel computing has advanced considerably  SENTENCE_END
SENTENCE_START In the setting of large datasets, simple linear models tend to under-fit, and often under-utilize computing resources  SENTENCE_END
SENTENCE_START Deep learning methods, in particular those based on deep belief networks (DNNs), which are greedily built by stacking restricted Boltzmann machines, and convolutional neural networks, which exploit the local dependency of visual information, have demonstrated record-setting results on many important applications  SENTENCE_END
SENTENCE_START However, despite their power, standard neural networks have limitations  SENTENCE_END
SENTENCE_START Most notably, they rely on the assumption of independence among the training and test examples  SENTENCE_END
SENTENCE_START After each example (data point) is processed, the entire state of the network is lost  SENTENCE_END
SENTENCE_START If each example is generated independently, this presents no problem  SENTENCE_END
SENTENCE_START But if data points are related in time or space, this is unacceptable  SENTENCE_END
SENTENCE_START Frames from video, snippets of audio, and words pulled from sentences, represent settings where the independence assumption fails  SENTENCE_END
SENTENCE_START Additionally, standard networks generally rely on examples being vectors of fixed length  SENTENCE_END
SENTENCE_START Thus it is desirable to extend these powerful learning tools to model data with temporal or sequential structure and varying length inputs and outputs, especially in the many domains where neural networks are already the state of the art  SENTENCE_END
SENTENCE_START Recurrent neural networks (RNNs) are connectionist models with the ability to selectively pass information across sequence steps, while processing sequential data one element at a time  SENTENCE_END
SENTENCE_START Thus they can model input and/or output consisting of sequences of elements that are not independent  SENTENCE_END
SENTENCE_START Further, recurrent neural networks can simultaneously model sequential and time dependencies on multiple scales  SENTENCE_END
SENTENCE_START In the following subsections, we explain the fundamental reasons why recurrent neural networks are worth investigating  SENTENCE_END
SENTENCE_START To be clear, we are motivated by a desire to achieve empirical results  SENTENCE_END
SENTENCE_START This motivation warrants clarification because recurrent networks have roots in both cognitive modeling and supervised machine learning  SENTENCE_END
SENTENCE_START Owing to this difference of perspectives, many published papers have different aims and priorities  SENTENCE_END
SENTENCE_START In many foundational papers, generally published in cognitive science and computational neuroscience journals, such as [Hopfield, 1982, Jordan, 1986, Elman, 1990], biologically plausible mechanisms are emphasized  SENTENCE_END
SENTENCE_START In other papers [Schuster and Paliwal, 1997, Socher et al , 2014, Karpathy and Fei-Fei, 2014], biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets  SENTENCE_END
SENTENCE_START This review is motivated by practical results rather than biological plausibility, but where appropriate, we draw connections to relevant concepts in neuroscience  SENTENCE_END
SENTENCE_START Given the empirical aim, we now address three significant questions that one might reasonably want answered before reading further  SENTENCE_END
SENTENCE_START 2 1 1 Why model sequentiality explicitly? SENTENCE_END
SENTENCE_START In light of the practical success and economic value of sequence-agnostic models, this is a fair question  SENTENCE_END
SENTENCE_START Support vector machines, logistic regression, and feedforward networks have proved immensely useful without explicitly modeling time  SENTENCE_END
SENTENCE_START Arguably, it is precisely the assumption of independence that has led to much recent progress in machine learning  SENTENCE_END
SENTENCE_START Further, many models implicitly capture time by concatenating each input with some number of its immediate predecessors and successors, presenting the machine learning model with a sliding window of context about each point of interest  SENTENCE_END
SENTENCE_START This approach has been used with deep belief nets for speech modeling by Maas et al  SENTENCE_END
SENTENCE_START [2012]  SENTENCE_END
SENTENCE_START Unfortunately, despite the usefulness of the independence assumption, it precludes modeling long-range dependencies  SENTENCE_END
SENTENCE_START For example, a model trained using a finite-length context window of length 5 could never be trained to answer the simple question, "what was the data point seen six time steps ago?" SENTENCE_END
SENTENCE_START For a practical application such as call center automation, such a limited system might learn to route calls, but could never participate with complete success in an extended dialogue  SENTENCE_END
SENTENCE_START Since the earliest conception of artificial intelligence, researchers have sought to build systems that interact with humans in time  SENTENCE_END
SENTENCE_START In Alan Turing's groundbreaking paper Computing Machinery and Intelligence, he proposes an "imitation game" which judges a machine's intelligence by its ability to convincingly engage in dialogue [Turing, 1950]  SENTENCE_END
SENTENCE_START Besides dialogue systems, modern interactive systems of economic importance include self-driving cars and robotic surgery, among others  SENTENCE_END
SENTENCE_START Without an explicit model of sequentiality or time, it seems unlikely that any combination of classifiers or regressors can be cobbled together to provide this functionality  SENTENCE_END
SENTENCE_START 1 2 Why not use Markov models? SENTENCE_END
SENTENCE_START Recurrent neural networks are not the only models capable of representing time dependencies  SENTENCE_END
SENTENCE_START Markov chains, which model transitions between states in an observed sequence, were first described by the mathematician Andrey Markov in 1906  SENTENCE_END
SENTENCE_START Hidden Markov models (HMMs), which model an observed sequence as probabilistically dependent upon a sequence of unobserved states, were described in the 1950s and have been widely studied since the 1960s [Stratonovich, 1960]  SENTENCE_END
SENTENCE_START However, traditional Markov model approaches are limited because their states must be drawn from a modestly sized discrete state space S  The dynamic programming algorithm that is used to perform efficient inference with hidden Markov models scales in time O(|S|2 ) [Viterbi, 1967]  SENTENCE_END
SENTENCE_START Further, the transition table capturing the probability of moving between any two time-adjacent states is of size |S|2   SENTENCE_END
SENTENCE_START Thus, standard operations become infeasible with an HMM when the set of possible hidden states grows large  SENTENCE_END
SENTENCE_START Further, each hidden state can depend only on the immediately previous state  SENTENCE_END
SENTENCE_START While it is possible to extend a Markov model to account for a larger context window by creating a new state space equal to the cross product of the possible states at each time in the window, this procedure grows the state space exponentially with the size of the 3 window, rendering Markov models computationally impractical for modeling long-range dependencies [Graves et al , 2014]  SENTENCE_END
SENTENCE_START Given the limitations of Markov models, we ought to explain why it is reasonable that connectionist models, i e , artificial neural networks, should fare better  SENTENCE_END
SENTENCE_START First, recurrent neural networks can capture long-range time dependencies, overcoming the chief limitation of Markov models  SENTENCE_END
SENTENCE_START This point requires a careful explanation  SENTENCE_END
SENTENCE_START As in Markov models, any state in a traditional RNN depends only on the current input as well as on the state of the network at the previous time step 1 However, the hidden state at any time step can contain information from a nearly arbitrarily long context window  SENTENCE_END
SENTENCE_START This is possible because the number of distinct states that can be represented in a hidden layer of nodes grows exponentially with the number of nodes in the layer  SENTENCE_END
SENTENCE_START Even if each node took only binary values, the network could represent 2N states where N is the number of nodes in the hidden layer  SENTENCE_END
SENTENCE_START When the value of each node is a real number, a network can represent even more distinct states  SENTENCE_END
SENTENCE_START While the potential expressive power of a network grows exponentially with the number of nodes, the complexity of both inference and training grows at most quadratically  SENTENCE_END
SENTENCE_START 1 3 Are RNNs too expressive? SENTENCE_END
SENTENCE_START Finite-sized RNNs with nonlinear activations are a rich family of models, capable of nearly arbitrary computation  SENTENCE_END
SENTENCE_START A well-known result is that a finite-sized recurrent neural network with sigmoidal activation functions can simulate a universal Turing machine [Siegelmann and Sontag, 1991]  SENTENCE_END
SENTENCE_START The capability of RNNs to perform arbitrary computation demonstrates their expressive power, but one could argue that the C programming language is equally capable of expressing arbitrary programs  SENTENCE_END
SENTENCE_START And yet there are no papers claiming that the invention of C represents a panacea for machine learning  SENTENCE_END
SENTENCE_START A fundamental reason is there is no simple way of efficiently exploring the space of C programs  SENTENCE_END
SENTENCE_START In particular, there is no general way to calculate the gradient of an arbitrary C program to minimize a chosen loss function  SENTENCE_END
SENTENCE_START Moreover, given any finite dataset, there exist countless programs which overfit the dataset, generating desired training output but failing to generalize to test examples  SENTENCE_END
SENTENCE_START Why then should RNNs suffer less from similar problems? SENTENCE_END
SENTENCE_START First, given any fixed architecture (set of nodes, edges, and activation functions), the recurrent neural networks with this architecture are differentiable end to end  SENTENCE_END
SENTENCE_START The derivative of the loss function can be calculated with respect to each of the parameters (weights) in the model  SENTENCE_END
SENTENCE_START Thus, RNNs are amenable to gradient-based training  SENTENCE_END
SENTENCE_START Second, while the Turing-completeness of RNNs is an impressive property, given a fixed-size RNN with a specific architecture, it is not actually possible to reproduce any arbitrary program  SENTENCE_END
SENTENCE_START Further, unlike a program composed in C, a recurrent neural network can be regularized via standard techniques that help 1 While traditional RNNs only model the dependence of the current state on the previous state, bidirectional recurrent neural networks (BRNNs) [Schuster and Paliwal, 1997] extend RNNs to model dependence on both past states and future states  SENTENCE_END
SENTENCE_START 4 prevent overfitting, such as weight decay, dropout, and limiting the degrees of freedom  SENTENCE_END
SENTENCE_START 1 4 Comparison to prior literature The literature on recurrent neural networks can seem impenetrable to the uninitiated  SENTENCE_END
SENTENCE_START Shorter papers assume familiarity with a large body of background lit- erature, while diagrams are frequently underspecified, failing to indicate which edges span time steps and which do not  SENTENCE_END
SENTENCE_START Jargon abounds, and notation is inconsistent across papers or overloaded within one paper  SENTENCE_END
SENTENCE_START Readers are frequently in the unenviable position of having to synthesize conflicting information across many papers in order to understand just one  SENTENCE_END
SENTENCE_START For example, in many papers subscripts index both nodes and time steps  SENTENCE_END
SENTENCE_START In others, h simultaneously stands for a link function and a layer of hidden nodes  SENTENCE_END
SENTENCE_START The variable t simultaneously stands for both time indices and targets, sometimes in the same equation  SENTENCE_END
SENTENCE_START Many excellent research papers have appeared recently, but clear reviews of the recurrent neural network literature are rare  SENTENCE_END
SENTENCE_START Among the most useful resources are a recent book on supervised sequence labeling with recurrent neural networks [Graves, 2012] and an earlier doctoral thesis [Gers, 2001]  SENTENCE_END
SENTENCE_START A recent survey covers recurrent neural nets for language modeling [De Mulder et al , 2015]  SENTENCE_END
SENTENCE_START Various authors focus on specific technical aspects; for example Pearlmutter [1995] surveys gradient calculations in continuous time recurrent neural networks  SENTENCE_END
SENTENCE_START In the present review paper, we aim to provide a readable, intuitive, consistently notated, and reasonably comprehensive but selective survey of research on recurrent neural networks for learning with sequences  SENTENCE_END
SENTENCE_START We emphasize architectures, algorithms, and results, but we aim also to distill the intuitions that have guided this largely heuristic and empirical field  SENTENCE_END
SENTENCE_START In addition to concrete modeling details, we offer qualitative arguments, a historical perspective, and comparisons to alternative methodologies where appropriate  SENTENCE_END
SENTENCE_START 2 Background This section introduces formal notation and provides a brief background on neural networks in general  SENTENCE_END
SENTENCE_START 2 1 Sequences The input to an RNN is a sequence, and/or its target is a sequence  SENTENCE_END
SENTENCE_START An input sequence can be denoted (x(1) , x(2) ,    , x(T ) ) where each data point x(t) is a real-valued vector  SENTENCE_END
SENTENCE_START Similarly, a target sequence can be denoted (y(1) , y(2) ,    , y(T ) )  SENTENCE_END
SENTENCE_START A training set typically is a set of examples where each example is an (input sequence, target sequence) pair, although commonly either the input or the output may be a single data point  SENTENCE_END
SENTENCE_START Sequences may be of finite or countably infinite length  SENTENCE_END
SENTENCE_START When they are finite, the maximum time index of the sequence 5 is called T  RNNs are not limited to time-based sequences  SENTENCE_END
SENTENCE_START They have been used successfully on non-temporal sequence data, including genetic data [Baldi and Pollastri, 2003]  SENTENCE_END
SENTENCE_START However, in many important applications of RNNs, the sequences have an explicit or implicit temporal aspect  SENTENCE_END
SENTENCE_START While we often refer to time in this survey, the methods described here are applicable to non-temporal as well as to temporal tasks  SENTENCE_END
SENTENCE_START Using temporal terminology, an input sequence consists of data points x(t) that arrive in a discrete sequence of time steps indexed by t  A target sequence consists of data points y(t)   SENTENCE_END
SENTENCE_START We use superscripts with parentheses for time, and not subscripts, to prevent confusion between sequence steps and indices of nodes in a network  SENTENCE_END
SENTENCE_START When a model produces predicted data points, these are labeled ^ y(t)   SENTENCE_END
SENTENCE_START The time-indexed data points may be equally spaced samples from a continuous real-world process  SENTENCE_END
SENTENCE_START Examples include the still images that comprise the frames of videos or the discrete amplitudes sampled at fixed intervals that comprise audio recordings  SENTENCE_END
SENTENCE_START The time steps may also be ordinal, with no exact correspondence to durations  SENTENCE_END
SENTENCE_START In fact, RNNs are frequently applied to domains where sequences have a defined order but no explicit notion of time  SENTENCE_END
SENTENCE_START This is the case with natural language  SENTENCE_END
SENTENCE_START In the word sequence "John Coltrane plays the saxophone", x(1) = John, x(2) = Coltrane, etc  SENTENCE_END
SENTENCE_START 2 2 Neural networks Neural networks are biologically inspired models of computation  SENTENCE_END
SENTENCE_START Generally, a neural network consists of a set of artificial neurons, commonly referred to as nodes or units, and a set of directed edges between them, which intuitively represent the synapses in a biological neural network  SENTENCE_END
SENTENCE_START Associated with each neuron j is an activation function lj(), which is sometimes called a link function  SENTENCE_END
SENTENCE_START We use the notation lj and not hj, unlike some other papers, to distinguish the activation function from the values of the hidden nodes in a network, which, as a vector, is commonly notated h in the literature  SENTENCE_END
SENTENCE_START Associated with each edge from node j to j is a weight wjj   SENTENCE_END
SENTENCE_START Following the convention adopted in several foundational papers [Hochreiter and Schmidhuber, 1997, Gers et al , 2000, Gers, 2001, Sutskever et al , 2011], we index neurons with j and j , and wjj denotes the "to-from" weight corresponding to the directed edge to node j from node j   SENTENCE_END
SENTENCE_START It is important to note that in many references the indices are flipped and wj j = wjj denotes the "from-to" weight on the directed edge from the node j to the node j, as in lecture notes by Elkan [2015] and in Wikipedia [2015]  SENTENCE_END
SENTENCE_START The value vj of each neuron j is calculated by applying its activation function to a weighted sum of the values of its input nodes (Figure 1): vj = lj j wjj  vj   SENTENCE_END
SENTENCE_START For convenience, we term the weighted sum inside the parentheses the incoming 6 Figure 1: An artificial neuron computes a nonlinear function of a weighted sum of its inputs  SENTENCE_END
SENTENCE_START activation and notate it as aj  SENTENCE_END
SENTENCE_START We represent this computation in diagrams by depicting neurons as circles and edges as arrows connecting them  SENTENCE_END
SENTENCE_START When appropriate, we indicate the exact activation function with a symbol, e g ,  for sigmoid  SENTENCE_END
SENTENCE_START Common choices for the activation function include the sigmoid (z) = 1/(1 + e-z ) and the tanh function (z) = (ez - e-z)/(ez + e-z)  SENTENCE_END
SENTENCE_START The latter has become common in feedforward neural nets and was applied to recurrent nets by Sutskever et al  SENTENCE_END
SENTENCE_START [2011]  SENTENCE_END
SENTENCE_START Another activation function which has become prominent in deep learning research is the rectified linear unit (ReLU) whose formula is lj(z) = max(0, z)  SENTENCE_END
SENTENCE_START This type of unit has been demonstrated to improve the performance of many deep neural networks [Nair and Hinton, 2010, Maas et al , 2012, Zeiler et al , 2013] on tasks as varied as speech processing and object recognition, and has been used in recurrent neural networks by Bengio et al  SENTENCE_END
SENTENCE_START [2013]  SENTENCE_END
SENTENCE_START The activation function at the output nodes depends upon the task  SENTENCE_END
SENTENCE_START For multiclass classification with K alternative classes, we apply a softmax nonlinearity in an output layer of K nodes  SENTENCE_END
SENTENCE_START The softmax function calculates ^ yk = eak K k =1 eak for k = 1 to k = K  The denominator is a normalizing term consisting of the sum of the numerators, ensuring that the outputs of all nodes sum to one  SENTENCE_END
SENTENCE_START For multilabel classification the activation function is simply a point-wise sigmoid, and for regression we typically have linear output  SENTENCE_END
SENTENCE_START 7 Figure 2: A feedforward neural network  SENTENCE_END
SENTENCE_START An example is presented to the network by setting the values of the blue (bottom) nodes  SENTENCE_END
SENTENCE_START The values of the nodes in each layer are computed successively as a function of the prior layers until output is produced at the topmost layer  SENTENCE_END
SENTENCE_START 2 3 Feedforward networks and backpropagation With a neural model of computation, one must determine the order in which computation should proceed  SENTENCE_END
SENTENCE_START Should nodes be sampled one at a time and updated, or should the value of all nodes be calculated at once and then all updates applied simultaneously? SENTENCE_END
SENTENCE_START Feedforward networks (Figure 2) are a restricted class of networks which deal with this problem by forbidding cycles in the directed graph of nodes  SENTENCE_END
SENTENCE_START Given the absence of cycles, all nodes can be arranged into layers, and the outputs in each layer can be calculated given the outputs from the lower layers  SENTENCE_END
SENTENCE_START The input x to a feedforward network is provided by setting the values of the lowest layer  SENTENCE_END
SENTENCE_START Each higher layer is then successively computed until output is generated at the topmost layer ^ y  Feedforward networks are frequently used for supervised learning tasks such as classification and regression  SENTENCE_END
SENTENCE_START Learning is accomplished by iteratively updating each of the weights to minimize a loss function, L(^ y, y), which penalizes the distance between the output ^ y and the target y  SENTENCE_END
SENTENCE_START The most successful algorithm for training neural networks is backpropagation, introduced for this purpose by Rumelhart et al  SENTENCE_END
SENTENCE_START [1985]  SENTENCE_END
SENTENCE_START Backpropagation uses the chain rule to calculate the derivative of the loss function L with respect to each parameter in the network  SENTENCE_END
SENTENCE_START The weights are then adjusted by gradient descent  SENTENCE_END
SENTENCE_START Because the loss surface is non-convex, there is no assurance that backpropagation will reach a global minimum  SENTENCE_END
SENTENCE_START Moreover, exact optimization is known to be an NP-hard problem  SENTENCE_END
SENTENCE_START However, a large body of work on heuristic 8 pre-training and optimization techniques has led to impressive empirical success on many supervised learning tasks  SENTENCE_END
SENTENCE_START In particular, convolutional neural networks, popularized by Le Cun et al  SENTENCE_END
SENTENCE_START [1990], are a variant of feedforward neural network that holds records since 2012 in many computer vision tasks such as object detection [Krizhevsky et al , 2012]  SENTENCE_END
SENTENCE_START Nowadays, neural networks are usually trained with stochastic gradient descent (SGD) using mini-batches  SENTENCE_END
SENTENCE_START With batch size equal to one, the stochastic gradient update equation is w  w - wFi where  is the learning rate and wFi is the gradient of the objective function with respect to the parameters w as calculated on a single example (xi, yi)  SENTENCE_END
SENTENCE_START Many variants of SGD are used to accelerate learning  SENTENCE_END
SENTENCE_START Some popular heuristics, such as AdaGrad [Duchi et al , 2011], AdaDelta [Zeiler, 2012], and RMSprop [Tieleman and Hinton, 2012], tune the learning rate adaptively for each feature  SENTENCE_END
SENTENCE_START AdaGrad, arguably the most popular, adapts the learning rate by caching the sum of squared gradients with respect to each parameter at each time step  SENTENCE_END
SENTENCE_START The step size for each feature is multiplied by the inverse of the square root of this cached value  SENTENCE_END
SENTENCE_START AdaGrad leads to fast convergence on convex error surfaces, but because the cached sum is monotonically increasing, the method has a monotonically decreasing learning rate, which may be undesirable on highly non-convex loss surfaces  SENTENCE_END
SENTENCE_START RMSprop modifies AdaGrad by introducing a decay factor in the cache, changing the monotonically growing value into a moving average  SENTENCE_END
SENTENCE_START Momentum methods are another common SGD variant used to train neural networks  SENTENCE_END
SENTENCE_START These methods add to each update a decaying sum of the previous updates  SENTENCE_END
SENTENCE_START When the momentum parameter is tuned well and the network is initialized well, momentum methods can train deep nets and recurrent nets competitively with more computationally expensive methods like the Hessian-free optimizer of Sutskever et al  SENTENCE_END
SENTENCE_START [2013]  SENTENCE_END
SENTENCE_START To calculate the gradient in a feedforward neural network, backpropagation proceeds as follows  SENTENCE_END
SENTENCE_START First, an example is propagated forward through the network to produce a value vj at each node and outputs ^ y at the topmost layer  SENTENCE_END
SENTENCE_START Then, a loss function value L(^ yk, yk) is computed at each output node k  Subsequently, for each output node k, we calculate k = L(^ yk, yk) ^ yk  lk(ak)  SENTENCE_END
SENTENCE_START Given these values k, for each node in the immediately prior layer we calculate j = l (aj) k k  wkj  SENTENCE_END
SENTENCE_START This calculation is performed successively for each lower layer to yield j for every node j given the  values for each node connected to j by an outgoing edge  SENTENCE_END
SENTENCE_START Each value j represents the derivative L/aj of the total loss function with respect to that node's incoming activation  SENTENCE_END
SENTENCE_START Given the values vj calculated 9 during the forward pass, and the values j calculated during the backward pass, the derivative of the loss L with respect a given parameter wjj is L wjj = jvj   SENTENCE_END
SENTENCE_START Other methods have been explored for learning the weights in a neural network  SENTENCE_END
SENTENCE_START A number of papers from the 1990s [Belew et al , 1990, Gruau et al , 1994] championed the idea of learning neural networks with genetic algorithms, with some even claiming that achieving success on real-world problems only by applying many small changes to the weights of a network was impossible  SENTENCE_END
SENTENCE_START Despite the subsequent success of backpropagation, interest in genetic algorithms continues  SENTENCE_END
SENTENCE_START Several recent papers explore genetic algorithms for neural networks, especially as a means of learning the architecture of neural networks, a problem not addressed by backpropagation [Bayer et al , 2009, Harp and Samad, 2013]  SENTENCE_END
SENTENCE_START By architecture we mean the number of layers, the number of nodes in each, the connectivity pattern among the layers, the choice of activation functions, etc  SENTENCE_END
SENTENCE_START One open question in neural network research is how to exploit sparsity in training  SENTENCE_END
SENTENCE_START In a neural network with sigmoidal or tanh activation functions, the nodes in each layer never take value exactly zero  SENTENCE_END
SENTENCE_START Thus, even if the inputs are sparse, the nodes at each hidden layer are not  SENTENCE_END
SENTENCE_START However, rectified linear units (ReLUs) introduce sparsity to hidden layers [Glorot et al , 2011]  SENTENCE_END
SENTENCE_START In this setting, a promising path may be to store the sparsity pattern when computing each layer's values and use it to speed up computation of the next layer in the network  SENTENCE_END
SENTENCE_START Some recent work shows that given sparse inputs to a linear model with a standard regularizer, sparsity can be fully exploited even if regularization makes the gradient be not sparse [Carpenter, 2008, Langford et al , 2009, Singer and Duchi, 2009, Lipton and Elkan, 2015]  SENTENCE_END
SENTENCE_START 3 Recurrent neural networks Recurrent neural networks are feedforward neural networks augmented by the inclusion of edges that span adjacent time steps, introducing a notion of time to the model  SENTENCE_END
SENTENCE_START Like feedforward networks, RNNs may not have cycles among conventional edges  SENTENCE_END
SENTENCE_START However, edges that connect adjacent time steps, called recurrent edges, may form cycles, including cycles of length one that are self-connections from a node to itself across time  SENTENCE_END
SENTENCE_START At time t, nodes with recurrent edges receive input from the current data point x(t) and also from hidden node values h(t-1) in the network's previous state  SENTENCE_END
SENTENCE_START The output ^ y(t) at each time t is calculated given the hidden node values h(t) at time t  Input x(t-1) at time t - 1 can influence the output ^ y(t) at time t and later by way of the recurrent connections  SENTENCE_END
SENTENCE_START Two equations specify all calculations necessary for computation at each time step on the forward pass in a simple recurrent neural network as in Figure 3: h(t) = (Whxx(t) + Whhh(t-1) + bh) 10 Figure 3: A simple recurrent network  SENTENCE_END
SENTENCE_START At each time step t, activation is passed along solid edges as in a feedforward network  SENTENCE_END
SENTENCE_START Dashed edges connect a source node at each time t to a target node at each following time t + 1  SENTENCE_END
SENTENCE_START ^ y(t) = softmax(Wyhh(t) + by)  SENTENCE_END
SENTENCE_START Here Whx is the matrix of conventional weights between the input and the hidden layer and Whh is the matrix of recurrent weights between the hidden layer and itself at adjacent time steps  SENTENCE_END
SENTENCE_START The vectors bh and by are bias parameters which allow each node to learn an offset  SENTENCE_END
SENTENCE_START The dynamics of the network depicted in Figure 3 across time steps can be visualized by unfolding it as in Figure 4  SENTENCE_END
SENTENCE_START Given this picture, the network can be interpreted not as cyclic, but rather as a deep network with one layer per time step and shared weights across time steps  SENTENCE_END
SENTENCE_START It is then clear that the unfolded network can be trained across many time steps using backpropagation  SENTENCE_END
SENTENCE_START This algorithm, called backpropagation through time (BPTT), was introduced by Werbos [1990]  SENTENCE_END
SENTENCE_START All recurrent networks in common current use apply it  SENTENCE_END
SENTENCE_START 3 1 Early recurrent network designs The foundational research on recurrent networks took place in the 1980s  SENTENCE_END
SENTENCE_START In 1982, Hopfield introduced a family of recurrent neural networks that have pattern recognition capabilities [Hopfield, 1982]  SENTENCE_END
SENTENCE_START They are defined by the values of the weights between nodes and the link functions are simple thresholding at zero  SENTENCE_END
SENTENCE_START In these nets, a pattern is placed in the network by setting the values of the nodes  SENTENCE_END
SENTENCE_START The network then runs for some time according to its update rules, and eventually another pattern is read out  SENTENCE_END
SENTENCE_START Hopfield networks are useful for recovering a stored pattern from a corrupted version and are the forerunners of Boltzmann machines and auto-encoders  SENTENCE_END
SENTENCE_START 11 Figure 4: The recurrent network of Figure 3 unfolded across time steps  SENTENCE_END
SENTENCE_START An early architecture for supervised learning on sequences was introduced by Jordan [1986]  SENTENCE_END
SENTENCE_START Such a network (Figure 5) is a feedforward network with a single hidden layer that is extended with special units 2 Output node values are fed to the special units, which then feed these values to the hidden nodes at the following time step  SENTENCE_END
SENTENCE_START If the output values are actions, the special units allow the network to remember actions taken at previous time steps  SENTENCE_END
SENTENCE_START Several modern architectures use a related form of direct transfer from output nodes; Sutskever et al  SENTENCE_END
SENTENCE_START [2014] translates sentences between natural languages, and when generating a text sequence, the word chosen at each time step is fed into the network as input at the following time step  SENTENCE_END
SENTENCE_START Additionally, the special units in a Jordan network are self-connected  SENTENCE_END
SENTENCE_START Intuitively, these edges allow sending information across multiple time steps without perturbing the output at each intermediate time step  SENTENCE_END
SENTENCE_START The architecture introduced by Elman [1990] is simpler than the earlier Jordan architecture  SENTENCE_END
SENTENCE_START Associated with each unit in the hidden layer is a context unit  SENTENCE_END
SENTENCE_START Each such unit j takes as input the state of the corresponding hidden node j at the previous time step, along an edge of fixed weight wj j = 1  SENTENCE_END
SENTENCE_START This value then feeds back into the same hidden node j along a standard edge  SENTENCE_END
SENTENCE_START This architecture is equivalent to a simple RNN in which each hidden node has a single self-connected recurrent edge  SENTENCE_END
SENTENCE_START The idea of fixed-weight recurrent edges that make hidden nodes self-connected is fundamental in subsequent work on LSTM networks [Hochreiter and Schmidhuber, 1997]  SENTENCE_END
SENTENCE_START Elman [1990] trains the network using backpropagation and demonstrates that the network can learn time dependencies  SENTENCE_END
SENTENCE_START The paper features two sets of experiments  SENTENCE_END
SENTENCE_START The first extends the logical operation exclusive or (XOR) to 2 Jordan [1986] calls the special units "state units" while Elman [1990] calls a corresponding structure "context units " SENTENCE_END
SENTENCE_START In this paper we simplify terminology by using only "context units"  SENTENCE_END
SENTENCE_START 12 Figure 5: A recurrent neural network as proposed by Jordan [1986]  SENTENCE_END
SENTENCE_START Output units are connected to special units that at the next time step feed into themselves and into hidden units  SENTENCE_END
SENTENCE_START the time domain by concatenating sequences of three tokens  SENTENCE_END
SENTENCE_START For each three-token segment, e g  SENTENCE_END
SENTENCE_START "011", the first two tokens ("01") are chosen randomly and the third ("1") is set by performing xor on the first two  SENTENCE_END
SENTENCE_START Random guessing should achieve accuracy of 50%  SENTENCE_END
SENTENCE_START A perfect system should perform the same as random for the first two tokens, but guess the third token perfectly, achieving accuracy of 66 7%  SENTENCE_END
SENTENCE_START The simple network of Elman [1990] does in fact approach this maximum achievable score  SENTENCE_END
SENTENCE_START 3 2 Training recurrent networks Learning with recurrent networks has long been considered to be difficult  SENTENCE_END
SENTENCE_START Even for standard feedforward networks, the optimization task is NP-complete Blum and Rivest [1993]  SENTENCE_END
SENTENCE_START But learning with recurrent networks can be especially challenging due to the difficulty of learning long-range dependencies, as described by Bengio et al  SENTENCE_END
SENTENCE_START [1994] and expanded upon by Hochreiter et al  SENTENCE_END
SENTENCE_START [2001]  SENTENCE_END
SENTENCE_START The problems of vanishing and exploding gradients occur when backpropagating errors across many time steps  SENTENCE_END
SENTENCE_START As a toy example, consider a network with a single input node, a single output node, and a single recurrent hidden node (Figure 7)  SENTENCE_END
SENTENCE_START Now consider an input passed to the network at time  and an error calculated at time t, assuming input of zero in the intervening time steps  SENTENCE_END
SENTENCE_START The tying of weights across time steps means that the recurrent edge at the hidden node j always has the same weight  SENTENCE_END
SENTENCE_START Therefore, the contribution of the input at time to the output at time t will either explode or approach zero, exponentially fast, as t -  grows large  SENTENCE_END
SENTENCE_START Hence the derivative of the error with respect to the input will either explode or vanish  SENTENCE_END
SENTENCE_START 13 Figure 6: A recurrent neural network as described by Elman [1990]  SENTENCE_END
SENTENCE_START Hidden units are connected to context units, which feed back into the hidden units at the next time step  SENTENCE_END
SENTENCE_START Which of the two phenomena occurs depends on whether the weight of the recurrent edge |wjj| > 1 or |wjj| < 1 and on the activation function in the hidden node (Figure 8)  SENTENCE_END
SENTENCE_START Given a sigmoid activation function, the vanishing gradient problem is more pressing, but with a rectified linear unit max(0, x), it is easier to imagine the exploding gradient  SENTENCE_END
SENTENCE_START Pascanu et al  SENTENCE_END
SENTENCE_START [2012] give a thorough mathematical treatment of the vanishing and exploding gradient problems, characterizing exact conditions under which these problems may occur  SENTENCE_END
SENTENCE_START Given these conditions, they suggest an approach to training via a regularization term that forces the weights to values where the gradient neither vanishes nor explodes  SENTENCE_END
SENTENCE_START Truncated backpropagation through time (TBPTT) is one solution to the exploding gradient problem for continuously running networks [Williams and Zipser, 1989]  SENTENCE_END
SENTENCE_START With TBPTT, some maximum number of time steps is set along which error can be propagated  SENTENCE_END
SENTENCE_START While TBPTT with a small cutoff can be used to alleviate the exploding gradient problem, it requires that one sacrifice the ability to learn long-range dependencies  SENTENCE_END
SENTENCE_START The LSTM architecture described below uses carefully designed nodes with recurrent edges with fixed unit weight as a solution to the vanishing gradient problem  SENTENCE_END
SENTENCE_START The issue of local optima is an obstacle to effective training that cannot be dealt with simply by modifying the network architecture  SENTENCE_END
SENTENCE_START Optimizing even a single hidden-layer feedforward network is an NP-complete problem [Blum and Rivest, 1993]  SENTENCE_END
SENTENCE_START However, recent empirical and theoretical studies suggest that in practice, the issue may not be as important as once thought  SENTENCE_END
SENTENCE_START Dauphin et al  SENTENCE_END
SENTENCE_START [2014] show that while many critical points exist on the error surfaces of large neural networks, the ratio of saddle points to true local minima increases exponentially with the size of the network, and algorithms can be designed to 14 Figure 7: A simple recurrent net with one input unit, one output unit, and one recurrent hidden unit  SENTENCE_END
SENTENCE_START Figure 8: A visualization of the vanishing gradient problem, using the network depicted in Figure 7, adapted from Graves [2012]  SENTENCE_END
SENTENCE_START If the weight along the recurrent edge is less than one, the contribution of the input at the first time step to the output at the final time step will decrease exponentially fast as a function of the length of the time interval in between  SENTENCE_END
SENTENCE_START 15 escape from saddle points  SENTENCE_END
SENTENCE_START Overall, along with the improved architectures explained below, fast implementations and better gradient-following heuristics have rendered RNN training feasible  SENTENCE_END
SENTENCE_START Implementations of forward and backward propagation using GPUs, such as the Theano [Bergstra et al , 2010] and Torch [Collobert et al , 2011] packages, have made it straightforward to implement fast training algorithms  SENTENCE_END
SENTENCE_START In 1996, prior to the introduction of the LSTM, attempts to train recurrent nets to bridge long time gaps were shown to perform no better than random guessing [Hochreiter and Schmidhuber, 1996]  SENTENCE_END
SENTENCE_START However, RNNs are now frequently trained successfully  SENTENCE_END
SENTENCE_START For some tasks, freely available software can be run on a single GPU and produce compelling results in hours [Karpathy, 2015]  SENTENCE_END
SENTENCE_START Martens and Sutskever [2011] reported success training recurrent neural networks with a Hessian-free truncated Newton approach, and applied the method to a network which learns to generate text one character at a time in [Sutskever et al , 2011]  SENTENCE_END
SENTENCE_START In the paper that describes the abundance of saddle points on the error surfaces of neural networks [Dauphin et al , 2014], the authors present a saddle-free version of Newton's method  SENTENCE_END
SENTENCE_START Unlike Newton's method, which is attracted to critical points, including saddle points, this variant is specially designed to escape from them  SENTENCE_END
SENTENCE_START Experimental results include a demonstration of improved performance on recurrent networks  SENTENCE_END
SENTENCE_START Newton's method requires computing the Hessian, which is prohibitively expensive for large networks, scaling quadratically with the number of parameters  SENTENCE_END
SENTENCE_START While their algorithm only approximates the Hessian, it is still computationally expensive compared to SGD  SENTENCE_END
SENTENCE_START Thus the authors describe a hybrid approach in which the saddle-free Newton method is applied only in places where SGD appears to be stuck  SENTENCE_END
SENTENCE_START 4 Modern RNN architectures The most successful RNN architectures for sequence learning stem from two papers published in 1997  SENTENCE_END
SENTENCE_START The first paper, Long Short-Term Memory by Hochreiter and Schmidhuber [1997], introduces the memory cell, a unit of computation that replaces traditional nodes in the hidden layer of a network  SENTENCE_END
SENTENCE_START With these memory cells, networks are able to overcome difficulties with training encountered by earlier recurrent networks  SENTENCE_END
SENTENCE_START The second paper, Bidirectional Recurrent Neural Networks by Schuster and Paliwal [1997], introduces an architecture in which information from both the future and the past are used to determine the output at any point in the sequence  SENTENCE_END
SENTENCE_START This is in contrast to previous networks, in which only past input can affect the output, and has been used successfully for sequence labeling tasks in natural language processing, among others  SENTENCE_END
SENTENCE_START Fortunately, the two innovations are not mutually exclusive, and have been successfully combined for phoneme classification [Graves and Schmidhuber, 2005] and handwriting recognition [Graves et al , 2009]  SENTENCE_END
SENTENCE_START In this section we explain the LSTM and BRNN and we describe the neural Turing machine (NTM), which extends RNNs with an addressable external memory [Graves et al , 2014]  SENTENCE_END
SENTENCE_START 16 Figure 9: One LSTM memory cell as proposed by Hochreiter and Schmidhuber [1997]  SENTENCE_END
SENTENCE_START The self-connected node is the internal state s  The diagonal line indicates that it is linear, i e  SENTENCE_END
SENTENCE_START the identity link function is applied  SENTENCE_END
SENTENCE_START The blue dashed line is the recurrent edge, which has fixed unit weight  SENTENCE_END
SENTENCE_START Nodes marked  output the product of their inputs  SENTENCE_END
SENTENCE_START All edges into and from  nodes also have fixed unit weight  SENTENCE_END
SENTENCE_START 4 1 Long short-term memory (LSTM) Hochreiter and Schmidhuber [1997] introduced the LSTM model primarily in order to overcome the problem of vanishing gradients  SENTENCE_END
SENTENCE_START This model resembles a standard recurrent neural network with a hidden layer, but each ordinary node (Figure 1) in the hidden layer is replaced by a memory cell (Figure 9)  SENTENCE_END
SENTENCE_START Each memory cell contains a node with a self-connected recurrent edge of fixed weight one, ensuring that the gradient can pass across many time steps without vanishing or exploding  SENTENCE_END
SENTENCE_START To distinguish references to a memory cell and not an ordinary node, we use the subscript c  The term "long short-term memory" comes from the following intuition  SENTENCE_END
SENTENCE_START Simple recurrent neural networks have long-term memory in the form of weights  SENTENCE_END
SENTENCE_START The weights change slowly during training, encoding general knowledge about the data  SENTENCE_END
SENTENCE_START They also have short-term memory in the form of ephemeral activations, which pass from each node to successive nodes  SENTENCE_END
SENTENCE_START The LSTM model introduces an intermediate type of storage via the memory cell  SENTENCE_END
SENTENCE_START A memory cell is a composite unit, built from simpler nodes in a specific connectivity pattern, with the novel inclusion of multiplicative nodes, represented in diagrams by the letter   SENTENCE_END
SENTENCE_START All elements of the LSTM cell are enumerated and described below  SENTENCE_END
SENTENCE_START Note that when we use vector notation, we are referring to the values of the 17 nodes in an entire layer of cells  SENTENCE_END
SENTENCE_START For example, s is a vector containing the value of sc at each memory cell c in a layer  SENTENCE_END
SENTENCE_START When the subscript c is used, it is to index an individual memory cell  SENTENCE_END
SENTENCE_START Input node: This unit, labeled gc, is a node that takes activation in the standard way from the input layer x(t) at the current time step and (along recurrent edges) from the hidden layer at the previous time step h(t-1)  SENTENCE_END
SENTENCE_START Typically, the summed weighted input is run through a tanh activation function, although in the original LSTM paper, the activation function is a sigmoid  SENTENCE_END
SENTENCE_START Input gate: Gates are a distinctive feature of the LSTM approach  SENTENCE_END
SENTENCE_START A gate is a sigmoidal unit that, like the input node, takes activation from the current data point x(t) as well as from the hidden layer at the previous time step  SENTENCE_END
SENTENCE_START A gate is so-called because its value is used to multiply the value of another node  SENTENCE_END
SENTENCE_START It is a gate in the sense that if its value is zero, then flow from the other node is cut off  SENTENCE_END
SENTENCE_START If the value of the gate is one, all flow is passed through  SENTENCE_END
SENTENCE_START The value of the input gate ic multiplies the value of the input node  SENTENCE_END
SENTENCE_START Internal state: At the heart of each memory cell is a node sc with linear activation, which is referred to in the original paper as the "internal state" of the cell  SENTENCE_END
SENTENCE_START The internal state sc has a self-connected recurrent edge with fixed unit weight  SENTENCE_END
SENTENCE_START Because this edge spans adjacent time steps with constant weight, error can flow across time steps without vanishing or exploding  SENTENCE_END
SENTENCE_START This edge is often called the constant error carousel  SENTENCE_END
SENTENCE_START In vector notation, the update for the internal state is s(t) = g(t) i(t) + s(t-1) where is pointwise multiplication  SENTENCE_END
SENTENCE_START Forget gate: These gates fc were introduced by Gers et al  SENTENCE_END
SENTENCE_START [2000]  SENTENCE_END
SENTENCE_START They provide a method by which the network can learn to flush the contents of the internal state  SENTENCE_END
SENTENCE_START This is especially useful in continuously running networks  SENTENCE_END
SENTENCE_START With forget gates, the equation to calculate the internal state on the forward pass is s(t) = g(t) i(t) + f(t) s(t-1)   SENTENCE_END
SENTENCE_START Output gate: The value vc ultimately produced by a memory cell is the value of the internal state sc multiplied by the value of the output gate oc  SENTENCE_END
SENTENCE_START It is customary that the internal state first be run through a tanh activation function, as this gives the output of each cell the same dynamic range as an ordinary tanh hidden unit  SENTENCE_END
SENTENCE_START However, in other neural network research, rectified linear units, which have a greater dynamic range, are easier to train  SENTENCE_END
SENTENCE_START Thus it seems plausible that the nonlinear function on the internal state might be omitted  SENTENCE_END
SENTENCE_START In the original paper and in most subsequent work, the input node is labeled g  We adhere to this convention but note that it may be confusing as g does 18 Figure 10: LSTM memory cell with a forget gate as described by Gers et al  SENTENCE_END
SENTENCE_START [2000]  SENTENCE_END
SENTENCE_START not stand for gate  SENTENCE_END
SENTENCE_START In the original paper, the gates are called yin and yout but this is confusing because y generally stands for output in the machine learning literature  SENTENCE_END
SENTENCE_START Seeking comprehensibility, we break with this convention and use i, f, and o to refer to input, forget and output gates respectively, as in Sutskever et al  SENTENCE_END
SENTENCE_START [2014]  SENTENCE_END
SENTENCE_START Since the original LSTM was introduced, several variations have been proposed  SENTENCE_END
SENTENCE_START Forget gates, described above, were proposed in 2000 and were not part of the original LSTM design  SENTENCE_END
SENTENCE_START However, they have proven effective and are standard in most modern implementations  SENTENCE_END
SENTENCE_START That same year, Gers and Schmidhuber [2000] proposed peephole connections that pass from the internal state directly to the input and output gates of that same node without first having to be modulated by the output gate  SENTENCE_END
SENTENCE_START They report that these connections improve performance on timing tasks where the network must learn to measure precise intervals between events  SENTENCE_END
SENTENCE_START The intuition of the peephole connection can be captured by the following example  SENTENCE_END
SENTENCE_START Consider a network which must learn to count objects and emit some desired output when n objects have been seen  SENTENCE_END
SENTENCE_START The network might learn to let some fixed amount of activation into the internal state after each object is seen  SENTENCE_END
SENTENCE_START This activation is trapped in the internal state sc by the constant error carousel, and is incremented iteratively each time another object is seen  SENTENCE_END
SENTENCE_START When the nth object is seen, the network needs to know to let out content from the internal state so that it can affect the output  SENTENCE_END
SENTENCE_START To accomplish this, the output gate oc must know the content of the internal state sc  SENTENCE_END
SENTENCE_START Thus sc should be an input to oc  SENTENCE_END
SENTENCE_START Put formally, computation in the LSTM model proceeds according to the 19 Figure 11: A recurrent neural network with a hidden layer consisting of two memory cells  SENTENCE_END
SENTENCE_START The network is shown unfolded across two time steps  SENTENCE_END
SENTENCE_START following calculations, which are performed at each time step  SENTENCE_END
SENTENCE_START These equations give the full algorithm for a modern LSTM with forget gates: g(t) = (Wgxx(t) + Wghh(t-1) + bg) i(t) = (Wixx(t) + Wihh(t-1) + bi) f(t) = (Wfxx(t) + Wfhh(t-1) + bf ) o(t) = (Woxx(t) + Wohh(t-1) + bo) s(t) = g(t) i(i) + s(t-1) f(t) h(t) = (s(t)) o(t)   SENTENCE_END
SENTENCE_START The value of the hidden layer of the LSTM at time t is the vector h(t) , while h(t-1) is the values output by each memory cell in the hidden layer at the previous time  SENTENCE_END
SENTENCE_START Note that these equations include the forget gate, but not peephole connections  SENTENCE_END
SENTENCE_START The calculations for the simpler LSTM without forget gates are obtained by setting f(t) = 1 for all t  We use the tanh function  for the input node g following the state-of-the-art design of Zaremba and Sutskever [2014]  SENTENCE_END
SENTENCE_START However, in the original LSTM paper, the activation function for g is the sigmoid   SENTENCE_END
SENTENCE_START Intuitively, in terms of the forward pass, the LSTM can learn when to let activation into the internal state  SENTENCE_END
SENTENCE_START As long as the input gate takes value zero, no activation can get in  SENTENCE_END
SENTENCE_START Similarly, the output gate learns when to let the 20 Figure 12: A bidirectional recurrent neural network as described by Schuster and Paliwal [1997], unfolded in time  SENTENCE_END
SENTENCE_START value out  SENTENCE_END
SENTENCE_START When both gates are closed, the activation is trapped in the memory cell, neither growing nor shrinking, nor affecting the output at intermediate time steps  SENTENCE_END
SENTENCE_START In terms of the backwards pass, the constant error carousel enables the gradient to propagate back across many time steps, neither exploding nor vanishing  SENTENCE_END
SENTENCE_START In this sense, the gates are learning when to let error in, and when to let it out  SENTENCE_END
SENTENCE_START In practice, the LSTM has shown a superior ability to learn long-range dependencies as compared to simple RNNs  SENTENCE_END
SENTENCE_START Consequently, the majority of state-of-the-art application papers covered in this review use the LSTM model  SENTENCE_END
SENTENCE_START One frequent point of confusion is the manner in which multiple memory cells are used together to comprise the hidden layer of a working neural network  SENTENCE_END
SENTENCE_START To alleviate this confusion, we depict in Figure 11 a simple network with two memory cells, analogous to Figure 4  SENTENCE_END
SENTENCE_START The output from each memory cell flows in the subsequent time step to the input node and all gates of each memory cell  SENTENCE_END
SENTENCE_START It is common to include multiple layers of memory cells [Sutskever et al , 2014]  SENTENCE_END
SENTENCE_START Typically, in these architectures each layer takes input from the layer below at the same time step and from the same layer in the previous time step  SENTENCE_END
SENTENCE_START 4 2 Bidirectional recurrent neural networks (BRNNs) Along with the LSTM, one of the most used RNN architectures is the bidirectional recurrent neural network (BRNN) (Figure 12) first described by Schuster and Paliwal [1997]  SENTENCE_END
SENTENCE_START In this architecture, there are two layers of hidden nodes  SENTENCE_END
SENTENCE_START Both hidden layers are connected to input and output  SENTENCE_END
SENTENCE_START The two hidden layers are differentiated in that the first has recurrent connections from the past time 21 steps while in the second the direction of recurrent of connections is flipped, passing activation backwards along the sequence  SENTENCE_END
SENTENCE_START Given an input sequence and a target sequence, the BRNN can be trained by ordinary backpropagation after unfolding across time  SENTENCE_END
SENTENCE_START The following three equations describe a BRNN: h(t) = (Whxx(t) + Whhh(t-1) + bh) z(t) = (Wzxx(t) + Wzzz(t+1) + bz) ^ y(t) = softmax(Wyhh(t) + Wyzz(t) + by) where h(t) and z(t) are the values of the hidden layers in the forwards and backwards directions respectively  SENTENCE_END
SENTENCE_START One limitation of the BRNN is that cannot run continuously, as it requires a fixed endpoint in both the future and in the past  SENTENCE_END
SENTENCE_START Further, it is not an appropriate machine learning algorithm for the online setting, as it is implausible to receive information from the future, i e , to know sequence elements that have not been observed  SENTENCE_END
SENTENCE_START But for prediction over a sequence of fixed length, it is often sensible to take into account both past and future sequence elements  SENTENCE_END
SENTENCE_START Consider the natural language task of part-of-speech tagging  SENTENCE_END
SENTENCE_START Given any word in a sentence, information about both the words which precede and those which follow it is useful for predicting that word's part-of-speech  SENTENCE_END
SENTENCE_START The LSTM and BRNN are in fact compatible ideas  SENTENCE_END
SENTENCE_START The former introduces a new basic unit from which to compose a hidden layer, while the latter concerns the wiring of the hidden layers, regardless of what nodes they contain  SENTENCE_END
SENTENCE_START Such an approach, termed a BLSTM has been used to achieve state of the art results on handwriting recognition and phoneme classification [Graves and Schmidhuber, 2005, Graves et al , 2009]  SENTENCE_END
SENTENCE_START 4 3 Neural Turing machines The neural Turing machine (NTM) extends recurrent neural networks with an addressable external memory [Graves et al , 2014]  SENTENCE_END
SENTENCE_START This work improves upon the ability of RNNs to perform complex algorithmic tasks such as sorting  SENTENCE_END
SENTENCE_START The authors take inspiration from theories in cognitive science, which suggest humans possess a "central executive" that interacts with a memory buffer [Baddeley et al , 1996]  SENTENCE_END
SENTENCE_START By analogy to a Turing machine, in which a program directs read heads and write heads to interact with external memory in the form of a tape, the model is named a Neural Turing Machine  SENTENCE_END
SENTENCE_START While technical details of the read/write heads are beyond the scope of this review, we aim to convey a high-level sense of the model and its applications  SENTENCE_END
SENTENCE_START The two primary components of an NTM are a controller and memory matrix  SENTENCE_END
SENTENCE_START The controller, which may be a recurrent or feedforward neural network, takes input and returns output to the outside world, as well as passing instructions to and reading from the memory  SENTENCE_END
SENTENCE_START The memory is represented by a large matrix of N memory locations, each of which is a vector of dimension M  Additionally, a number of read and write heads facilitate the interaction between 22 the controller and the memory matrix  SENTENCE_END
SENTENCE_START Despite these additional capabilities, the NTM is differentiable end-to-end and can be trained by variants of stochastic gradient descent using BPTT  SENTENCE_END
SENTENCE_START Graves et al  SENTENCE_END
SENTENCE_START [2014] select five algorithmic tasks to test the performance of the NTM model  SENTENCE_END
SENTENCE_START By algorithmic we mean that for each task, the target output for a given input can be calculated by following a simple program, as might be easily implemented in any universal programming language  SENTENCE_END
SENTENCE_START One example is the copy task, where the input is a sequence of fixed length binary vectors followed by a delimiter symbol  SENTENCE_END
SENTENCE_START The target output is a copy of the input sequence  SENTENCE_END
SENTENCE_START In another task, priority sort, an input consists of a sequence of binary vectors together with a distinct scalar priority value for each vector  SENTENCE_END
SENTENCE_START The target output is the sequence of vectors sorted by priority  SENTENCE_END
SENTENCE_START The experiments test whether an NTM can be trained via supervised learning to implement these common algorithms correctly and efficiently  SENTENCE_END
SENTENCE_START Interestingly, solutions found in this way generalize reasonably well to inputs longer than those presented in the training set  SENTENCE_END
SENTENCE_START In contrast, the LSTM without external memory does not generalize well to longer inputs  SENTENCE_END
SENTENCE_START The authors compare three different architectures, namely an LSTM RNN, an NTM with a feedforward controller, and an NTM with an LSTM controller  SENTENCE_END
SENTENCE_START On each task, both NTM architectures significantly outperform the LSTM RNN both in training set performance and in generalization to test data  SENTENCE_END
SENTENCE_START 5 Applications of LSTMs and BRNNs The previous sections introduced the building blocks from which nearly all state-of-the-art recurrent neural networks are composed  SENTENCE_END
SENTENCE_START This section looks at several application areas where recurrent networks have been employed successfully  SENTENCE_END
SENTENCE_START Before describing state of the art results in detail, it is appropriate to convey a concrete sense of the precise architectures with which many important tasks can be expressed clearly as sequence learning problems with recurrent neural networks  SENTENCE_END
SENTENCE_START Figure 13 demonstrates several common RNN architectures and associates each with corresponding well-documented tasks  SENTENCE_END
SENTENCE_START In the following subsections, we first introduce the representations of natural language used for input and output to recurrent neural networks and the commonly used performance metrics for sequence prediction tasks  SENTENCE_END
SENTENCE_START Then we survey state-of-the-art results in machine translation, image captioning, video captioning, and handwriting recognition  SENTENCE_END
SENTENCE_START Many applications of RNNs involve processing written language  SENTENCE_END
SENTENCE_START Some applications, such as image captioning, involve generating strings of text  SENTENCE_END
SENTENCE_START Others, such as machine translation and dialogue systems, require both inputting and outputting text  SENTENCE_END
SENTENCE_START Systems which output text are more difficult to evaluate empirically than those which produce binary predictions or numerical output  SENTENCE_END
SENTENCE_START As a result several methods have been developed to assess the quality of translations and captions  SENTENCE_END
SENTENCE_START In the next subsection, we provide the background necessary to understand how text is represented in most modern recurrent net applications  SENTENCE_END
SENTENCE_START We then explain the commonly reported evaluation metrics  SENTENCE_END
SENTENCE_START 23 Figure 13: Recurrent neural networks have been used successfully to model both sequential inputs and sequential outputs as well as mappings between single data points and sequences (in both directions)  SENTENCE_END
SENTENCE_START This figure, based on a similar figure in Karpathy [2015] shows how numerous tasks can be modeled with RNNs with sequential inputs and/or sequential outputs  SENTENCE_END
SENTENCE_START In each subfigure, blue rectangles correspond to inputs, red rectangles to outputs and green rectangles to the entire hidden state of the neural network  SENTENCE_END
SENTENCE_START (a) This is the conventional independent case, as assumed by standard feedforward networks  SENTENCE_END
SENTENCE_START (b) Text and video classification are tasks in which a sequence is mapped to one fixed length vector  SENTENCE_END
SENTENCE_START (c) Image captioning presents the converse case, where the input image is a single non-sequential data point  SENTENCE_END
SENTENCE_START (d) This architecture has been used for natural language translation, a sequence-to-sequence task in which the two sequences may have varying and different lengths  SENTENCE_END
SENTENCE_START (e) This architecture has been used to learn a generative model for text, predicting at each step the following character  SENTENCE_END
SENTENCE_START 24 5 1 Representations of natural language inputs and outputs When words are output at each time step, generally the output consists of a softmax vector y(t) RK where K is the size of the vocabulary  SENTENCE_END
SENTENCE_START A softmax layer is an element-wise logistic function that is normalized so that all of its components sum to one  SENTENCE_END
SENTENCE_START Intuitively, these outputs correspond to the probabilities that each word is the correct output at that time step  SENTENCE_END
SENTENCE_START For application where an input consists of a sequence of words, typically the words are fed to the network one at a time in consecutive time steps  SENTENCE_END
SENTENCE_START In these cases, the simplest way to represent words is a one-hot encoding, using binary vectors with a length equal to the size of the vocabulary, so "1000" and "0100" would represent the first and second words in the vocabulary respectively  SENTENCE_END
SENTENCE_START Such an encoding is discussed by Elman [1990] among others  SENTENCE_END
SENTENCE_START However, this encoding is inefficient, requiring as many bits as the vocabulary is large  SENTENCE_END
SENTENCE_START Further, it offers no direct way to capture different aspects of similarity between words in the encoding itself  SENTENCE_END
SENTENCE_START Thus it is common now to model words with a distributed representation using a meaning vector  SENTENCE_END
SENTENCE_START In some cases, these meanings for words are learned given a large corpus of supervised data, but it is more usual to initialize the meaning vectors using an embedding based on word co-occurrence statistics  SENTENCE_END
SENTENCE_START Freely available code to produce word vectors from these statistics include GloVe [Pennington et al , 2014], and word2vec [Goldberg and Levy, 2014], which implements a word embedding algorithm from Mikolov et al  SENTENCE_END
SENTENCE_START [2013]  SENTENCE_END
SENTENCE_START Distributed representations for textual data were described by Hinton [1986], used extensively for natural language by Bengio et al  SENTENCE_END
SENTENCE_START [2003], and more recently brought to wider attention in the deep learning community in a number of papers describing recursive auto-encoder (RAE) networks [Socher et al , 2010, 2011a,b,c]  SENTENCE_END
SENTENCE_START For clarity we point out that these recursive networks are not recurrent neural networks, and in the present survey the abbreviation RNN always means recurrent neural network  SENTENCE_END
SENTENCE_START While they are distinct approaches, recurrent and recursive neural networks have important features in common, namely that they both involve extensive weight tying and are both trained end-to-end via backpropagation  SENTENCE_END
SENTENCE_START In many experiments with recurrent neural networks [Elman, 1990, Sutskever et al , 2011, Zaremba and Sutskever, 2014], input is fed in one character at a time, and output generated one character at a time, as opposed to one word at a time  SENTENCE_END
SENTENCE_START While the output is nearly always a softmax layer, many papers omit details of how they represent single-character inputs  SENTENCE_END
SENTENCE_START It seems reasonable to infer that characters are encoded with a one-hot encoding  SENTENCE_END
SENTENCE_START We know of no cases of paper using a distributed representation at the single-character level  SENTENCE_END
SENTENCE_START 5 2 Evaluation methodology A serious obstacle to training systems well to output variable length sequences of words is the flaws of the available performance metrics  SENTENCE_END
SENTENCE_START In the case of captioning or translation, there maybe be multiple correct translations  SENTENCE_END
SENTENCE_START Further, 25 a labeled dataset may contain multiple reference translations for each example  SENTENCE_END
SENTENCE_START Comparing against such a gold standard is more fraught than applying standard performance measure to binary classification problems  SENTENCE_END
SENTENCE_START One commonly used metric for structured natural language output with multiple references is BLEU score  SENTENCE_END
SENTENCE_START Developed in 2002, BLEU score is related to modified unigram precision [Papineni et al , 2002]  SENTENCE_END
SENTENCE_START It is the geometric mean of the n-gram precisions for all values of n between 1 and some upper limit N  In practice, 4 is a typical value for N, shown to maximize agreement with human raters  SENTENCE_END
SENTENCE_START Because precision can be made high by offering excessively short translations, the BLEU score includes a brevity penalty B  SENTENCE_END
SENTENCE_START Where c is average the length of the candidate translations and r the average length of the reference translations, the brevity penalty is B = 1 if c > r e(1-r/c) if c  r   SENTENCE_END
SENTENCE_START Then the BLEU score is BLEU = B  exp 1 N N n=1 log pn where pn is the modified n-gram precision, which is the number of n-grams in the candidate translation that occur in any of the reference translations, divided by the total number of n-grams in the candidate translation  SENTENCE_END
SENTENCE_START This is called modified precision because it is an adaptation of precision to the case of multiple references  SENTENCE_END
SENTENCE_START BLEU scores are commonly used in recent papers to evaluate both translation and captioning systems  SENTENCE_END
SENTENCE_START While BLEU score does appear highly correlated with human judgments, there is no guarantee that any given translation with a higher BLEU score is superior to another which receives a lower BLEU score  SENTENCE_END
SENTENCE_START In fact, while BLEU scores tend to be correlated with human judgement across large sets of translations, they are not accurate predictors of human judgement at the single sentence level  SENTENCE_END
SENTENCE_START METEOR is an alternative metric intended to overcome the weaknesses of the BLEU score [Banerjee and Lavie, 2005]  SENTENCE_END
SENTENCE_START METEOR is based on explicit word to word matches between candidates and reference sentences  SENTENCE_END
SENTENCE_START When multiple references exist, the best score is used  SENTENCE_END
SENTENCE_START Unlike BLEU, METEOR exploits known synonyms and stemming  SENTENCE_END
SENTENCE_START The first step is to compute an F-score F = P  R  P + (1 - )  R based on single word matches where P is the precision and R is the recall  SENTENCE_END
SENTENCE_START The next step is to calculate a fragmentation penalty M  c/m where c is the smallest number of chunks of consecutive words such that the words are adjacent in both the candidate and the reference, and m is the total number of matched unigrams yielding the score  SENTENCE_END
SENTENCE_START Finally, the score is METEOR = (1 - M)  F   26 Empirically, this metric has been found to agree with human raters more than BLEU score  SENTENCE_END
SENTENCE_START However, METEOR is less straightforward to calculate than BLEU  SENTENCE_END
SENTENCE_START To replicate the METEOR score reported by another party, one must exactly replicate their stemming and synonym matching, as well as the calculations  SENTENCE_END
SENTENCE_START Both metrics rely upon having the exact same set of reference translations  SENTENCE_END
SENTENCE_START Even in the straightforward case of binary classification, without sequential dependencies, commonly used performance metrics like F1 give rise to optimal thresholding strategies which may not accord with intuition about what should constitute good performance [Lipton et al , 2014]  SENTENCE_END
SENTENCE_START Along the same lines, given that performance metrics such as the ones above are weak proxies for true objectives, it may be difficult to distinguish between systems which are truly stronger and those which most overfit the performance metrics in use  SENTENCE_END
SENTENCE_START 5 3 Natural language translation Translation of text is a fundamental problem in machine learning that resists solutions with shallow methods  SENTENCE_END
SENTENCE_START Some tasks, like document classification, can be performed successfully with a bag-of-words representation that ignores word order  SENTENCE_END
SENTENCE_START But word order is essential in translation  SENTENCE_END
SENTENCE_START The sentences "Scientist killed by raging virus" and "Virus killed by raging scientist" have identical bag-of-words representations  SENTENCE_END
SENTENCE_START Sutskever et al  SENTENCE_END
SENTENCE_START [2014] present a translation model using two multilayered LSTMs that demonstrates impressive performance translating from English to French  SENTENCE_END
SENTENCE_START The first LSTM is used for encoding an input phrase from the source language and the second LSTM for decoding the output phrase in the target language  SENTENCE_END
SENTENCE_START The model works according to the following procedure (Figure 14):  The source phrase is fed to the encoding LSTM one word at a time, which does not output anything  SENTENCE_END
SENTENCE_START The authors found that significantly better results are achieved when the input sentence is fed into the network in reverse order  SENTENCE_END
SENTENCE_START When the end of the phrase is reached, a special symbol that indicates the beginning of the output sentence is sent to the decoding LSTM  SENTENCE_END
SENTENCE_START Additionally, the decoding LSTM receives as input the final state of the first LSTM  SENTENCE_END
SENTENCE_START The second LSTM outputs softmax probabilities over the vocabulary at each time step  SENTENCE_END
SENTENCE_START At inference time, beam search is used to choose the most likely words from the distribution at each time step, running the second LSTM until the endof-sentence (EOS) token is reached  SENTENCE_END
SENTENCE_START For training, the true inputs are fed to the encoder, the true translation is fed to the decoder, and loss is propagated back from the outputs of the decoder across the entire sequence to sequence model  SENTENCE_END
SENTENCE_START The network is trained to maximize the likelihood of the correct translation of each sentence in the training set  SENTENCE_END
SENTENCE_START At inference time, a left to right beam search is used to determine 27 Figure 14: Sequence to sequence LSTM model of Sutskever et al  SENTENCE_END
SENTENCE_START [2014]  SENTENCE_END
SENTENCE_START The network consists of an encoding model (first LSTM) and a decoding model (second LSTM)  SENTENCE_END
SENTENCE_START The input blocks (blue and purple) correspond to word vectors, which are fully connected to the corresponding hidden state  SENTENCE_END
SENTENCE_START Red nodes are softmax outputs  SENTENCE_END
SENTENCE_START Weights are tied among all encoding steps and among all decoding time steps  SENTENCE_END
SENTENCE_START 28 which words to output  SENTENCE_END
SENTENCE_START A few among the most likely next words are chosen for expansion after each time step  SENTENCE_END
SENTENCE_START The beam search ends when the network outputs an end-of-sentence (EOS) token  SENTENCE_END
SENTENCE_START Sutskever et al  SENTENCE_END
SENTENCE_START [2014] train the model using stochastic gradient descent without momentum, halving the learning rate twice per epoch, after the first five epochs  SENTENCE_END
SENTENCE_START The approach achieves a BLEU score of 34 81, outperforming the best previous neural network NLP systems, and matching the best published results for non-neural network approaches, including systems that have explicitly programmed domain expertise  SENTENCE_END
SENTENCE_START When their system is used to rerank candidate translations from another system, it achieves a BLEU score of 36 5  SENTENCE_END
SENTENCE_START The implementation which achieved these results involved eight GPUS  SENTENCE_END
SENTENCE_START Nevertheless, training took 10 days to complete  SENTENCE_END
SENTENCE_START One GPU was assigned to each layer of the LSTM, and an additional four GPUs were used simply to calculate softmax  SENTENCE_END
SENTENCE_START The implementation was coded in C++, and each hidden layer of the LSTM contained 1000 nodes  SENTENCE_END
SENTENCE_START The input vocabulary contained 160,000 words and the output vocabulary contained 80,000 words  SENTENCE_END
SENTENCE_START Weights were initialized uniformly randomly in the range between -0 08 and 0 08  SENTENCE_END
SENTENCE_START Another RNN approach to language translation is presented by Auli et al  SENTENCE_END
SENTENCE_START [2013]  SENTENCE_END
SENTENCE_START Their RNN model uses the word embeddings of Mikolov and a lattice representation of the decoder output to facilitate search over the space of possible translations  SENTENCE_END
SENTENCE_START In the lattice, each node corresponds to a sequence of words  SENTENCE_END
SENTENCE_START They report a BLEU score of 28 5 on French-English translation tasks  SENTENCE_END
SENTENCE_START Both papers provide results on similar datasets but Sutskever et al  SENTENCE_END
SENTENCE_START [2014] only report on English to French translation while Auli et al  SENTENCE_END
SENTENCE_START [2013] only report on French to English translation, so it is not possible to compare the performance of the two models  SENTENCE_END
SENTENCE_START 5 4 Image captioning Recently, recurrent neural networks have been used successfully for generating sentences that describe photographs [Vinyals et al , 2015, Karpathy and Fei-Fei, 2014, Mao et al , 2014]  SENTENCE_END
SENTENCE_START In this task, a training set consists of input images x and target captions y  SENTENCE_END
SENTENCE_START Given a large set of image-caption pairs, a model is trained to predict the appropriate caption for an image  SENTENCE_END
SENTENCE_START Vinyals et al  SENTENCE_END
SENTENCE_START [2015] follow up on the success in language to language translation by considering captioning as a case of image to language translation  SENTENCE_END
SENTENCE_START Instead of both encoding and decoding with LSTMs, they introduce the idea of encoding an image with a convolutional neural network, and then decoding it with an LSTM  SENTENCE_END
SENTENCE_START Mao et al  SENTENCE_END
SENTENCE_START [2014] independently developed a similar RNN image captioning network, and achieved then state-of-the-art results on the Pascal, Flickr30K, and COCO datasets  SENTENCE_END
SENTENCE_START Karpathy and Fei-Fei [2014] follows on this work, using a convolutional network to encode images together with a bidirectional network attention mech- anism and standard RNN to decode captions, using word2vec embeddings as word representations  SENTENCE_END
SENTENCE_START They consider both full-image captioning and a model that captures correspondences between image regions and text snippets  SENTENCE_END
SENTENCE_START At 29 inference time, their procedure resembles the one described by Sutskever et al  SENTENCE_END
SENTENCE_START [2014], where sentences are decoded one word at a time  SENTENCE_END
SENTENCE_START The most probable word is chosen and fed to the network at the next time step  SENTENCE_END
SENTENCE_START This process is repeated until an EOS token is produced  SENTENCE_END
SENTENCE_START To convey a sense of the scale of these problems, Karpathy and Fei-Fei [2014] focus on three datasets of captioned images: Flickr8K, Flickr30K, and COCO, of size 50MB (8000 images), 200MB (30,000 images), and 750MB (328,000 images) respectively  SENTENCE_END
SENTENCE_START The implementation uses the Caffe library [Jia et al , 2014], and the convolutional network is pretrained on ImageNet data  SENTENCE_END
SENTENCE_START In a revised version, the authors report that LSTMs outperform simpler RNNs and that learning word representations from random initializations is often preferable to word2vec embeddings  SENTENCE_END
SENTENCE_START As an explanation, they say that word2vec embeddings may cluster words like colors together in the embedding space, which can be not suitable for visual descriptions of images  SENTENCE_END
SENTENCE_START 5 5 Further applications Handwriting recognition is an application area where bidirectional LSTMs have been used to achieve state of the art results  SENTENCE_END
SENTENCE_START In work by Liwicki et al  SENTENCE_END
SENTENCE_START [2007] and Graves et al  SENTENCE_END
SENTENCE_START [2009], data is collected from an interactive whiteboard  SENTENCE_END
SENTENCE_START Sensors record the (x, y) coordinates of the pen at regularly sampled time steps  SENTENCE_END
SENTENCE_START In the more recent paper, they use a bidirectional LSTM model, outperforming an HMM model by achieving 81 5% word-level accuracy, compared to 70 1% for the HMM  SENTENCE_END
SENTENCE_START In the last year, a number of papers have emerged that extend the success of recurrent networks for translation and image captioning to new domains  SENTENCE_END
SENTENCE_START Among the most interesting of these applications are unsupervised video encoding [Srivastava et al , 2015], video captioning [Venugopalan et al , 2015], and program execution [Zaremba and Sutskever, 2014]  SENTENCE_END
SENTENCE_START Venugopalan et al  SENTENCE_END
SENTENCE_START [2015] demonstrate a sequence to sequence architecture that encodes frames from a video and decode words  SENTENCE_END
SENTENCE_START At each time step the input to the encoding LSTM is the topmost hidden layer of a convolutional neural network  SENTENCE_END
SENTENCE_START At decoding time, the network outputs probabilities over the vocabulary at each time step  SENTENCE_END
SENTENCE_START Zaremba and Sutskever [2014] experiment with networks which read computer programs one character at a time and predict their output  SENTENCE_END
SENTENCE_START They focus on programs which output integers and find that for simple programs, including adding two nine-digit numbers, their network, which uses LSTM cells in several stacked hidden layers and makes a single left to right pass through the program, can predict the output with 99% accuracy  SENTENCE_END
SENTENCE_START 6 Discussion Over the past thirty years, recurrent neural networks have gone from models primarily of interest for cognitive modeling and computational neuroscience, to 30 powerful and practical tools for large-scale supervised learning from sequences  SENTENCE_END
SENTENCE_START This progress owes to advances in model architectures, training algorithms, and parallel computing  SENTENCE_END
SENTENCE_START Recurrent networks are especially interesting because they overcome many of the restrictions placed on input and output data by traditional machine learning approaches  SENTENCE_END
SENTENCE_START With recurrent networks, the assumption of independence between consecutive examples is broken, and hence also the assumption of fixed-dimension inputs and outputs  SENTENCE_END
SENTENCE_START While LSTMs and BRNNs have set records in accuracy on many tasks in recent years, it is noteworthy that advances come from novel architectures rather than from fundamentally novel algorithms  SENTENCE_END
SENTENCE_START Therefore, automating exploration of the space of possible models, for example via genetic algorithms or a Markov chain Monte Carlo approach, could be promising  SENTENCE_END
SENTENCE_START Neural networks offer a wide range of transferable and combinable techniques  SENTENCE_END
SENTENCE_START New activation functions, training procedures, initializations procedures, etc  SENTENCE_END
SENTENCE_START are generally transferable across networks and tasks, often conferring similar benefits  SENTENCE_END
SENTENCE_START As the number of such techniques grows, the practicality of testing all combinations diminishes  SENTENCE_END
SENTENCE_START It seems reasonable to infer that as a community, neural network researchers are exploring the space of model architectures and configurations much as a genetic algorithm might, mixing and matching techniques, with a fitness function in the form of evaluation metrics on major datasets of interest  SENTENCE_END
SENTENCE_START This inference suggests two corollaries  SENTENCE_END
SENTENCE_START First, as just stated, this body of research could benefit from automated procedures to explore the space of models  SENTENCE_END
SENTENCE_START Second, as we build systems designed to perform more complex tasks, we would benefit from improved fitness functions  SENTENCE_END
SENTENCE_START A BLEU score inspires less con- fidence than the accuracy reported on a binary classification task  SENTENCE_END
SENTENCE_START To this end, when possible, it seems prudent to individually test techniques first with classic feedforward networks on datasets with established benchmarks before applying them to recurrent networks in settings with less reliable evaluation criteria  SENTENCE_END
SENTENCE_START Lastly, the rapid success of recurrent neural networks on natural language tasks leads us to believe that extensions of this work to longer texts would be fruitful  SENTENCE_END
SENTENCE_START Additionally, we imagine that dialogue systems could be built along similar principles to the architectures used for translation, encoding prompts and generating responses, while retaining the entirety of conversation history as contextual information  SENTENCE_END






SENTENCE_START LONG SHORT-TERM MEMORY Neural Computation 9(8):1735{1780, 1997 Sepp Hochreiter Fakultat fur Informatik Technische Universitat Munchen 80290 Munchen, Germany hochreit@informatik tu-muenchen de http://www7 informatik tu-muenchen de/~hochreit Jurgen Schmidhuber IDSIA Corso Elvezia 36 6900 Lugano, Switzerland juergen@idsia ch http://www idsia ch/~juergen Abstract Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insu cient, decaying error backflow  SENTENCE_END
SENTENCE_START We brie y review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, e cient, gradient-based method called \Long Short-Term Memory" (LSTM)  SENTENCE_END
SENTENCE_START Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant errorflow through \constant error carrousels" within special units  SENTENCE_END
SENTENCE_START Multiplicative gate units learn to open and close access to the constant errorflow  SENTENCE_END
SENTENCE_START LSTM is local in space and time; its computational complexity per time step and weight is O(1)  SENTENCE_END
SENTENCE_START Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations  SENTENCE_END
SENTENCE_START In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster  SENTENCE_END
SENTENCE_START LSTM also solves complex, artificial long time lag tasks that have never been solved by previous recurrent network algorithms  SENTENCE_END
SENTENCE_START 1 INTRODUCTION Recurrent networks can in principle use their feedback connections to store representations of recent input events in form of activations (\short-term memory", as opposed to \long-term memory" embodied by slowly changing weights)  SENTENCE_END
SENTENCE_START This is potentially significant for many applications, including speech processing, non-Markovian control, and music composition (e g , Mozer 1992)  SENTENCE_END
SENTENCE_START The most widely used algorithms for learning what to put in short-term memory, however, take too much time or do not work well at all, especially when minimal time lags between inputs and corresponding teacher signals are long  SENTENCE_END
SENTENCE_START Although theoretically fascinating, existing methods do not provide clear practical advantages over, say, backprop in feedforward nets with limited time windows  SENTENCE_END
SENTENCE_START This paper will review an analysis of the problem and suggest a remedy  SENTENCE_END
SENTENCE_START The problem  SENTENCE_END
SENTENCE_START With conventional \Back-Propagation Through Time" (BPTT, e g , Williams and Zipser 1992, Werbos 1988) or \Real-Time Recurrent Learning" (RTRL, e g , Robinson and Fallside 1987), error signals \ flowing backwards in time" tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error exponentially depends on the size of the weights (Hochreiter 1991)  SENTENCE_END
SENTENCE_START Case (1) may lead to oscillating weights, while in case (2) learning to bridge long time lags takes a prohibitive amount of time, or does not work at all (see section 3)  SENTENCE_END
SENTENCE_START The remedy  SENTENCE_END
SENTENCE_START This paper presents \Long Short-Term Memory" (LSTM), a novel recurrent network architecture in conjunction with an appropriate gradient-based learning algorithm  SENTENCE_END
SENTENCE_START LSTM is designed to overcome these error back-flow problems  SENTENCE_END
SENTENCE_START It can learn to bridge time intervals in excess of 1000 steps even in case of noisy, incompressible input sequences, without loss of short time lag capabilities  SENTENCE_END
SENTENCE_START This is achieved by an e cient, gradient-based algorithm for an architecture 1 enforcing constant (thus neither exploding nor vanishing) errorflow through internal states of special units (provided the gradient computation is truncated at certain architecture-specific points | this does not a ect long-term errorflow though)  SENTENCE_END
SENTENCE_START Outlineof paper  SENTENCE_END
SENTENCE_START Section 2 will brie y review previous work  SENTENCE_END
SENTENCE_START Section 3 begins with an outline of the detailed analysis of vanishing errors due to Hochreiter (1991)  SENTENCE_END
SENTENCE_START It will then introduce a naive approach to constant error backprop for didactic purposes, and highlight its problems concerning information storage and retrieval  SENTENCE_END
SENTENCE_START These problems will lead to the LSTM architecture as described in Section 4  SENTENCE_END
SENTENCE_START Section 5 will present numerous experiments and comparisons with competing methods  SENTENCE_END
SENTENCE_START LSTM outperforms them, and also learns to solve complex, artificial tasks no other recurrent net algorithm has solved  SENTENCE_END
SENTENCE_START Section 6 will discuss LSTM's limitations and advantages  SENTENCE_END
SENTENCE_START The appendix contains a detailed description of the algorithm (A 1), and explicit errorflow formulae (A 2)  SENTENCE_END
SENTENCE_START 2 PREVIOUS WORK This section will focus on recurrent nets with time-varying inputs (as opposed to nets with stationary inputs and xpoint-based gradient calculations, e g , Almeida 1987, Pineda 1987)  SENTENCE_END
SENTENCE_START Gradient-descent variants  SENTENCE_END
SENTENCE_START The approaches of Elman (1988), Fahlman (1991), Williams (1989), Schmidhuber (1992a), Pearlmutter (1989), and many of the related algorithms in Pearlmutter's comprehensive overview (1995) su er from the same problems as BPTT and RTRL (see Sections 1 and 3)  SENTENCE_END
SENTENCE_START Time-delays  SENTENCE_END
SENTENCE_START Other methods that seem practical for short time lags only are Time-Delay Neural Networks (Lang et al  SENTENCE_END
SENTENCE_START 1990) and Plate's method (Plate 1993), which updates unit activations based on a weighted sum of old activations (see also de Vries and Principe 1991)  SENTENCE_END
SENTENCE_START Lin et al  SENTENCE_END
SENTENCE_START (1995) propose variants of time-delay networks called NARX networks  SENTENCE_END
SENTENCE_START Time constants  SENTENCE_END
SENTENCE_START To deal with long time lags, Mozer (1992) uses time constants influencing changes of unit activations (deVries and Principe's above-mentioned approach (1991) may in fact be viewed as a mixture of TDNN and time constants)  SENTENCE_END
SENTENCE_START For long time lags, however, the time constants need external ne tuning (Mozer 1992)  SENTENCE_END
SENTENCE_START Sun et al  SENTENCE_END
SENTENCE_START 's alternative approach (1993) updates the activation of a recurrent unit by adding the old activation and the (scaled) current net input  SENTENCE_END
SENTENCE_START The net input, however, tends to perturb the stored information, which makes long-term storage impractical  SENTENCE_END
SENTENCE_START Ring's approach  SENTENCE_END
SENTENCE_START Ring (1993) also proposed a method for bridging long time lags  SENTENCE_END
SENTENCE_START Whenever a unit in his network receives con icting error signals, he adds a higher order unit in uencing appropriate connections  SENTENCE_END
SENTENCE_START Although his approach can sometimes be extremely fast, to bridge a time lag involving 100 steps may require the addition of 100 units  SENTENCE_END
SENTENCE_START Also, Ring's net does not generalize to unseen lag durations  SENTENCE_END
SENTENCE_START Bengio et al  SENTENCE_END
SENTENCE_START 's approaches  SENTENCE_END
SENTENCE_START Bengio et al  SENTENCE_END
SENTENCE_START (1994) investigate methods such as simulated annealing, multi-grid random search, time-weighted pseudo-Newton optimization, and discrete error propagation  SENTENCE_END
SENTENCE_START Their \latch" and \2-sequence" problems are very similar to problem 3a with minimal time lag 100(see Experiment 3)  SENTENCE_END
SENTENCE_START Bengioand Frasconi(1994)also proposean EM approach for propagating targets  SENTENCE_END
SENTENCE_START With n so-called \state networks", at a given time, their system can be in one of only n different states  SENTENCE_END
SENTENCE_START See also beginning of Section 5  SENTENCE_END
SENTENCE_START But to solve continuous problems such as the \adding problem" (Section 5 4), their system would require an unacceptable number of states (i e , state networks)  SENTENCE_END
SENTENCE_START Kalman lters  SENTENCE_END
SENTENCE_START Puskorius and Feldkamp (1994) use Kalman lter techniques to improve recurrent net performance  SENTENCE_END
SENTENCE_START Since they use \a derivative discount factor imposed to decay exponentially the e ects of past dynamic derivatives," there is no reason to believe that their Kalman Filter Trained Recurrent Networks will be useful for very long minimal time lags  SENTENCE_END
SENTENCE_START Second order nets  SENTENCE_END
SENTENCE_START We will see that LSTM uses multiplicative units (MUs) to protect errorflow from unwanted perturbations  SENTENCE_END
SENTENCE_START It is not the rst recurrent net method using MUs though  SENTENCE_END
SENTENCE_START For instance, Watrous and Kuhn (1992) use MUs in second order nets  SENTENCE_END
SENTENCE_START Some differences to LSTM are: (1) Watrous and Kuhn's architecture does not enforce constant errorflow and is not designed 2 to solve long time lag problems  SENTENCE_END
SENTENCE_START (2) It has fully connected second-order sigma-pi units, while the LSTM architecture's MUs are used only to gate access to constant errorflow  SENTENCE_END
SENTENCE_START (3) Watrous and Kuhn's algorithm costs O(W2) operations per time step, ours only O(W), where W is the number of weights  SENTENCE_END
SENTENCE_START See also Miller and Giles (1993) for additional work on MUs  SENTENCE_END
SENTENCE_START Simple weight guessing  SENTENCE_END
SENTENCE_START To avoid long time lag problems of gradient-based approaches we may simply randomly initialize all network weights until the resulting net happens to classify all training sequences correctly  SENTENCE_END
SENTENCE_START In fact, recently we discovered (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) that simple weight guessing solves many of the problems in (Bengio 1994, Bengio and Frasconi 1994, Miller and Giles 1993, Lin et al  SENTENCE_END
SENTENCE_START 1995) faster than the algorithms proposed therein  SENTENCE_END
SENTENCE_START This does not mean that weight guessing is a good algorithm  SENTENCE_END
SENTENCE_START It just means that the problems are very simple  SENTENCE_END
SENTENCE_START More realistic tasks require either many free parameters (e g , input weights) or high weight precision (e g , for continuous-valued parameters), such that guessing becomes completely infeasible  SENTENCE_END
SENTENCE_START Adaptive sequence chunkers  SENTENCE_END
SENTENCE_START Schmidhuber's hierarchical chunker systems (1992b, 1993) do have a capability to bridge arbitrary time lags, but only if there is local predictability across the subsequences causing the time lags (see also Mozer 1992)  SENTENCE_END
SENTENCE_START For instance, in his postdoctoral thesis (1993), Schmidhuber uses hierarchical recurrent nets to rapidly solve certain grammar learning tasks involving minimal time lags in excess of 1000 steps  SENTENCE_END
SENTENCE_START The performance of chunker systems, however, deteriorates as the noise level increases and the input sequences become less compressible  SENTENCE_END
SENTENCE_START LSTM does not suffer from this problem  SENTENCE_END
SENTENCE_START 3 CONSTANT ERROR BACKPROP 3 1 EXPONENTIALLY DECAYING ERROR Conventional BPTT (e g  SENTENCE_END
SENTENCE_START Williams and Zipser 1992)  SENTENCE_END
SENTENCE_START Output unit k's target at time t is denoted by dk(t)  SENTENCE_END
SENTENCE_START Using mean squared error, k's error signal is #k(t) = f0 k(netk(t))(dk(t) ?yk(t)); where yi(t) = fi(neti(t)) is the activation of a non-input unit i with differentiable activation function fi, neti(t) = X j wijyj(t?1) is unit i's current net input, and wij is the weight on the connection from unit j to i  SENTENCE_END
SENTENCE_START Some non-output unit j's backpropagated error signal is #j(t) = f0 j(netj(t)) X i wij#i(t + 1): The corresponding contribution to wjl's total weight update is #j(t)yl(t ? SENTENCE_END
SENTENCE_START 1), where is the learning rate, and l stands for an arbitrary unit connected to unit j  SENTENCE_END
SENTENCE_START Outline of Hochreiter's analysis (1991, page 19-21)  SENTENCE_END
SENTENCE_START Suppose we have a fully connected net whose non-input unit indices range from 1 to n  Let us focus on local errorflow from unit u to unit v (later we will see that the analysis immediately extends to global errorflow)  SENTENCE_END
SENTENCE_START The error occurring at an arbitrary unit u at time step t is propagated \back into time" for q time steps, to an arbitrary unit v  This will scale the error by the following factor: @#v(t?q) @#u(t) = ( f0 v(netv(t ?1))wuv q = 1 f0 v(netv(t ?q)) Pn l=1 @#l(t?q+1) @#u(t) wlv q > 1 : (1) 3 With lq = v and l0 = u, we obtain: @#v(t?q) @#u(t) = n X l1=1 ::: n X lq?1=1 q Y m=1 f0 lm(netlm(t ?m))wlmlm?1 (2) (proof by induction)  SENTENCE_END
SENTENCE_START The sum of the nq?1 terms Qq m=1 f0 lm(netlm(t?m))wlmlm?1 determines the total error back flow (note that since the summation terms may have different signs, increasing the number of units n does not necessarily increase errorflow)  SENTENCE_END
SENTENCE_START Intuitive explanation of equation (2)  SENTENCE_END
SENTENCE_START If jf0 lm(netlm(t ?m))wlmlm?1j > 1:0 for all m (as can happen, e g , with linear flm ) then the largest product increases exponentially with q  SENTENCE_END
SENTENCE_START That is, the error blows up, and con icting error signals arriving at unit v can lead to oscillating weights and unstable learning (for error blow-ups or bifurcations see also Pineda 1988, Baldi and Pineda 1991, Doya 1992)  SENTENCE_END
SENTENCE_START On the other hand, if jf0 lm(netlm(t ?m))wlmlm?1j < 1:0 for all m, then the largest product decreases exponentially with q  SENTENCE_END
SENTENCE_START That is, the error vanishes, and nothing can be learned in acceptable time  SENTENCE_END
SENTENCE_START If flm is the logistic sigmoid function, then the maximal value of f0 lm is 0 25  SENTENCE_END
SENTENCE_START If ylm?1 is constant and not equal to zero, then jf0 lm(netlm)wlmlm?1j takes on maximal values where wlmlm?1 = 1 ylm?1 coth(1 2netlm); goes to zero for jwlmlm?1j ! SENTENCE_END
SENTENCE_START 1, and is less than 1:0 for jwlmlm?1j < 4:0 (e g , if the absolute max- imal weight value wmax is smaller than 4 0)  SENTENCE_END
SENTENCE_START Hence with conventional logistic sigmoid activation functions, the errorflow tends to vanish as long as the weights have absolute values below 4 0, especially in the beginning of the training phase  SENTENCE_END
SENTENCE_START In general the use of larger initial weights will not help though | as seen above, for jwlmlm?1j ! SENTENCE_END
SENTENCE_START 1 the relevant derivative goes to zero \faster" than the absolute weight can grow (also, some weights will have to change their signs by crossing zero)  SENTENCE_END
SENTENCE_START Likewise, increasing the learning rate does not help either | it will not change the ratio of long-range errorflow and short-range errorflow  SENTENCE_END
SENTENCE_START BPTT is too sensitive to recent distractions  SENTENCE_END
SENTENCE_START (A very similar, more recent analysis was presented by Bengio et al  SENTENCE_END
SENTENCE_START 1994)  SENTENCE_END
SENTENCE_START Global errorflow  SENTENCE_END
SENTENCE_START The local errorflow analysis above immediately shows that global errorflow vanishes, too  SENTENCE_END
SENTENCE_START To see this, compute X u: u output unit @#v(t ?q) @#u(t) : Weak upper bound for scaling factor  SENTENCE_END
SENTENCE_START The following, slightly extended vanishing error analysis also takes n, the number of units, into account  SENTENCE_END
SENTENCE_START m)]ij := f0 i(neti(t ? SENTENCE_END
SENTENCE_START m)) otherwise  SENTENCE_END
SENTENCE_START Here T is the transposition operator, A]ij is the element in the i-th column and j-th row of matrix A, and x]i is the i-th component of vector x  SENTENCE_END
SENTENCE_START 4 Using a matrix norm k : kA compatible with vector norm k : kx, we de ne f0 max := maxm=1;:::;qfk F0(t?m) kAg: For maxi=1;:::;nfjxijg k x kx we get jxTyj n k x kx k y kx : Since jf0 v(netv(t ?q))j k F0(t?q) kA f0 max; we obtain the following inequality: j @#v(t ?q) @#u(t) j n (f0 max)q k Wv kx k WuT kx k W kq?2 A n(f0 max k W kA)q : This inequality results from k Wv kx = k Wev kx k W kA k ev kx k W kA and k WuT kx = k euW kx k W kA k eu kx k W kA; where ek is the unit vector whose components are 0 except for the k-th component, which is 1  SENTENCE_END
SENTENCE_START Note that this is a weak, extreme case upper bound | it will be reached only if all k F0(t?m) kA take on maximal values, and if the contributions of all paths across which errorflows back from unit u to unit v have the same sign  SENTENCE_END
SENTENCE_START Large k W kA, however, typically result in small values of k F0(t?m) kA, as con rmed by experiments (see, e g , Hochreiter 1991)  SENTENCE_END
SENTENCE_START For example, with norms k W kA := maxr X s jwrsj and k x kx:= maxrjxrj; we have f0 max = 0:25 for the logistic sigmoid  SENTENCE_END
SENTENCE_START We observe that if jwijj wmax < 4:0 n 8i;j; then k W kA nwmax < 4:0 will result in exponential decay | by setting := ?nwmax 4:0 < 1:0, we obtain j @#v(t ?q) @#u(t) j n( )q : We refer to Hochreiter's 1991 thesis for additional results  SENTENCE_END
SENTENCE_START 3 2 CONSTANT ERROR FLOW: NAIVE APPROACH A single unit  SENTENCE_END
SENTENCE_START To avoid vanishing error signals, how can we achieve constant errorflow through a single unit j with a single connection to itself? SENTENCE_END
SENTENCE_START According to the rules above, at time t, j's local error back flow is #j(t) = f0 j(netj(t))#j(t + 1)wjj  SENTENCE_END
SENTENCE_START To enforce constant errorflow through j, we require f0 j(netj(t))wjj = 1:0: Note the similarity to Mozer's xed time constant system (1992) | a time constant of 1:0 is appropriate for potentially in nite time lags1  SENTENCE_END
SENTENCE_START The constant error carrousel  SENTENCE_END
SENTENCE_START Integrating the differential equation above, we obtain fj(netj(t)) = netj(t) wjj for arbitrary netj(t)  SENTENCE_END
SENTENCE_START This means: fj has to be linear, and unit j's acti- vation has to remain constant: yj(t + 1) = fj(netj(t + 1)) = fj(wjjyj(t)) = yj(t): 1We do not use the expression \time constant" in the differential sense, as, e g , Pearlmutter (1995)  SENTENCE_END
SENTENCE_START 5 In the experiments, this will be ensured by using the identity function fj : fj(x) = x;8x, and by setting wjj = 1:0  SENTENCE_END
SENTENCE_START We refer to this as the constant error carrousel (CEC)  SENTENCE_END
SENTENCE_START CEC will be LSTM's central feature (see Section 4)  SENTENCE_END
SENTENCE_START Of course unit j will not only be connected to itself but also to other units  SENTENCE_END
SENTENCE_START This invokes two obvious, related problems (also inherent in all other gradient-based approaches): 1  SENTENCE_END
SENTENCE_START Input weight con ict: for simplicity, let us focus on a single additional input weight wji  SENTENCE_END
SENTENCE_START Assume that the total error can be reduced by switching on unit j in response to a certain input, and keeping it active for a long time (until it helps to compute a desired output)  SENTENCE_END
SENTENCE_START Provided i is non-zero, since the same incoming weight has to be used for both storing certain inputs and ignoring others, wji will often receive con icting weight update signals during this time (recall that j is linear): these signals will attempt to make wji participate in (1) storing the input (by switching on j) and (2) protecting the input (by preventing j from being switched o by irrelevant later inputs)  SENTENCE_END
SENTENCE_START This con ict makes learning difficult \long time lag errors"  SENTENCE_END
SENTENCE_START Again, this con ict makes learning difficult, and calls for a more context-sensitive mechanism for controlling \read operations" through output weights  SENTENCE_END
SENTENCE_START Of course, input and output weight conflicts are not specific for long time lags, but occur for short time lags as well  SENTENCE_END
SENTENCE_START Their e ects, however, become particularly pronounced in the long time lag case: as the time lag increases, (1) stored information must be protected against perturbation for longer and longer periods, and | especially in advanced stages of learning | (2) more and more already correct outputs also require protection against perturbation  SENTENCE_END
SENTENCE_START Due to the problems above the naive approach does not work well except in case of certain simple problems involving local input/output representations and non-repeating input patterns (see Hochreiter 1991 and Silva et al  SENTENCE_END
SENTENCE_START 1996)  SENTENCE_END
SENTENCE_START The next section shows how to do it right  SENTENCE_END
SENTENCE_START 4 LONG SHORT-TERM MEMORY Memory cells and gate units  SENTENCE_END
SENTENCE_START To construct an architecture that allows for constant errorflow through special, self-connected units without the disadvantages of the naive approach, we extend the constant error carrousel CEC embodied by the self-connected, linear unit j from Section 3 2 by introducing additional features  SENTENCE_END
SENTENCE_START A multiplicative input gate unit is introduced to protect the memory contents stored in j from perturbation by irrelevant inputs  SENTENCE_END
SENTENCE_START Likewise, a multiplicative output gate unit is introduced which protects other units from perturbation by currently irrelevant memory contents stored in j  SENTENCE_END
SENTENCE_START The resulting, more complex unit is called a memory cell (see Figure 1)  SENTENCE_END
SENTENCE_START The j-th memory cell is denoted cj  SENTENCE_END
SENTENCE_START Each memory cell is built around a central linear unit with a xed self-connection (the CEC)  SENTENCE_END
SENTENCE_START In addition to netcj, cj gets input from a multiplicative unit outj (the \output gate"), and from another multiplicative unit inj (the \input gate")  SENTENCE_END
SENTENCE_START inj's activation at time t is denoted by yinj(t), outj's by youtj(t)  SENTENCE_END
SENTENCE_START We have youtj(t) = foutj(netoutj(t));yinj(t) = finj(netinj(t)); 6 where netoutj(t) = X u woutjuyu(t ?1); and netinj(t) = X u winjuyu(t?1): We also have netcj(t) = X u wcjuyu(t?1): The summation indices u may stand for input units, gate units, memory cells, or even conventional hidden units if there are any (see also paragraph on \network topology" below)  SENTENCE_END
SENTENCE_START All these different types of units may convey useful information about the current state of the net  SENTENCE_END
SENTENCE_START For instance, an input gate (output gate) may use inputs from other memory cells to decide whether to store (access) certain information in its memory cell  SENTENCE_END
SENTENCE_START There even may be recurrent self-connections like wcjcj  SENTENCE_END
SENTENCE_START It is up to the user to de ne the network topology  SENTENCE_END
SENTENCE_START See Figure 2 for an example  SENTENCE_END
SENTENCE_START At time t, cj's output ycj(t) is computed as ycj(t) = youtj(t)h(scj(t)); where the \internal state" scj(t) is scj(0) = 0;scj(t) = scj(t?1) + yinj(t)g ?netcj(t) for t > 0: The differentiable function g squashes netcj; the differentiable function h scales memory cell outputs computed from the internal state scj  SENTENCE_END
SENTENCE_START inj inj outj outj w i c j wic j yc j g h 1 0 net w i w i yinj youtj net c j g yinj = g + sc j sc j yinj h youtj net Figure 1: Architecture of memory cell cj (the box) and its gate units inj;outj  SENTENCE_END
SENTENCE_START The self-recurrent connection (with weight 1 0) indicates feedback with a delay of 1 time step  SENTENCE_END
SENTENCE_START It builds the basis of the \constant error carrousel" CEC  SENTENCE_END
SENTENCE_START The gate units open and close access to CEC  SENTENCE_END
SENTENCE_START See text and appendix A 1 for details  SENTENCE_END
SENTENCE_START Why gate units? SENTENCE_END
SENTENCE_START To avoid input weight conflicts, inj controls the errorflow to memory cell cj's input connections wcji  SENTENCE_END
SENTENCE_START To circumvent cj's output weight conflicts, outj controls the errorflow from unit j's output connections  SENTENCE_END
SENTENCE_START In other words, the net can use inj to decide when to keep or override information in memory cell cj, and outj to decide when to access memory cell cj and when to prevent other units from being perturbed by cj (see Figure 1)  SENTENCE_END
SENTENCE_START Error signals trapped within a memory cell's CEC cannot change { but different error signals flowing into the cell (at different times) via its output gate may get superimposed  SENTENCE_END
SENTENCE_START The output gate will have to learn which errors to trap in its CEC, by appropriately scaling them  SENTENCE_END
SENTENCE_START The input 7 gate will have to learn when to release errors, again by appropriately scaling them  SENTENCE_END
SENTENCE_START Essentially, the multiplicative gate units open and close access to constant errorflow through CEC  SENTENCE_END
SENTENCE_START Distributed output representations typically do require output gates  SENTENCE_END
SENTENCE_START Not always are both gate types necessary, though | one may be su cient  SENTENCE_END
SENTENCE_START For instance, in Experiments 2a and 2b in Section 5, it will be possible to use input gates only  SENTENCE_END
SENTENCE_START In fact, output gates are not required in case of local output encoding | preventing memory cells from perturbing already learned outputs can be done by simply setting the corresponding weights to zero  SENTENCE_END
SENTENCE_START Even in this case, however, output gates can be beneficial: they prevent the net's attempts at storing long time lag memories (which are usually hard to learn) from perturbing activations representing easily learnable short time lag memories  SENTENCE_END
SENTENCE_START (This will prove quite useful in Experiment 1, for instance ) SENTENCE_END
SENTENCE_START Network topology  SENTENCE_END
SENTENCE_START We use networks with one input layer, one hidden layer, and one output layer  SENTENCE_END
SENTENCE_START The (fully) self-connected hidden layer contains memory cells and corresponding gate units (for convenience, we refer to both memory cells and gate units as being located in the hidden layer)  SENTENCE_END
SENTENCE_START The hidden layer may also contain \conventional" hidden units providing inputs to gate units and memory cells  SENTENCE_END
SENTENCE_START All units (except for gate units) in all layers have directed connections (serve as inputs) to all units in the layer above (or to all higher layers { Experiments 2a and 2b)  SENTENCE_END
SENTENCE_START Memory cell blocks  SENTENCE_END
SENTENCE_START S memory cells sharing the same input gate and the same output gate form a structure called a \memory cell block of size S"  SENTENCE_END
SENTENCE_START Memory cell blocks facilitate information storage | as with conventional neural nets, it is not so easy to code a distributed input within a single cell  SENTENCE_END
SENTENCE_START Since each memory cell block has as many gate units as a single memory cell (namely two), the block architecture can be even slightly more e cient (see paragraph \computational complexity")  SENTENCE_END
SENTENCE_START A memory cell block of size 1 is just a simple memory cell  SENTENCE_END
SENTENCE_START In the experiments (Section 5), we will use memory cell blocks of various sizes  SENTENCE_END
SENTENCE_START Learning  SENTENCE_END
SENTENCE_START We use a variant of RTRL (e g , Robinson and Fallside 1987) which properly takes into account the altered, multiplicative dynamics caused by input and output gates  SENTENCE_END
SENTENCE_START However, to ensure non-decaying error backprop through internal states of memory cells, as with truncated BPTT (e g , Williams and Peng 1990), errors arriving at \memory cell net inputs" (for cell cj, this includes netcj, netinj, netoutj) do not get propagated back further in time (although they do serve to change the incoming weights)  SENTENCE_END
SENTENCE_START Only within2 memory cells, errors are propagated back through previous internal states scj  SENTENCE_END
SENTENCE_START To visualize this: once an error signal arrives at a memory cell output, it gets scaled by output gate activation and h0  SENTENCE_END
SENTENCE_START Then it is within the memory cell's CEC, where it can flow back indefinitely without ever being scaled  SENTENCE_END
SENTENCE_START Only when it leaves the memory cell through the input gate and g, it is scaled once more by input gate activation and g0  SENTENCE_END
SENTENCE_START It then serves to change the incoming weights before it is truncated (see appendix for explicit formulae)  SENTENCE_END
SENTENCE_START Computational complexity  SENTENCE_END
SENTENCE_START As with Mozer's focused recurrent backprop algorithm (Mozer 1989), only the derivatives @scj @wil need to be stored and updated  SENTENCE_END
SENTENCE_START Hence the LSTM algorithm is very e cient, with an excellent update complexity of O(W), where W the number of weights (see details in appendix A 1)  SENTENCE_END
SENTENCE_START Hence, LSTM and BPTT for fully recurrent nets have the same update complexity per time step (while RTRL's is much worse)  SENTENCE_END
SENTENCE_START Unlike full BPTT, however, LSTM is local in space and time3: there is no need to store activation values observed during sequence processing in a stack with potentially unlimited size  SENTENCE_END
SENTENCE_START Abuse problem and solutions  SENTENCE_END
SENTENCE_START In the beginning of the learning phase, error reduction may be possible without storing information over time  SENTENCE_END
SENTENCE_START The network will thus tend to abuse memory cells, e g , as bias cells (i e , it might make their activations constant and use the outgoing connections as adaptive thresholds for other units)  SENTENCE_END
SENTENCE_START The potential difficulty is: it may take a long time to release abused memory cells and make them available for further learning  SENTENCE_END
SENTENCE_START A similar \abuse problem" appears if two memory cells store the same (redundant) information  SENTENCE_END
SENTENCE_START There are at least two solutions to the abuse problem: (1) Sequential network construction (e g , Fahlman 1991): a memory cell and the corresponding gate units are added to the network whenever the 2For intra-cellular backprop in a quite different context see also Doya and Yoshizawa (1989)  SENTENCE_END
SENTENCE_START 3Following Schmidhuber (1989), we say that a recurrent net algorithm is local in space if the update complexity per time step and weight does not depend on network size  SENTENCE_END
SENTENCE_START We say that a method is local in time if its storage requirements do not depend on input sequence length  SENTENCE_END
SENTENCE_START For instance, RTRL is local in time but not in space  SENTENCE_END
SENTENCE_START BPTT is local in space but not in time  SENTENCE_END
SENTENCE_START 8 1 1 2 output hidden input out 1 in 1 out 2 in 2 1 cell block block 1 cell block block 2 cell 2 cell 2 Figure 2: Example of a net with 8 input units, 4 output units, and 2 memory cell blocks of size 2  in1 marks the input gate, out1 marks the output gate, and cell1=block1 marks the rst memory cell of block 1  cell1=block1's architecture is identical to the one in Figure 1, with gate units in1 and out1 (note that by rotating Figure 1 by 90 degrees anti-clockwise, it will match with the corresponding parts of Figure 1)  SENTENCE_END
SENTENCE_START The example assumes dense connectivity: each gate unit and each memory cell see all non-output units  SENTENCE_END
SENTENCE_START For simplicity, however, outgoing weights of only one type of unit are shown for each layer  SENTENCE_END
SENTENCE_START With the e cient, truncated update rule, errorflows only through connections to output units, and through xed self-connections within cell blocks (not shown here | see Figure 1)  SENTENCE_END
SENTENCE_START Error flow is truncated once it \wants" to leave memory cells or gate units  SENTENCE_END
SENTENCE_START Therefore, no connection shown above serves to propagate error back to the unit from which the connection originates (except for connections to output units), although the connections themselves are modi able  SENTENCE_END
SENTENCE_START That's why the truncated LSTM algorithm is so e cient, despite its ability to bridge very long time lags  SENTENCE_END
SENTENCE_START See text and appendix A 1 for details  SENTENCE_END
SENTENCE_START Figure 2 actually shows the architecture used for Experiment 6a | only the bias of the non-input units is omitted  SENTENCE_END
SENTENCE_START error stops decreasing (see Experiment 2 in Section 5)  SENTENCE_END
SENTENCE_START (2) Output gate bias: each output gate gets a negative initial bias, to push initial memory cell activations towards zero  SENTENCE_END
SENTENCE_START Memory cells with more negative bias automatically get \allocated" later (see Experiments 1, 3, 4, 5, 6 in Section 5)  SENTENCE_END
SENTENCE_START Internal state drift and remedies  SENTENCE_END
SENTENCE_START If memory cell cj's inputs are mostly positive or mostly negative, then its internal state sj will tend to drift away over time  SENTENCE_END
SENTENCE_START This is potentially dangerous, for the h0(sj) will then adopt very small values, and the gradient will vanish  SENTENCE_END
SENTENCE_START One way to circumvent this problem is to choose an appropriate function h  But h(x) = x, for instance, has the disadvantage of unrestricted memory cell output range  SENTENCE_END
SENTENCE_START Our simple but e ective way of solving drift problems at the beginning of learning is to initially bias the input gate inj towards zero  SENTENCE_END
SENTENCE_START Although there is a tradeo between the magnitudes of h0(sj) on the one hand and of yinj and f0 inj on the other, the potential negative e ect of input gate bias is negligible compared to the one of the drifting e ect  SENTENCE_END
SENTENCE_START With logistic sigmoid activation functions, there appears to be no need for ne-tuning the initial bias, as con rmed by Experiments 4 and 5 in Section 5 4  SENTENCE_END
SENTENCE_START 5 EXPERIMENTS Introduction  SENTENCE_END
SENTENCE_START Which tasks are appropriate to demonstrate the quality of a novel long time lag 9 algorithm? SENTENCE_END
SENTENCE_START First of all, minimal time lags between relevant input signals and corresponding teacher signals must be long for all training sequences  SENTENCE_END
SENTENCE_START In fact, many previous recurrent net algorithms sometimes manage to generalize from very short training sequences to very long test sequences  SENTENCE_END
SENTENCE_START See, e g , Pollack (1991)  SENTENCE_END
SENTENCE_START But a real long time lag problem does not have any short time lag exemplars in the training set  SENTENCE_END
SENTENCE_START For instance, Elman's training procedure, BPTT, o ine RTRL, online RTRL, etc , fail miserably on real long time lag problems  SENTENCE_END
SENTENCE_START See, e g , Hochreiter (1991) and Mozer (1992)  SENTENCE_END
SENTENCE_START A second important requirement is that the tasks should be complex enough such that they cannot be solved quickly by simple-minded strategies such as random weight guessing  SENTENCE_END
SENTENCE_START Guessing can outperform many long time lag algorithms  SENTENCE_END
SENTENCE_START Recently we discovered (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) that many long time lag tasks used in previous work can be solved more quickly by simple random weight guessing than by the proposed algorithms  SENTENCE_END
SENTENCE_START For instance, guessing solved a variant of Bengio and Frasconi's \parity problem" (1994) problem much faster4 than the seven methods tested by Bengio et al  SENTENCE_END
SENTENCE_START (1994) and Bengio and Frasconi (1994)  SENTENCE_END
SENTENCE_START Similarly for some of Miller and Giles' problems (1993)  SENTENCE_END
SENTENCE_START Of course, this does not mean that guessing is a good algorithm  SENTENCE_END
SENTENCE_START It just means that some previously used problems are not extremely appropriate to demonstrate the quality of previously proposed algorithms  SENTENCE_END
SENTENCE_START What's common to Experiments 1{6  SENTENCE_END
SENTENCE_START All our experiments (except for Experiment 1) involve long minimal time lags | there are no short time lag training exemplars facilitating learning  SENTENCE_END
SENTENCE_START Solutions to most of our tasks are sparse in weight space  SENTENCE_END
SENTENCE_START They require either many parameters/inputs or high weight precision, such that random weight guessing becomes infeasible  SENTENCE_END
SENTENCE_START We always use on-line learning (as opposed to batch learning), and logistic sigmoids as activation functions  SENTENCE_END
SENTENCE_START For Experiments 1 and 2, initial weights are chosen in the range ?0:2;0:2], for the other experiments in ?0:1;0:1]  SENTENCE_END
SENTENCE_START Training sequences are generated randomly according to the various task descriptions  SENTENCE_END
SENTENCE_START In slight deviation from the notation in Appendix A1, each discrete time step of each input sequence involves three processing steps: (1) use current input to set the input units  SENTENCE_END
SENTENCE_START (2) Compute activations of hidden units (including input gates, output gates, memory cells)  SENTENCE_END
SENTENCE_START (3) Compute output unit activations  SENTENCE_END
SENTENCE_START Except for Experiments 1, 2a, and 2b, sequence elements are randomly generated on-line, and error signals are generated only at sequence ends  SENTENCE_END
SENTENCE_START Net activations are reset after each processed input sequence  SENTENCE_END
SENTENCE_START For comparisons with recurrent nets taught by gradient descent, we give results only for RTRL, except for comparison 2a, which also includes BPTT  SENTENCE_END
SENTENCE_START Note, however, that untruncated BPTT (see, e g , Williams and Peng 1990) computes exactly the same gradient as o ine RTRL  SENTENCE_END
SENTENCE_START With long time lag problems, o ine RTRL (or BPTT) and the online version of RTRL (no activation resets, online weight changes) lead to almost identical, negative results (as con rmed by additional simulations in Hochreiter 1991; see also Mozer 1992)  SENTENCE_END
SENTENCE_START This is because o ine RTRL, online RTRL, and full BPTT all su er badly from exponential error decay  SENTENCE_END
SENTENCE_START Our LSTM architectures are selected quite arbitrarily  SENTENCE_END
SENTENCE_START If nothing is known about the complexity of a given problem, a more systematic approach would be: start with a very small net consisting of one memory cell  SENTENCE_END
SENTENCE_START If this does not work, try two cells, etc  SENTENCE_END
SENTENCE_START Alternatively, use sequential network construction (e g , Fahlman 1991)  SENTENCE_END
SENTENCE_START Outline of experiments  SENTENCE_END
SENTENCE_START Experiment 1 focuses on a standard benchmark test for recurrent nets: the embedded Reber grammar  SENTENCE_END
SENTENCE_START Since it allows for training sequences with short time lags, it is not a long time lag problem  SENTENCE_END
SENTENCE_START We include it because (1) it provides a nice example where LSTM's output gates are truly beneficial, and (2) it is a popular benchmark for recurrent nets that has been used by many authors | we want to include at least one experiment where conventional BPTT and RTRL do not fail completely (LSTM, however, clearly outperforms them)  SENTENCE_END
SENTENCE_START The embedded Reber grammar's minimal time lags represent a border case in the sense that it is still possible to learn to bridge them with conventional algorithms  SENTENCE_END
SENTENCE_START Only slightly longer 4It should be mentioned, however, that different input representations and different types of noise may lead to worse guessing performance (Yoshua Bengio, personal communication, 1996)  SENTENCE_END
SENTENCE_START 10 minimal time lags would make this almost impossible  SENTENCE_END
SENTENCE_START The more interesting tasks in our paper, however, are those that RTRL, BPTT, etc  SENTENCE_END
SENTENCE_START cannot solve at all  SENTENCE_END
SENTENCE_START Experiment 2 focuses on noise-free and noisy sequences involving numerous input symbols distracting from the few important ones  SENTENCE_END
SENTENCE_START The most difficult task (Task 2c) involves hundreds of distractor symbols at random positions, and minimal time lags of 1000 steps  SENTENCE_END
SENTENCE_START LSTM solves it, while BPTT and RTRL already fail in case of 10-step minimal time lags (see also, e g , Hochreiter 1991 and Mozer 1992)  SENTENCE_END
SENTENCE_START For this reason RTRL and BPTT are omitted in the remaining, more complex experiments, all of which involve much longer time lags  SENTENCE_END
SENTENCE_START Experiment 3 addresses long time lag problems with noise and signal on the same input line  SENTENCE_END
SENTENCE_START Experiments 3a/3b focus on Bengio et al  SENTENCE_END
SENTENCE_START 's 1994 \2-sequence problem"  SENTENCE_END
SENTENCE_START Because this problem actually can be solved quickly by random weight guessing, we also include a far more difficult 2-sequence problem (3c) which requires to learn real-valued, conditional expectations of noisy targets, given the inputs  SENTENCE_END
SENTENCE_START Experiments 4 and 5 involve distributed, continuous-valued input representations and require learning to store precise, real values for very long time periods  SENTENCE_END
SENTENCE_START Relevant input signals can occur at quite different positions in input sequences  SENTENCE_END
SENTENCE_START Again minimal time lags involve hundreds of steps  SENTENCE_END
SENTENCE_START Similar tasks never have been solved by other recurrent net algorithms  SENTENCE_END
SENTENCE_START Experiment 6 involves tasks of a different complex type that also has not been solved by other recurrent net algorithms  SENTENCE_END
SENTENCE_START Again, relevant input signals can occur at quite different positions in input sequences  SENTENCE_END
SENTENCE_START The experiment shows that LSTM can extract information conveyed by the temporal order of widely separated inputs  SENTENCE_END
SENTENCE_START Subsection 5 7 will provide a detailed summary of experimental conditions in two tables for reference  SENTENCE_END
SENTENCE_START 5 1 EXPERIMENT 1: EMBEDDED REBER GRAMMAR Task  SENTENCE_END
SENTENCE_START Our rst task is to learn the \embedded Reber grammar", e g  SENTENCE_END
SENTENCE_START Smith and Zipser (1989), Cleeremans et al  SENTENCE_END
SENTENCE_START (1989), and Fahlman (1991)  SENTENCE_END
SENTENCE_START Since it allows for training sequences with short time lags (of as few as 9 steps), it is not a long time lag problem  SENTENCE_END
SENTENCE_START We include it for two reasons: (1) it is a popular recurrent net benchmark used by many authors | we wanted to have at least one experiment where RTRL and BPTT do not fail completely, and (2) it shows nicely how output gates can be beneficial  SENTENCE_END
SENTENCE_START B T S X X P V T P V S E Figure 3: Transition diagram for the Reber grammar  SENTENCE_END
SENTENCE_START B T P E T P GRAMMAR GRAMMAR REBER REBER Figure 4: Transition diagram for the embedded Reber grammar  SENTENCE_END
SENTENCE_START Each box represents a copy of the Reber grammar (see Figure 3)  SENTENCE_END
SENTENCE_START Starting at the leftmost node of the directed graph in Figure 4, symbol strings are generated sequentially (beginning with the empty string) by following edges | and appending the associated 11 symbols to the current string | until the rightmost node is reached  SENTENCE_END
SENTENCE_START Edges are chosen randomly if there is a choice (probability: 0 5)  SENTENCE_END
SENTENCE_START The net's task is to read strings, one symbol at a time, and to permanently predict the next symbol (error signals occur at every time step)  SENTENCE_END
SENTENCE_START To correctly predict the symbol before last, the net has to remember the second symbol  SENTENCE_END
SENTENCE_START Comparison  SENTENCE_END
SENTENCE_START We compare LSTM to \Elman nets trained by Elman's training procedure" (ELM) (results taken from Cleeremans et al  SENTENCE_END
SENTENCE_START 1989), Fahlman's \Recurrent Cascade-Correlation" (RCC) (results taken from Fahlman 1991), and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed)  SENTENCE_END
SENTENCE_START It should be mentioned that Smith and Zipser actually make the task easier by increasing the probability of short time lag exemplars  SENTENCE_END
SENTENCE_START We didn't do this for LSTM  SENTENCE_END
SENTENCE_START Training/Testing  SENTENCE_END
SENTENCE_START We use a local input/output representation (7 input units, 7 output units)  SENTENCE_END
SENTENCE_START Following Fahlman, we use 256 training strings and 256 separate test strings  SENTENCE_END
SENTENCE_START The training set is generated randomly; training exemplars are picked randomly from the training set  SENTENCE_END
SENTENCE_START Test sequences are generated randomly, too, but sequences already used in the training set are not used for testing  SENTENCE_END
SENTENCE_START After string presentation, all activations are reinitialized with zeros  SENTENCE_END
SENTENCE_START A trial is considered successful if all string symbols of all sequences in both test set and training set are predicted correctly | that is, if the output unit(s) corresponding to the possible next symbol(s) is(are) always the most active ones  SENTENCE_END
SENTENCE_START Architectures  SENTENCE_END
SENTENCE_START Architectures for RTRL, ELM, RCC are reported in the references listed above  SENTENCE_END
SENTENCE_START For LSTM, we use 3 (4) memory cell blocks  SENTENCE_END
SENTENCE_START Each block has 2 (1) memory cells  SENTENCE_END
SENTENCE_START The output layer's only incoming connections originate at memory cells  SENTENCE_END
SENTENCE_START Each memory cell and each gate unit receives incoming connections from all memory cells and gate units (the hidden layer is fully connected | less connectivity may work as well)  SENTENCE_END
SENTENCE_START The input layer has forward connections to all units in the hidden layer  SENTENCE_END
SENTENCE_START The gate units are biased  SENTENCE_END
SENTENCE_START These architecture parameters make it easy to store at least 3 input signals (architectures 3-2 and 4-1 are employed to obtain comparable numbers of weights for both architectures: 264 for 4-1 and 276 for 3-2)  SENTENCE_END
SENTENCE_START Other parameters may be appropriate as well, however  SENTENCE_END
SENTENCE_START All sigmoid functions are logistic with output range 0;1], except for h, whose range is ?1;1], and g, whose range is ?2;2]  SENTENCE_END
SENTENCE_START All weights are initialized in ?0:2;0:2], except for the output gate biases, which are initialized to -1, -2, and -3, respectively (see abuse problem, solution (2) of Section 4)  SENTENCE_END
SENTENCE_START We tried learning rates of 0 1, 0 2 and 0 5  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START We use 3 different, randomly generated pairs of training and test sets  SENTENCE_END
SENTENCE_START With each such pair we run 10 trials with different initial weights  SENTENCE_END
SENTENCE_START See Table 1 for results (mean of 30 trials)  SENTENCE_END
SENTENCE_START Unlike the other methods, LSTM always learns to solve the task  SENTENCE_END
SENTENCE_START Even when we ignore the unsuccessful trials of the other approaches, LSTM learns much faster  SENTENCE_END
SENTENCE_START Importanceof output gates  SENTENCE_END
SENTENCE_START The experiment provides a nice example where the output gate is truly beneficial  SENTENCE_END
SENTENCE_START Learning to store the rst T or P should not perturb activations representing the more easily learnable transitions of the original Reber grammar  SENTENCE_END
SENTENCE_START This is the job of the output gates  SENTENCE_END
SENTENCE_START Without output gates, we did not achieve fast learning  SENTENCE_END
SENTENCE_START 5 2 EXPERIMENT 2: NOISE-FREE AND NOISY SEQUENCES Task 2a: noise-free sequences with long time lags  SENTENCE_END
SENTENCE_START There are p+1 possible input symbols denoted a1;:::;ap?1;ap = x;ap+1 = y  ai is \locally" represented by the p + 1-dimensional vector whose i-th component is 1 (all other components are 0)  SENTENCE_END
SENTENCE_START A net with p + 1 input units and p + 1 output units sequentially observes input symbol sequences, one at a time, permanently trying to predict the next symbol | error signals occur at every single time step  SENTENCE_END
SENTENCE_START To emphasize the \long time lag problem", we use a training set consisting of only two very similar sequences: (y;a1;a2;:::;ap?1;y) and (x;a1;a2;:::;ap?1;x)  SENTENCE_END
SENTENCE_START Each is selected with probability 0 5  SENTENCE_END
SENTENCE_START To predict the nal element, the net has to learn to store a representation of the rst element for p time steps  SENTENCE_END
SENTENCE_START We compare \Real-Time Recurrent Learning" for fully recurrent nets (RTRL), \Back-Propagation Through Time" (BPTT), the sometimes very successful 2-net \Neural Sequence Chunker" (CH, Schmidhuber 1992b), and our new method (LSTM)  SENTENCE_END
SENTENCE_START In all cases, weights are initialized in -0 2,0 2]  SENTENCE_END
SENTENCE_START 1989), \Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and our new approach (LSTM)  SENTENCE_END
SENTENCE_START Weight numbers in the rst 4 rows are estimates | the corresponding papers do not provide all the technical details  SENTENCE_END
SENTENCE_START Only LSTM almost always learns to solve the task (only two failures out of 150 trials)  SENTENCE_END
SENTENCE_START Even when we ignore the unsuccessful trials of the other approaches, LSTM learns much faster (the number of required training examples in the bottom row varies between 3,800 and 24,100)  SENTENCE_END
SENTENCE_START tations  SENTENCE_END
SENTENCE_START A successful run is one that ful lls the following criterion: after training, during 10,000 successive, randomly chosen input sequences, the maximal absolute error of all output units is always below 0:25  SENTENCE_END
SENTENCE_START Architectures  SENTENCE_END
SENTENCE_START RTRL: one self-recurrent hidden unit, p+1 non-recurrent output units  SENTENCE_END
SENTENCE_START Each layer has connections from all layers below  SENTENCE_END
SENTENCE_START All units use the logistic activation function sigmoid in 0,1]  SENTENCE_END
SENTENCE_START BPTT: same architecture as the one trained by RTRL  SENTENCE_END
SENTENCE_START CH: both net architectures like RTRL's, but one has an additional output for predicting the hidden unit of the other one (see Schmidhuber 1992b for details)  SENTENCE_END
SENTENCE_START LSTM: like with RTRL, but the hidden unit is replaced by a memory cell and an input gate (no output gate required)  SENTENCE_END
SENTENCE_START g is the logistic sigmoid, and h is the identity function h : h(x) = x;8x  SENTENCE_END
SENTENCE_START Memory cell and input gate are added once the error has stopped decreasing (see abuse problem: solution (1) in Section 4)  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START Using RTRL and a short 4 time step delay (p = 4), 7 9 of all trials were successful  SENTENCE_END
SENTENCE_START No trial was successful with p = 10  SENTENCE_END
SENTENCE_START With long time lags, only the neural sequence chunker and LSTM achieved successful trials, while BPTT and RTRL failed  SENTENCE_END
SENTENCE_START With p = 100, the 2-net sequence chunker solved the task in only 1 3 of all trials  SENTENCE_END
SENTENCE_START LSTM, however, always learned to solve the task  SENTENCE_END
SENTENCE_START Comparing successful trials only, LSTM learned much faster  SENTENCE_END
SENTENCE_START See Table 2 for details  SENTENCE_END
SENTENCE_START It should be mentioned, however, that a hierarchical chunker can also always quickly solve this task (Schmidhuber 1992c, 1993)  SENTENCE_END
SENTENCE_START Task 2b: no local regularities  SENTENCE_END
SENTENCE_START With the task above, the chunker sometimes learns to correctly predict the nal element, but only because of predictable local regularities in the input stream that allow for compressing the sequence  SENTENCE_END
SENTENCE_START In an additional, more difficult task (involving many more different possible sequences), we remove compressibility by replacing the deterministic subsequence (a1;a2;:::;ap?1) by a random subsequence (of length p ? SENTENCE_END
SENTENCE_START 1) over the alphabet a1;a2;:::;ap?1  SENTENCE_END
SENTENCE_START We obtain 2 classes (two sets of sequences) f(y;ai1;ai2;:::;aip?1;y) j 1 i1;i2;:::;ip?1 p ?1g and f(x;ai1;ai2;:::;aip?1;x) j 1 i1;i2;:::;ip?1 p ?1g  SENTENCE_END
SENTENCE_START Again, every next sequence element has to be predicted  SENTENCE_END
SENTENCE_START The only totally predictable targets, however, are x and y, which occur at sequence ends  SENTENCE_END
SENTENCE_START Training exemplars are chosen randomly from the 2 classes  SENTENCE_END
SENTENCE_START Architectures and parameters are the same as in Experiment 2a  SENTENCE_END
SENTENCE_START Table entries refer to means of 18 trials  SENTENCE_END
SENTENCE_START With 100 time step delays, only CH and LSTM achieve successful trials  SENTENCE_END
SENTENCE_START Even when we ignore the unsuccessful trials of the other approaches, LSTM learns much faster  SENTENCE_END
SENTENCE_START sequences, the maximal absolute error of all output units is below 0:25 at sequence end  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START As expected, the chunker failed to solve this task (so did BPTT and RTRL, of course)  SENTENCE_END
SENTENCE_START LSTM, however, was always successful  SENTENCE_END
SENTENCE_START On average (mean of 18 trials), success for p = 100 was achieved after 5,680 sequence presentations  SENTENCE_END
SENTENCE_START This demonstrates that LSTM does not require sequence regularities to work well  SENTENCE_END
SENTENCE_START Task 2c: very long time lags | no local regularities  SENTENCE_END
SENTENCE_START This is the most difficult task in this subsection  SENTENCE_END
SENTENCE_START To our knowledge no other recurrent net algorithm can solve it  SENTENCE_END
SENTENCE_START Now there are p+4 possible input symbols denoted a1;:::;ap?1;ap;ap+1 = e;ap+2 = b;ap+3 = x;ap+4 = y  a1;:::;ap are also called \distractor symbols"  SENTENCE_END
SENTENCE_START Again, ai is locally represented by the p+4-dimensionalvector whose ith component is 1 (all other components are 0)  SENTENCE_END
SENTENCE_START A net with p+4 input units and 2 output units sequentially observes input symbol sequences, one at a time  SENTENCE_END
SENTENCE_START Training sequences arerandomly chosen from the union of two very similar subsets of sequences: f(b;y;ai1;ai2;:::;aiq+k;e;y) j 1 i1;i2;:::;iq+k qg and f(b;x;ai1;ai2;:::;aiq+k;e;x) j 1 i1;i2;:::;iq+k qg  SENTENCE_END
SENTENCE_START To produce a training sequence, we (1) randomly generate a sequence pre x of length q + 2, (2) randomly generate a sequence su x of additional elements (6= b;e;x;y) with probability 9 10 or, alternatively, an e with probability 1 10  SENTENCE_END
SENTENCE_START In the latter case, we (3) conclude the sequence with x or y, depending on the second element  SENTENCE_END
SENTENCE_START For a given k, this leads to a uniform distribution on the possible sequences with length q + k + 4  SENTENCE_END
SENTENCE_START The minimal sequence length is q + 4; the expected length is 4 + 1 X k=0 1 10( 9 10)k(q + k) = q + 14: The expected number of occurrences of element ai;1 i p, in a sequence is q+10 p q p  The goal is to predict the last symbol, which always occurs after the \trigger symbol" e  Error signals are generated only at sequence ends  SENTENCE_END
SENTENCE_START To predict the nal element, the net has to learn to store a representation of the second element for at least q +1 time steps (until it sees the trigger symbol e)  SENTENCE_END
SENTENCE_START Success is de ned as \prediction error (for nal sequence element) of both output units always below 0:2, for 10,000 successive, randomly chosen input sequences"  SENTENCE_END
SENTENCE_START Architecture/Learning  SENTENCE_END
SENTENCE_START The net has p + 4 input units and 2 output units  SENTENCE_END
SENTENCE_START Weights are initialized in -0 2,0 2]  SENTENCE_END
SENTENCE_START To avoid too much learning time variance due to different weight initializations, the hidden layer gets two memory cells (two cell blocks of size 1 | although one would be su cient)  SENTENCE_END
SENTENCE_START There are no other hidden units  SENTENCE_END
SENTENCE_START The output layer receives connections only from memory cells  SENTENCE_END
SENTENCE_START Memory cells and gate units receive connections from input units, memory cells and gate units (i e , the hidden layer is fully connected)  SENTENCE_END
SENTENCE_START No bias weights are used  SENTENCE_END
SENTENCE_START h and g are logistic sigmoids with output ranges ?1;1] and ?2;2], respectively  SENTENCE_END
SENTENCE_START The learning rate is 0 01  SENTENCE_END
SENTENCE_START p is the number of available distractor symbols (p + 4 is the number of input units)  SENTENCE_END
SENTENCE_START q p is the expected number of occurrences of a given distractor symbol in a sequence  SENTENCE_END
SENTENCE_START The rightmost column lists the number of training sequences required by LSTM (BPTT, RTRL and the other competitors have no chance of solving this task)  SENTENCE_END
SENTENCE_START If we let the number of distractor symbols (and weights) increase in proportion to the time lag, learning time increases very slowly  SENTENCE_END
SENTENCE_START The lower block illustrates the expected slow-down due to increased frequency of distractor symbols  SENTENCE_END
SENTENCE_START Note that the minimal time lag is q +1 | the net never sees short training sequences facilitating the classi cation of long test sequences  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START 20 trials were made for all tested pairs (p;q)  SENTENCE_END
SENTENCE_START Table 3 lists the mean of the number of training sequences required by LSTM to achieve success (BPTT and RTRL have no chance of solving non-trivial tasks with minimal time lags of 1000 steps)  SENTENCE_END
SENTENCE_START Scaling  SENTENCE_END
SENTENCE_START Table 3 shows that if we let the number of input symbols (and weights) increase in proportion to the time lag, learning time increases very slowly  SENTENCE_END
SENTENCE_START This is a another remarkable property of LSTM not shared by any other method we are aware of  SENTENCE_END
SENTENCE_START Indeed, RTRL and BPTT are far from scaling reasonably | instead, they appear to scale exponentially, and appear quite useless when the time lags exceed as few as 10 steps  SENTENCE_END
SENTENCE_START Distractor in uence  SENTENCE_END
SENTENCE_START In Table 3, the column headed by q p gives the expected frequency of distractor symbols  SENTENCE_END
SENTENCE_START Increasing this frequency decreases learning speed, an e ect due to weight oscillations caused by frequently observed input symbols  SENTENCE_END
SENTENCE_START 5 3 EXPERIMENT 3: NOISE AND SIGNAL ON SAME CHANNEL This experiment serves to illustrate that LSTM does not encounter fundamental problems if noise and signal are mixed on the same input line  SENTENCE_END
SENTENCE_START We initially focus on Bengio et al  SENTENCE_END
SENTENCE_START 's simple 1994 \2-sequence problem"; in Experiment 3c we will then pose a more challenging 2-sequence problem  SENTENCE_END
SENTENCE_START Task 3a (\2-sequence problem")  SENTENCE_END
SENTENCE_START The task is to observe and then classify input sequences  SENTENCE_END
SENTENCE_START There are two classes, each occurring with probability 0 5  SENTENCE_END
SENTENCE_START There is only one input line  SENTENCE_END
SENTENCE_START Only the rst N real-valued sequence elements convey relevant information about the class  SENTENCE_END
SENTENCE_START Sequence elements at positions t > N are generated by a Gaussian with mean zero and variance 0 2  SENTENCE_END
SENTENCE_START Case N = 1: the rst sequence element is 1 0 for class 1, and -1 0 for class 2  SENTENCE_END
SENTENCE_START Case N = 3: the rst three elements are 1 0 for class 1 and -1 0 for class 2  SENTENCE_END
SENTENCE_START The target at the sequence end is 1 0 for class 1 and 0 0 for class 2  SENTENCE_END
SENTENCE_START Correct classi cation is de ned as \absolute output error at sequence end below 0 2"  SENTENCE_END
SENTENCE_START Given a constant T, the sequence length is randomly selected between T and T + T/10 (a difference to Bengio et al  SENTENCE_END
SENTENCE_START 's problem is that they also permit shorter sequences of length T/2)  SENTENCE_END
SENTENCE_START Guessing  SENTENCE_END
SENTENCE_START Bengio et al  SENTENCE_END
SENTENCE_START (1994) and Bengio and Frasconi (1994) tested 7 different methods on the 2-sequence problem  SENTENCE_END
SENTENCE_START 's 2-sequence problem  SENTENCE_END
SENTENCE_START T is minimal sequence length  SENTENCE_END
SENTENCE_START N is the number of information-conveying elements at sequence begin  SENTENCE_END
SENTENCE_START The column headed by ST1 (ST2) gives the number of sequence presentations required to achieve stopping criterion ST1 (ST2)  SENTENCE_END
SENTENCE_START The rightmost column lists the fraction of misclassified post-training sequences (with absolute error > 0 2) from a test set consisting of 2560 sequences (tested after ST2 was achieved)  SENTENCE_END
SENTENCE_START All values are means of 10 trials  SENTENCE_END
SENTENCE_START We discovered, however, that this problem is so simple that random weight guessing solves it faster than LSTM and any other method for which there are published results  SENTENCE_END
SENTENCE_START forms them all, because the problem is so simple5  SENTENCE_END
SENTENCE_START See Schmidhuber and Hochreiter (1996) and Hochreiter and Schmidhuber (1996, 1997) for additional results in this vein  SENTENCE_END
SENTENCE_START LSTM architecture  SENTENCE_END
SENTENCE_START We use a 3-layer net with 1 input unit, 1 output unit, and 3 cell blocks of size 1  SENTENCE_END
SENTENCE_START The output layer receives connections only from memory cells  SENTENCE_END
SENTENCE_START Memory cells and gate units receive inputs from input units, memory cells and gate units, and have bias weights  SENTENCE_END
SENTENCE_START Gate units and output unit are logistic sigmoid in 0;1], h in ?1;1], and g in ?2;2]  SENTENCE_END
SENTENCE_START Training/Testing  SENTENCE_END
SENTENCE_START All weights (except the bias weights to gate units) are randomly initialized in the range ?0:1;0:1]  SENTENCE_END
SENTENCE_START The rst input gate bias is initialized with ?1:0, the second with ?3:0, and the third with ?5:0  SENTENCE_END
SENTENCE_START The rst output gate bias is initialized with ?2:0, the second with ?4:0 and the third with ?6:0  SENTENCE_END
SENTENCE_START The precise initialization values hardly matter though, as con rmed by additional experiments  SENTENCE_END
SENTENCE_START The learning rate is 1 0  SENTENCE_END
SENTENCE_START All activations are reset to zero at the beginning of a new sequence  SENTENCE_END
SENTENCE_START We stop training (and judge the task as being solved) according to the following criteria: ST1: none of 256 sequences from a randomly chosen test set is misclassified  SENTENCE_END
SENTENCE_START ST2: ST1 is satis ed, and mean absolute test set error is below 0 01  SENTENCE_END
SENTENCE_START In case of ST2, an additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassified sequences  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START See Table 4  SENTENCE_END
SENTENCE_START The results are means of 10 trials with different weight initializations in the range ?0:1;0:1]  SENTENCE_END
SENTENCE_START LSTM is able to solve this problem, though by far not as fast as random weight guessing (see paragraph \Guessing" above)  SENTENCE_END
SENTENCE_START Clearly, this trivial problem does not provide a very good testbed to compare performance of various non-trivial algorithms  SENTENCE_END
SENTENCE_START Still, it demonstrates that LSTM does not encounter fundamental problems when faced with signal and noise on the same channel  SENTENCE_END
SENTENCE_START Task 3b  SENTENCE_END
SENTENCE_START Architecture, parameters, etc  SENTENCE_END
SENTENCE_START like in Task 3a, but now with Gaussian noise (mean 0 and variance 0 2) added to the information-conveying elements (t <= N)  SENTENCE_END
SENTENCE_START We stop training (and judge the task as being solved) according to the following, slightly rede ned criteria: ST1: less than 6 out of 256 sequences from a randomly chosen test set are misclassified  SENTENCE_END
SENTENCE_START ST2: ST1 is satis ed, and mean absolute test set error is below 0 04  SENTENCE_END
SENTENCE_START In case of ST2, an additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassified sequences  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START See Table 5  SENTENCE_END
SENTENCE_START The results represent means of 10 trials with different weight initializations  SENTENCE_END
SENTENCE_START LSTM easily solves the problem  SENTENCE_END
SENTENCE_START Task 3c  SENTENCE_END
SENTENCE_START Architecture, parameters, etc  SENTENCE_END
SENTENCE_START like in Task 3a, but with a few essential changes that make the task non-trivial: the targets are 0 2 and 0 8 for class 1 and class 2, respectively, and there is Gaussian noise on the targets (mean 0 and variance 0 1; st dev  SENTENCE_END
SENTENCE_START 0 32)  SENTENCE_END
SENTENCE_START To minimize mean squared error, the system has to learn the conditional expectations of the targets given the inputs  SENTENCE_END
SENTENCE_START Misclassi cation is de ned as \absolute difference between output and noise-free target (0 2 for 5It should be mentioned, however, that different input representations and different types of noise may lead to worse guessing performance (Yoshua Bengio, personal communication, 1996)  SENTENCE_END
SENTENCE_START 16 T N stop: ST1 stop: ST2 # weights ST2: fraction misclassified 100 3 41,740 43,250 102 0 00828 100 1 74,950 78,430 102 0 01500 1000 1 481,060 485,080 102 0 01207 Table 5: Task 3b: modi ed 2-sequence problem  SENTENCE_END
SENTENCE_START Same as in Table 4, but now the information-conveying elements are also perturbed by noise  SENTENCE_END
SENTENCE_START T N stop # weights fraction misclassified av  SENTENCE_END
SENTENCE_START difference to mean 100 3 269,650 102 0 00558 0 014 100 1 565,640 102 0 00441 0 012 Table 6: Task 3c: modi ed, more challenging 2-sequence problem  SENTENCE_END
SENTENCE_START Same as in Table 4, but with noisy real-valued targets  SENTENCE_END
SENTENCE_START The system has to learn the conditional expectations of the targets given the inputs  SENTENCE_END
SENTENCE_START The rightmost column provides the average difference between network output and expected target  SENTENCE_END
SENTENCE_START Unlike 3a and 3b, this task cannot be solved quickly by random weight guessing  SENTENCE_END
SENTENCE_START class 1 and 0 8 for class 2) > 0 1  " SENTENCE_END
SENTENCE_START The network output is considered acceptable if the mean absolute difference between noise-free target and output is below 0 015  SENTENCE_END
SENTENCE_START Since this requires high weight precision, Task 3c (unlike 3a and 3b) cannot be solved quickly by random guessing  SENTENCE_END
SENTENCE_START Training/Testing  SENTENCE_END
SENTENCE_START The learning rate is 0:1  SENTENCE_END
SENTENCE_START We stop training according to the following criterion: none of 256 sequences from a randomly chosen test set is misclassified, and mean absolute difference between noise free target and output is below 0 015  SENTENCE_END
SENTENCE_START An additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassified sequences  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START See Table 6  SENTENCE_END
SENTENCE_START The results represent means of 10 trials with different weight initializations  SENTENCE_END
SENTENCE_START Despite the noisy targets, LSTM still can solve the problem by learning the expected target values  SENTENCE_END
SENTENCE_START 5 4 EXPERIMENT 4: ADDING PROBLEM The difficult task in this section is of a type that has never been solved by other recurrent net algorithms  SENTENCE_END
SENTENCE_START It shows that LSTM can solve long time lag problems involving distributed, continuous-valued representations  SENTENCE_END
SENTENCE_START Task  SENTENCE_END
SENTENCE_START Each element of each input sequence is a pair of components  SENTENCE_END
SENTENCE_START The rst component is a real value randomly chosen from the interval ?1;1]; the second is either 1 0, 0 0, or -1 0, and is used as a marker: at the end of each sequence, the task is to output the sum of the rst components of those pairs that are marked by second components equal to 1 0  SENTENCE_END
SENTENCE_START Sequences have random lengths between the minimal sequence length T and T + T 10  SENTENCE_END
SENTENCE_START In a given sequence exactly two pairs are marked as follows: we rst randomly select and mark one of the rst ten pairs (whose rst component we call X1)  SENTENCE_END
SENTENCE_START Then we randomly select and mark one of the rst T 2 ? SENTENCE_END
SENTENCE_START 1 still unmarked pairs (whose rst component we call X2)  SENTENCE_END
SENTENCE_START The second components of all remaining pairs are zero except for the rst and nal pair, whose second components are -1  SENTENCE_END
SENTENCE_START (In the rare case where the rst pair of the sequence gets marked, we set X1 to zero ) SENTENCE_END
SENTENCE_START An error signal is generated only at the sequence end: the target is 0:5+ X1+X2 4:0 (the sum X1 +X2 scaled to the interval 0;1])  SENTENCE_END
SENTENCE_START A sequence is processed correctly if the absolute error at the sequence end is below 0 04  SENTENCE_END
SENTENCE_START Architecture  SENTENCE_END
SENTENCE_START We use a 3-layer net with 2 input units, 1 output unit, and 2 cell blocks of size 2  SENTENCE_END
SENTENCE_START The output layer receives connections only from memory cells  SENTENCE_END
SENTENCE_START Memory cells and gate units receive inputs from memory cells and gate units (i e , the hidden layer is fully connected | less connectivity may work as well)  SENTENCE_END
SENTENCE_START The input layer has forward connections to all units in the hidden 17 T minimal lag # weights # wrong predictions Success after 100 50 93 1 out of 2560 74,000 500 250 93 0 out of 2560 209,000 1000 500 93 1 out of 2560 853,000 Table 7: EXPERIMENT 4: Results for the Adding Problem  SENTENCE_END
SENTENCE_START T is the minimal sequence length, T=2 the minimal time lag  SENTENCE_END
SENTENCE_START \# wrong predictions" is the number of incorrectly processed sequences (error > 0 04) from a test set containing 2560 sequences  SENTENCE_END
SENTENCE_START The rightmost column gives the number of training sequences required to achieve the stopping criterion  SENTENCE_END
SENTENCE_START All values are means of 10 trials  SENTENCE_END
SENTENCE_START For T = 1000 the number of required training examples varies between 370,000 and 2,020,000, exceeding 700,000 in only 3 cases  SENTENCE_END
SENTENCE_START layer  SENTENCE_END
SENTENCE_START All non-input units have bias weights  SENTENCE_END
SENTENCE_START These architecture parameters make it easy to store at least 2 input signals (a cell block size of 1 works well, too)  SENTENCE_END
SENTENCE_START All activation functions are logistic with output range 0;1], except for h, whose range is ?1;1], and g, whose range is ?2;2]  SENTENCE_END
SENTENCE_START State drift versus initial bias  SENTENCE_END
SENTENCE_START Note that the task requires storing the precise values of real numbers for long durations | the system must learn to protect memory cell contents against even minor internal state drift (see Section 4)  SENTENCE_END
SENTENCE_START To study the significance of the drift problem, we make the task even more difficult by biasing all non-input units, thus artificially inducing internal state drift  SENTENCE_END
SENTENCE_START All weights (including the bias weights) are randomly initialized in the range ?0:1;0:1]  SENTENCE_END
SENTENCE_START Following Section 4's remedy for state drifts, the rst input gate bias is initialized with ?3:0, the second with ?6:0 (though the precise values hardly matter, as con rmed by additional experiments)  SENTENCE_END
SENTENCE_START Training/Testing  SENTENCE_END
SENTENCE_START The learning rate is 0 5  SENTENCE_END
SENTENCE_START Training is stopped once the average training error is below 0 01, and the 2000 most recent sequences were processed correctly  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START With a test set consisting of 2560 randomly chosen sequences, the average test set error was always below 0 01, and there were never more than 3 incorrectly processed sequences  SENTENCE_END
SENTENCE_START Table 7 shows details  SENTENCE_END
SENTENCE_START The experiment demonstrates: (1) LSTM is able to work well with distributed representations  SENTENCE_END
SENTENCE_START (2) LSTM is able to learn to perform calculations involving continuous values  SENTENCE_END
SENTENCE_START (3) Since the system manages to store continuous values without deterioration for minimal delays of T 2 time steps, there is no significant, harmful internal state drift  SENTENCE_END
SENTENCE_START 5 5 EXPERIMENT 5: MULTIPLICATION PROBLEM One may argue that LSTM is a bit biased towards tasks such as the Adding Problem from the previous subsection  SENTENCE_END
SENTENCE_START Solutions to the Adding Problem may exploit the CEC's built-in integration capabilities  SENTENCE_END
SENTENCE_START Although this CEC property may be viewed as a feature rather than a disadvantage (integration seems to be a natural subtask of many tasks occurring in the real world), the question arises whether LSTM can also solve tasks with inherently non-integrative solutions  SENTENCE_END
SENTENCE_START To test this, we change the problem by requiring the nal target to equal the product (instead of the sum) of earlier marked inputs  SENTENCE_END
SENTENCE_START Task  SENTENCE_END
SENTENCE_START Like the task in Section 5 4, except that the rst component of each pair is a real value randomly chosen from the interval 0;1]  SENTENCE_END
SENTENCE_START In the rare case where the rst pair of the input sequence gets marked, we set X1 to 1 0  SENTENCE_END
SENTENCE_START The target at sequence end is the product X1 X2  SENTENCE_END
SENTENCE_START Architecture  SENTENCE_END
SENTENCE_START Like in Section 5 4  SENTENCE_END
SENTENCE_START All weights (including the bias weights) are randomly initialized in the range ?0:1;0:1]  SENTENCE_END
SENTENCE_START Training/Testing  SENTENCE_END
SENTENCE_START The learning rate is 0 1  SENTENCE_END
SENTENCE_START We test performance twice: as soon as less than nseq of the 2000 most recent training sequences lead to absolute errors exceeding 0 04, where nseq = 140, and nseq = 13  SENTENCE_END
SENTENCE_START Why these values? SENTENCE_END
SENTENCE_START nseq = 140 is su cient to learn storage of the relevant inputs  SENTENCE_END
SENTENCE_START It is not enough though to ne-tune the precise nal outputs  SENTENCE_END
SENTENCE_START nseq = 13, however, 18 T minimal lag # weights nseq # wrong predictions MSE Success after 100 50 93 140 139 out of 2560 0 0223 482,000 100 50 93 13 14 out of 2560 0 0139 1,273,000 Table 8: EXPERIMENT 5: Results for the Multiplication Problem  SENTENCE_END
SENTENCE_START T is the minimal sequence length, T=2 the minimal time lag  SENTENCE_END
SENTENCE_START We test on a test set containing 2560 sequences as soon as less than nseq of the 2000 most recent training sequences lead to error > 0 04  SENTENCE_END
SENTENCE_START \# wrong predictions" is the number of test sequences with error > 0 04  SENTENCE_END
SENTENCE_START MSE is the mean squared error on the test set  SENTENCE_END
SENTENCE_START The rightmost column lists numbers of training sequences required to achieve the stopping criterion  SENTENCE_END
SENTENCE_START All values are means of 10 trials  SENTENCE_END
SENTENCE_START leads to quite satisfactory results  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START For nseq = 140 (nseq = 13) with a test set consisting of 2560 randomly chosen sequences, the average test set error was always below 0 026 (0 013), and there were never more than 170 (15) incorrectly processed sequences  SENTENCE_END
SENTENCE_START Table 8 shows details  SENTENCE_END
SENTENCE_START (A net with additional standard hidden units or with a hidden layer above the memory cells may learn the ne-tuning part more quickly ) SENTENCE_END
SENTENCE_START The experiment demonstrates: LSTM can solve tasks involving both continuous-valued representations and non-integrative information processing  SENTENCE_END
SENTENCE_START 5 6 EXPERIMENT 6: TEMPORAL ORDER In this subsection, LSTM solves other difficult (but artificial) tasks that have never been solved by previous recurrent net algorithms  SENTENCE_END
SENTENCE_START The experiment shows that LSTM is able to extract information conveyed by the temporal order of widely separated inputs  SENTENCE_END
SENTENCE_START Task 6a: two relevant, widely separated symbols  SENTENCE_END
SENTENCE_START The goal is to classify sequences  SENTENCE_END
SENTENCE_START Elements and targets are represented locally (input vectors with only one non-zero bit)  SENTENCE_END
SENTENCE_START The sequence starts with an E, ends with a B (the \triggersymbol") and otherwise consists of randomly chosen symbols from the set fa;b;c;dgexcept for two elements at positions t1 and t2 that are either X or Y   SENTENCE_END
SENTENCE_START The sequence length is randomly chosen between 100 and 110, t1 is randomly chosen between 10 and 20, and t2 is randomly chosen between 50 and 60  SENTENCE_END
SENTENCE_START There are 4 sequence classes Q;R;S;U which depend on the temporal order of X and Y   SENTENCE_END
SENTENCE_START The rules are: X;X ! SENTENCE_END
SENTENCE_START Q; X;Y ! SENTENCE_END
SENTENCE_START R; Y;X ! SENTENCE_END
SENTENCE_START S; Y;Y ! SENTENCE_END
SENTENCE_START Task 6b: three relevant, widely separated symbols  SENTENCE_END
SENTENCE_START Again, the goal is to classify sequences  SENTENCE_END
SENTENCE_START Elements/targets are represented locally  SENTENCE_END
SENTENCE_START The sequence starts with an E, ends with a B (the \trigger symbol"), and otherwise consists of randomly chosen symbols from the set fa;b;c;dg except for three elements at positions t1;t2 and t3 that are either X or Y   SENTENCE_END
SENTENCE_START The sequence length is randomly chosen between 100 and 110, t1 is randomly chosen between 10 and 20, t2 is randomly chosen between 33 and 43, and t3 is randomly chosen between 66 and 76  SENTENCE_END
SENTENCE_START There are 8 sequence classes Q;R;S;U;V;A;B;C which depend on the temporal order of the Xs and Ys  SENTENCE_END
SENTENCE_START The rules are: X;X;X ! SENTENCE_END
SENTENCE_START Q; X;X;Y ! SENTENCE_END
SENTENCE_START R; X;Y;X ! SENTENCE_END
SENTENCE_START S; X;Y;Y ! SENTENCE_END
SENTENCE_START U; Y;X;X ! SENTENCE_END
SENTENCE_START V ; Y;X;Y ! SENTENCE_END
SENTENCE_START A; Y;Y;X ! SENTENCE_END
SENTENCE_START B; Y;Y;Y ! SENTENCE_END
SENTENCE_START C  There are as many output units as there are classes  SENTENCE_END
SENTENCE_START Each class is locally represented by a binary target vector with one non-zero component  SENTENCE_END
SENTENCE_START With both tasks, error signals occur only at the end of a sequence  SENTENCE_END
SENTENCE_START The sequence is classi ed correctly if the nal absolute error of all output units is below 0 3  SENTENCE_END
SENTENCE_START Architecture  SENTENCE_END
SENTENCE_START We use a 3-layer net with 8 input units, 2 (3) cell blocks of size 2 and 4 (8) output units for Task 6a (6b)  SENTENCE_END
SENTENCE_START Again all non-input units have bias weights, and the output layer receives connections from memory cells only  SENTENCE_END
SENTENCE_START Memory cells and gate units receive inputs from input units, memory cells and gate units (i e , the hidden layer is fully connected | less connectivity may work as well)  SENTENCE_END
SENTENCE_START The architecture parameters for Task 6a (6b) make it easy to 19 store at least 2 (3) input signals  SENTENCE_END
SENTENCE_START All activation functions are logistic with output range 0;1], except for h, whose range is ?1;1], and g, whose range is ?2;2]  SENTENCE_END
SENTENCE_START Training/Testing  SENTENCE_END
SENTENCE_START The learning rate is 0 5 (0 1) for Experiment 6a (6b)  SENTENCE_END
SENTENCE_START Training is stopped once the average training error falls below 0 1 and the 2000 most recent sequences were classi ed correctly  SENTENCE_END
SENTENCE_START All weights are initialized in the range ?0:1;0:1]  SENTENCE_END
SENTENCE_START The rst input gate bias is initialized with ?2:0, the second with ?4:0, and (for Experiment 6b) the third with ?6:0 (again, we con rmed by additional experiments that the precise values hardly matter)  SENTENCE_END
SENTENCE_START Results  SENTENCE_END
SENTENCE_START With a test set consisting of 2560 randomly chosen sequences, the average test set error was always below 0 1, and there were never more than 3 incorrectly classi ed sequences  SENTENCE_END
SENTENCE_START Table 9 shows details  SENTENCE_END
SENTENCE_START The experiment shows that LSTM is able to extract information conveyed by the temporal order of widely separated inputs  SENTENCE_END
SENTENCE_START In Task 6a, for instance, the delays between rst and second relevant input and between second relevant input and sequence end are at least 30 time steps  SENTENCE_END
SENTENCE_START task # weights # wrong predictions Success after Task 6a 156 1 out of 2560 31,390 Task 6b 308 2 out of 2560 571,100 Table 9: EXPERIMENT 6: Results for the Temporal Order Problem  SENTENCE_END
SENTENCE_START \# wrong predictions" is the number of incorrectly classi ed sequences (error > 0 3 for at least one output unit) from a test set containing 2560 sequences  SENTENCE_END
SENTENCE_START The rightmost column gives the number of training sequences required to achieve the stopping criterion  SENTENCE_END
SENTENCE_START The results for Task 6a are means of 20 trials; those for Task 6b of 10 trials  SENTENCE_END
SENTENCE_START Typical solutions  SENTENCE_END
SENTENCE_START In Experiment 6a, how does LSTM distinguish between temporal orders (X;Y ) and (Y;X)? SENTENCE_END
SENTENCE_START One of many possible solutions is to store the rst X or Y in cell block 1, and the second X=Y in cell block 2  SENTENCE_END
SENTENCE_START Before the rst X=Y occurs, block 1 can see that it is still empty by means of its recurrent connections  SENTENCE_END
SENTENCE_START After the rst X=Y, block 1 can close its input gate  SENTENCE_END
SENTENCE_START Once block 1 is lled and closed, this fact will become visible to block 2 (recall that all gate units and all memory cells receive connections from all non-output units)  SENTENCE_END
SENTENCE_START Typical solutions, however, require only one memory cell block  SENTENCE_END
SENTENCE_START The block stores the rst X or Y; once the second X=Y occurs, it changes its state depending on the rst stored symbol  SENTENCE_END
SENTENCE_START Solution type 1 exploits the connection between memory cell output and input gate unit | the following events cause different input gate activations: \X occurs in conjunction with a lled block"; \X occurs in conjunction with an empty block"  SENTENCE_END
SENTENCE_START Solution type 2 is based on a strong positive connection between memory cell output and memory cell input  SENTENCE_END
SENTENCE_START The previous occurrence of X (Y ) is represented by a positive (negative) internal state  SENTENCE_END
SENTENCE_START Once the input gate opens for the second time, so does the output gate, and the memory cell output is fed back to its own input  SENTENCE_END
SENTENCE_START This causes (X;Y) to be represented by a positive internal state, because X contributes to the new internal state twice (via current internal state and cell output feedback)  SENTENCE_END
SENTENCE_START Similarly, (Y;X) gets represented by a negative internal state  SENTENCE_END
SENTENCE_START 5 7 SUMMARY OF EXPERIMENTAL CONDITIONS The two tables in this subsection provide an overview of the most important LSTM parameters and architectural details for Experiments 1{6  SENTENCE_END
SENTENCE_START The conditions of the simple experiments 2a and 2b differ slightly from those of the other, more systematic experiments, due to historical reasons  SENTENCE_END
SENTENCE_START 1st column: task number  SENTENCE_END
SENTENCE_START 2nd column: minimal sequence length p  3rd column: minimal number of steps between most recent relevant input information and teacher signal  SENTENCE_END
SENTENCE_START 4th column: number of cell blocks b  SENTENCE_END
SENTENCE_START 5th column: block size s  6th column: number of input units in  SENTENCE_END
SENTENCE_START 7th column: number of output units out  SENTENCE_END
SENTENCE_START 8th column: number of weights w  9th column: c describes connectivity: \F" means \output layer receives connections from memory cells; memory cells and gate units receive connections from input units, memory cells and gate units"; \B" means \each layer receives connections from all layers below"  SENTENCE_END
SENTENCE_START 10th column: initial output gate bias ogb, where \r" stands for \randomly chosen from the interval ?0:1;0:1]" and \no og" means \no output gate used"  SENTENCE_END
SENTENCE_START 11th column: initial input gate bias igb (see 10th column)  SENTENCE_END
SENTENCE_START 12th column: which units have bias weights? SENTENCE_END
SENTENCE_START \b1" stands for \all hidden units", \ga" for \only gate units", and \all" for \all non-input units"  SENTENCE_END
SENTENCE_START 13th column: the function h, where \id" is identity function, \h1" is logistic sigmoid in ?2;2]  SENTENCE_END
SENTENCE_START 14th column: the logistic function g, where \g1" is sigmoid in 0;1], \g2" in ?1;1]  SENTENCE_END
SENTENCE_START 15th column: learning rate   SENTENCE_END
SENTENCE_START 1 2 3 4 5 6 Task select interval test set size stopping criterion success 1 t1 ?0:2;0:2] 256 training & test correctly pred  SENTENCE_END
SENTENCE_START see text 2a t1 ?0:2;0:2] no test set after 5 million exemplars ABS(0 25) 2b t2 ?0:2;0:2] 10000 after 5 million exemplars ABS(0 25) 2c t2 ?0:2;0:2] 10000 after 5 million exemplars ABS(0 2) 3a t3 ?0:1;0:1] 2560 ST1 and ST2 (see text) ABS(0 2) 3b t3 ?0:1;0:1] 2560 ST1 and ST2 (see text) ABS(0 2) 3c t3 ?0:1;0:1] 2560 ST1 and ST2 (see text) see text 4 t3 ?0:1;0:1] 2560 ST3(0 01) ABS(0 04) 5 t3 ?0:1;0:1] 2560 see text ABS(0 04) 6a t3 ?0:1;0:1] 2560 ST3(0 1) ABS(0 3) 6b t3 ?0:1;0:1] 2560 ST3(0 1) ABS(0 3) 21 Table 11: Summary of experimental conditions for LSTM, Part II  SENTENCE_END
SENTENCE_START 1st column: task number  SENTENCE_END
SENTENCE_START 2nd column: training exemplar selection, where \t1" stands for \randomly chosen from training set", \t2" for \randomly chosen from 2 classes", and \t3" for \randomly generated on-line"  SENTENCE_END
SENTENCE_START 3rd column: weight initialization interval  SENTENCE_END
SENTENCE_START 4th column: test set size  SENTENCE_END
SENTENCE_START 5th column: stopping criterion for training, where \ST3( )" stands for \average training error below and the 2000 most recent sequences were processed correctly"  SENTENCE_END
SENTENCE_START 6th column: success (correct classi cation) criterion, where \ABS( )" stands for \absolute error of all output units at sequence end is below "  SENTENCE_END
SENTENCE_START 6 DISCUSSION Limitations of LSTM  SENTENCE_END
SENTENCE_START The particularly e cient truncated backprop version of the LSTM algorithm will not easily solve problems similar to \strongly delayed XOR problems", where the goal is to compute the XOR of two widely separated inputs that previously occurred somewhere in a noisy sequence  SENTENCE_END
SENTENCE_START The reason is that storing only one of the inputs will not help to reduce the expected error | the task is non-decomposable in the sense that it is impossible to incrementally reduce the error by rst solving an easier subgoal  SENTENCE_END
SENTENCE_START In theory, this limitation can be circumvented by using the full gradient (perhaps with additional conventional hidden units receiving input from the memory cells)  SENTENCE_END
SENTENCE_START But we do not recommend computing the full gradient for the following reasons: (1) It increases computational complexity  SENTENCE_END
SENTENCE_START (2) Constant errorflow through CECs can be shown only for truncated LSTM  SENTENCE_END
SENTENCE_START (3) We actually did conduct a few experiments with non-truncated LSTM  SENTENCE_END
SENTENCE_START There was no significant difference to truncated LSTM, exactly because outside the CECs errorflow tends to vanish quickly  SENTENCE_END
SENTENCE_START For the same reason full BPTT does not outperform truncated BPTT  SENTENCE_END
SENTENCE_START Each memory cell block needs two additional units (input and output gate)  SENTENCE_END
SENTENCE_START In comparison to standard recurrent nets, however, this does not increase the number of weights by more than a factor of 9: each conventional hidden unit is replaced by at most 3 units in the LSTM architecture, increasing the number of weights by a factor of 32 in the fully connected case  SENTENCE_END
SENTENCE_START Note, however, that our experiments use quite comparable weight numbers for the architectures of LSTM and competing approaches  SENTENCE_END
SENTENCE_START Generally speaking, due to its constant errorflow through CECs within memory cells, LSTM runs into problems similar to those of feedforward nets seeing the entire input string at once  SENTENCE_END
SENTENCE_START For instance, there are tasks that can be quickly solved by random weight guessing but not by the truncated LSTM algorithm with small weight initializations, such as the 500-step parity problem (see introduction to Section 5)  SENTENCE_END
SENTENCE_START Here, LSTM's problems are similar to the ones of a feedforward net with 500 inputs, trying to solve 500-bit parity  SENTENCE_END
SENTENCE_START Indeed LSTM typically behaves much like a feedforward net trained by backprop that sees the entire input  SENTENCE_END
SENTENCE_START But that's also precisely why it so clearly outperforms previous approaches on many non-trivial tasks with significant search spaces  SENTENCE_END
SENTENCE_START LSTM does not have any problems with the notion of \recency" that go beyond those of other approaches  SENTENCE_END
SENTENCE_START All gradient-based approaches, however, su er from practical inability to precisely count discrete time steps  SENTENCE_END
SENTENCE_START If it makes a difference whether a certain signal occurred 99 or 100 steps ago, then an additional counting mechanism seems necessary  SENTENCE_END
SENTENCE_START Easier tasks, however, such as one that only requires to make a difference between, say, 3 and 11 steps, do not pose any problems to LSTM  SENTENCE_END
SENTENCE_START For instance, by generating an appropriate negative connection between memory cell output and input, LSTM can give more weight to recent inputs and learn decays where necessary  SENTENCE_END
SENTENCE_START 22 Advantages of LSTM  SENTENCE_END
SENTENCE_START The constant error backpropagation within memory cells results in LSTM's ability to bridge very long time lags in case of problems similar to those discussed above  SENTENCE_END
SENTENCE_START For long time lag problems such as those discussed in this paper, LSTM can handle noise, distributed representations, and continuous values  SENTENCE_END
SENTENCE_START In contrast to nite state automata or hidden Markov models LSTM does not require an a priori choice of a nite number of states  SENTENCE_END
SENTENCE_START In principle it can deal with unlimited state numbers  SENTENCE_END
SENTENCE_START For problems discussed in this paper LSTM generalizes well | even if the positions of widely separated, relevant inputs in the input sequence do not matter  SENTENCE_END
SENTENCE_START Unlike previous approaches, ours quickly learns to distinguish between two or more widely separated occurrences of a particular element in an input sequence, without depending on appropriate short time lag training exemplars  SENTENCE_END
SENTENCE_START There appears to be no need for parameter ne tuning  SENTENCE_END
SENTENCE_START LSTM works well over a broad range of parameters such as learning rate, input gate bias and output gate bias  SENTENCE_END
SENTENCE_START For instance, to some readers the learning rates used in our experiments may seem large  SENTENCE_END
SENTENCE_START However, a large learning rate pushes the output gates towards zero, thus automatically countermanding its own negative e ects  SENTENCE_END
SENTENCE_START The LSTM algorithm's update complexity per weight and time step is essentially that of BPTT, namely O(1)  SENTENCE_END
SENTENCE_START This is excellent in comparison to other approaches such as RTRL  SENTENCE_END
SENTENCE_START Unlike full BPTT, however, LSTM is local in both space and time  SENTENCE_END
SENTENCE_START 7 CONCLUSION Each memory cell's internal architecture guarantees constant errorflow within its constant error carrousel CEC, provided that truncated backprop cuts o errorflow trying to leak out of memory cells  SENTENCE_END
SENTENCE_START This represents the basis for bridging very long time lags  SENTENCE_END
SENTENCE_START Two gate units learn to open and close access to errorflow within each memory cell's CEC  SENTENCE_END
SENTENCE_START The multiplicative input gate a ords protection of the CEC from perturbation by irrelevant inputs  SENTENCE_END
SENTENCE_START Likewise, the multiplicative output gate protects other units from perturbation by currently irrelevant memory contents  SENTENCE_END
SENTENCE_START Future work  SENTENCE_END
SENTENCE_START To nd out about LSTM's practical limitations we intend to apply it to real world data  SENTENCE_END
SENTENCE_START Application areas will include (1) time series prediction, (2) music composition, and (3) speech processing  SENTENCE_END
SENTENCE_START It will also be interesting to augment sequence chunkers (Schmidhuber 1992b, 1993) by LSTM to combine the advantages of both  SENTENCE_END
SENTENCE_START 8 ACKNOWLEDGMENTS Thanks to Mike Mozer, Wilfried Brauer, Nic Schraudolph, and several anonymous referees for valuable comments and suggestions that helped to improve a previous version of this paper (Hochreiter and Schmidhuber 1995)  SENTENCE_END
SENTENCE_START This work was supported by DFG grant SCHM 942/3-1 from \Deutsche Forschungsgemeinschaft"  SENTENCE_END
SENTENCE_START APPENDIX A 1 ALGORITHM DETAILS In what follows, the index k ranges over output units, i ranges over hidden units, cj stands for the j-th memory cell block, cv j denotes the v-th unit of memory cell block cj, u;l;m stand for arbitrary units, t ranges over all time steps of a given input sequence  SENTENCE_END
SENTENCE_START 23 The gate unit logistic sigmoid (with range 0;1]) used in the experiments is f(x) = 1 1 + exp(?x)   SENTENCE_END
SENTENCE_START (3) The function h (with range ?1;1]) used in the experiments is h(x) = 2 1 + exp(?x) ?1   SENTENCE_END
SENTENCE_START (4) The function g (with range ?2;2]) used in the experiments is g(x) = 4 1 + exp(?x) ?2   SENTENCE_END
SENTENCE_START (5) Forward pass  SENTENCE_END
SENTENCE_START The net input and the activation of hidden unit i are neti(t) = X u wiuyu(t ?1) (6) yi(t) = fi(neti(t))   SENTENCE_END
SENTENCE_START The net input and the activation of inj are netinj(t) = X u winjuyu(t ?1) (7) yinj(t) = finj(netinj(t))   SENTENCE_END
SENTENCE_START The net input and the activation of outj are netoutj(t) = X u woutjuyu(t?1) (8) youtj(t) = foutj(netoutj(t))   SENTENCE_END
SENTENCE_START The net input netcv j , the internal state scv j , and the output activation ycv j of the v-th memory cell of memory cell block cj are: netcv j (t) = X u wcv juyu(t ?1) (9) scv j (t) = scv j (t ?1) + yinj(t)g netcv j (t) ycv j (t) = youtj(t)h(scv j (t))   SENTENCE_END
SENTENCE_START The net input and the activation of output unit k are netk(t) = X u: u not a gate wkuyu(t ?1) yk(t) = fk(netk(t))   SENTENCE_END
SENTENCE_START The backward pass to be described later is based on the following truncated backprop formulae  SENTENCE_END
SENTENCE_START Approximate derivatives for truncated backprop  SENTENCE_END
SENTENCE_START The truncated version (see Section 4) only approximates the partial derivatives, which is reflected by the \ tr" signs in the notation below  SENTENCE_END
SENTENCE_START It truncates errorflow once it leaves memory cells or gate units  SENTENCE_END
SENTENCE_START Truncation ensures that there are no loops across which an error that left some memory cell through its input or input gate can reenter the cell through its output or output gate  SENTENCE_END
SENTENCE_START This in turn ensures constant error flow through the memory cell's CEC  SENTENCE_END
SENTENCE_START tr (10) f0 k(netk(t)) 0 @X j Sj X v=1 cv jlwkcv j @ycv j (t ?1) @wlm + X j ? SENTENCE_END
SENTENCE_START injl + outjl Sj X v=1 wkcv j @ycv j (t ?1) @wlm + X i: i hidden unit wki @yi(t ?1) @wlm + klym(t ?1) ! SENTENCE_END
SENTENCE_START = f0 k(netk(t)) 8 > > > > > < > > > > > : ym(t?1) l = k wkcv j @ycv j (t?1) @wlm l = cv j PSj v=1 wkcv j @ycv j (t?1) @wlm l = inj OR l = outj P i: i hidden unit wki @yi(t?1) @wlm l otherwise , where is the Kronecker delta ( ab = 1 if a = b and 0 otherwise), and Sj is the size of memory cell block cj  SENTENCE_END
SENTENCE_START The truncated derivatives of a hidden unit i that is not part of a memory cell are: @yi(t) @wlm = f0 i(neti(t))@neti(t) @wlm tr lif0 i(neti(t))ym(t ?1)   SENTENCE_END
SENTENCE_START (11) (Note: here it would be possible to use the full gradient without a ecting constant errorflow through internal states of memory cells ) SENTENCE_END
SENTENCE_START 25 Cell block cj's truncated derivatives are: @yinj(t) @wlm = f0 inj(netinj(t)) @netinj(t) @wlm tr injlf0 inj(netinj(t))ym(t ?1)   SENTENCE_END
SENTENCE_START (12) @youtj(t) @wlm = f0 outj(netoutj(t)) @netoutj(t) @wlm tr outjlf0 outj(netoutj(t))ym(t ?1)   SENTENCE_END
SENTENCE_START @ycv j (t) @wlm = @youtj(t) @wlm h(scv j (t)) + h0(scv j (t)) @scv j (t) @wlm youtj(t) tr (15) outjl @youtj(t) @wlm h(scv j (t)) + injl + cv jl h0(scv j (t)) @scv j (t) @wlm youtj(t)   SENTENCE_END
SENTENCE_START To e ciently update the system at time t, the only (truncated) derivatives that need to be stored at time t?1 are @scv j (t?1) @wlm , where l = cv j or l = inj  SENTENCE_END
SENTENCE_START Backward pass  SENTENCE_END
SENTENCE_START We will describe the backward pass only for the particularly e cient \truncated gradient version" of the LSTM algorithm  SENTENCE_END
SENTENCE_START For simplicity we will use equal signs even where approximations are made according to the truncated backprop equations above  SENTENCE_END
SENTENCE_START The squared error at time t is given by E(t) = X k: k output unit ?tk(t) ?yk(t) 2 , (16) where tk(t) is output unit k's target at time t  Time t's contribution to wlm's gradient-based update with learning rate is wlm(t) = ? SENTENCE_END
SENTENCE_START @E(t) @wlm   SENTENCE_END
SENTENCE_START (17) We de ne some unit l's error at time step t by el(t) := ? SENTENCE_END
SENTENCE_START @E(t) @netl(t)   SENTENCE_END
SENTENCE_START (18) Using (almost) standard backprop, we rst compute updates for weights to output units (l = k), weights to hidden units (l = i) and weights to output gates (l = outj)  SENTENCE_END
SENTENCE_START We obtain (compare formulae (10), (11), (13)): l = k (output) : ek(t) = f0 k(netk(t)) ?tk(t) ?yk(t) , (19) l = i (hidden) : ei(t) = f0 i(neti(t)) X k: k output unit wkiek(t) , (20) 26 l = outj (output gates) : (21) eoutj(t) = f0 outj(netoutj(t)) 0 @ Sj X v=1 h(scv j (t)) X k: k output unit wkcv j ek(t) 1 A   SENTENCE_END
SENTENCE_START For all possible l time t's contribution to wlm's update is wlm(t) = el(t) ym(t?1)   SENTENCE_END
SENTENCE_START (22) The remaining updates for weights to input gates (l = inj) and to cell units (l = cv j) are less conventional  SENTENCE_END
SENTENCE_START We de ne some internal state scv j 's error: escv j := ? SENTENCE_END
SENTENCE_START @E(t) @scv j (t) = (23) foutj(netoutj(t)) h0(scv j (t)) X k: k output unit wkcv j ek(t)   SENTENCE_END
SENTENCE_START We obtain for l = inj or l = cv j; v = 1;:::;Sj ? SENTENCE_END
SENTENCE_START @E(t) @wlm = Sj X v=1 escv j (t) @scv j (t) @wlm   SENTENCE_END
SENTENCE_START (24) The derivatives of the internal states with respect to weights and the corresponding weight updates are as follows (compare expression (14)): l = inj (input gates) : (25) @scv j (t) @winjm = @scv j (t ?1) @winjm + g(netcv j (t)) f0 inj(netinj(t)) ym(t?1) ; therefore time t's contribution to winjm's update is (compare expression (10)): winjm(t) = Sj X v=1 escv j (t) @scv j (t) @winjm   SENTENCE_END
SENTENCE_START (26) Similarly we get (compare expression (14)): l = cv j (memory cells) : (27) @scv j (t) @wcv jm = @scv j (t ?1) @wcv jm + g0(netcv j (t)) finj(netinj(t)) ym(t ?1) ; therefore time t's contribution to wcv jm's update is (compare expression (10)): wcv jm(t) = escv j (t) @scv j (t) @wcv jm   SENTENCE_END
SENTENCE_START (28) All we need to implement for the backward pass are equations (19), (20), (21), (22), (23), (25), (26), (27), (28)  SENTENCE_END
SENTENCE_START Each weight's total update is the sum of the contributions of all time steps  SENTENCE_END
SENTENCE_START Computational complexity  SENTENCE_END
SENTENCE_START LSTM's update complexity per time step is O(KH + KCS + HI + CSI) = O(W); (29) where K is the number of output units, C is the number of memory cell blocks, S > 0 is the size of the memory cell blocks, H is the number of hidden units, I is the (maximal) number of units forward-connected to memory cells, gate units and hidden units, and W = KH + KCS + CSI + 2CI + HI = O(KH + KCS + CSI + HI) 27 is the number of weights  SENTENCE_END
SENTENCE_START Expression (29) is obtained by considering all computations of the backward pass: equation (19) needs K steps; (20) needs KH steps; (21) needs KSC steps; (22) needs K(H + C) steps for output units, HI steps for hidden units, CI steps for output gates; (23) needs KCS steps; (25) needs CSI steps; (26) needs CSI steps; (27) needs CSI steps; (28) needs CSI steps  SENTENCE_END
SENTENCE_START The total is K + 2KH + KC + 2KSC + HI + CI + 4CSI steps, or O(KH +KSC +HI +CSI) steps  SENTENCE_END
SENTENCE_START We conclude: LSTM algorithm's update complexity per time step is just like BPTT's for a fully recurrent net  SENTENCE_END
SENTENCE_START At a given time step, only the 2CSI most recent @scv j @wlm values from equations (25) and (27) need to be stored  SENTENCE_END
SENTENCE_START Hence LSTM's storage complexity also is O(W) | it does not depend on the input sequence length  SENTENCE_END
SENTENCE_START A 2 ERROR FLOW We compute how much an error signal is scaled while flowing back through a memory cell for q time steps  SENTENCE_END
SENTENCE_START As a by-product, this analysis recon rms that the errorflow within a memory cell's CEC is indeed constant, provided that truncated backprop cuts o errorflow trying to leave memory cells (see also Section 3 2)  SENTENCE_END
SENTENCE_START The analysis also highlights a potential for undesirable long-term drifts of scj (see (2) below), as well as the beneficial, countermanding in uence of negatively biased input gates (see (3) below)  SENTENCE_END
SENTENCE_START Using the truncated backprop learning rule, we obtain @scj(t ?k) @scj(t ?k ?1) = (30) 1 + @yinj(t ?k) @scj(t ?k ?1)g ?netcj(t?k) + yinj(t ?k)g0 ?netcj(t ?k) @netcj(t?k) @scj(t ?k ?1) = 1 + X u @yinj(t?k) @yu(t ?k ?1) @yu(t ?k ?1) @scj(t ?k ?1) g ?netcj(t ?k) + yinj(t ?k)g0 ?netcj(t ?k) X u @netcj(t ?k) @yu(t ?k ?1) @yu(t ?k ?1) @scj(t?k ?1) tr 1: The tr sign indicates equality due to the fact that truncated backprop replaces by zero the following derivatives: @yinj(t?k) @yu(t?k?1) 8u and @netcj(t?k) @yu(t?k?1) 8u  SENTENCE_END
SENTENCE_START In what follows, an error #j(t) starts flowing back at cj's output  SENTENCE_END
SENTENCE_START We rede ne #j(t) := X i wicj#i(t+ 1)   SENTENCE_END
SENTENCE_START (31) Following the definitions/conventions of Section 3 1, we compute error flow for the truncated backprop learning rule  SENTENCE_END
SENTENCE_START The error occurring at the output gate is #outj(t) tr @youtj(t) @netoutj(t) @ycj(t) @youtj(t)#j(t)   SENTENCE_END
SENTENCE_START (32) The error occurring at the internal state is #scj (t) = @scj(t+ 1) @scj(t) #scj (t+ 1) + @ycj(t) @scj(t)#j(t)   SENTENCE_END
SENTENCE_START (33) Since we use truncated backprop we have #j(t) = P i, i no gate and no memory cell wicj#i(t + 1); therefore we get @#j(t) @#scj (t + 1) = X i wicj @#i(t+ 1) @#scj (t+ 1) tr 0   SENTENCE_END
SENTENCE_START (34) 28 The previous equations (33) and (34) imply constant errorflow through internal states of memory cells: @#scj (t) @#scj (t+ 1) = @scj(t + 1) @scj(t) tr 1   SENTENCE_END
SENTENCE_START (35) The error occurring at the memory cell input is #cj(t) = @g(netcj(t)) @netcj(t) @scj(t) @g(netcj(t))#scj (t)   SENTENCE_END
SENTENCE_START (36) The error occurring at the input gate is #inj(t) tr @yinj(t) @netinj(t) @scj(t) @yinj(t))#scj (t)   SENTENCE_END
SENTENCE_START (37) No external errorflow  SENTENCE_END
SENTENCE_START Errors are propagated back from units l to unit v along outgoing connections with weights wlv  SENTENCE_END
SENTENCE_START This \external error" (note that for conventional units there is nothing but external error) at time t is #e v(t) = @yv(t) @netv(t) X l @netl(t + 1) @yv(t) #l(t + 1)   SENTENCE_END
SENTENCE_START (38) We obtain @#e v(t ?1) @#j(t) = (39) @yv(t?1) @netv(t ?1) @#outj(t) @#j(t) @netoutj(t) @yv(t ?1) + @#inj(t) @#j(t) @netinj(t) @yv(t ?1) + @#cj(t) @#j(t) @netcj(t) @yv(t ?1) tr 0   SENTENCE_END
SENTENCE_START We observe: the error #j arriving at the memory cell output is not backpropagated to units v via external connections to inj;outj;cj  SENTENCE_END
SENTENCE_START Error flow within memory cells  SENTENCE_END
SENTENCE_START We now focus on the error back flow within a memory cell's CEC  SENTENCE_END
SENTENCE_START This is actually the only type of errorflow that can bridge several time steps  SENTENCE_END
SENTENCE_START Suppose error #j(t) arrives at cj's output at time t and is propagated back for q steps until it reaches inj or the memory cell input g(netcj)  SENTENCE_END
SENTENCE_START It is scaled by a factor of @#v(t?q) @#j(t) , where v = inj;cj  SENTENCE_END
SENTENCE_START We rst compute @#scj (t ?q) @#j(t) tr 8 < : @ycj(t) @scj(t) q = 0 @scj(t?q+1) @scj(t?q) @#scj (t?q+1) @#j(t) q > 0   SENTENCE_END
SENTENCE_START (40) Expanding equation (40), we obtain @#v(t ?q) @#j(t) tr @#v(t?q) @#scj (t?q) @#scj (t ?q) @#j(t) tr (41) @#v(t?q) @#scj (t?q) 1 Y m=q @scj(t?m + 1) @scj(t ?m) ! SENTENCE_END
SENTENCE_START @ycj(t) @scj(t) tr youtj(t)h0(scj(t)) g0(netcj(t ?q)yinj(t?q) v = cj g(netcj(t?q)f0 inj(netinj(t?q)) v = inj   SENTENCE_END
SENTENCE_START Consider the factors in the previous equation's last expression  SENTENCE_END
SENTENCE_START Obviously, errorflow is scaled only at times t (when it enters the cell) and t ? SENTENCE_END
SENTENCE_START q (when it leaves the cell), but not in between (constant errorflow through the CEC)  SENTENCE_END
SENTENCE_START We observe: 29 (1) The output gate's e ect is: youtj(t) scales down those errors that can be reduced early during training without using the memory cell  SENTENCE_END
SENTENCE_START Likewise, it scales down those errors resulting from using (activating/deactivating) the memory cell at later training stages | without the output gate, the memory cell might for instance suddenly start causing avoidable errors in situations that already seemed under control (because it was easy to reduce the corresponding errors without memory cells)  SENTENCE_END
SENTENCE_START See \output weight con ict" and \abuse problem" in Sections 3/4  SENTENCE_END
SENTENCE_START (2) If there are large positive or negative scj(t) values (because scj has drifted since time step t ?q), then h0(scj(t)) may be small (assuming that h is a logistic sigmoid)  SENTENCE_END
SENTENCE_START See Section 4  SENTENCE_END
SENTENCE_START Drifts of the memory cell's internal state scj can be countermanded by negatively biasing the input gate inj (see Section 4 and next point)  SENTENCE_END
SENTENCE_START Recall from Section 4 that the precise bias value does not matter much  SENTENCE_END
SENTENCE_START (3) yinj(t ?q) and f0 inj(netinj(t ?q)) are small if the input gate is negatively biased (assume finj is a logistic sigmoid)  SENTENCE_END
SENTENCE_START However, the potential significance of this is negligible compared to the potential significance of drifts of the internal state scj  SENTENCE_END
SENTENCE_START Some of the factors above may scale down LSTM's overall error flow, but not in a manner that depends on the length of the time lag  SENTENCE_END
SENTENCE_START The flow will still be much more effective than an exponentially (of order q) decaying flow without memory cells  SENTENCE_END





SENTENCE_START Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation Kyunghyun Cho Bart van Merri enboer Caglar Gulcehre Universite de Montreal firstname lastname@umontreal ca Dzmitry Bahdanau Jacobs University, Germany d bahdanau@jacobs-university de Fethi Bougares Holger Schwenk Universit e du Maine, France firstname lastname@lium univ-lemans fr Yoshua Bengio Universite de Montreal, CIFAR Senior Fellow find me@on the web Abstract In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN)  SENTENCE_END
SENTENCE_START One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols  SENTENCE_END
SENTENCE_START The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence  SENTENCE_END
SENTENCE_START The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model  SENTENCE_END
SENTENCE_START Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases  SENTENCE_END
SENTENCE_START 1 Introduction Deep neural networks have shown great success in various applications such as objection recognition (see, e g , (Krizhevsky et al , 2012)) and speech recognition (see, e g , (Dahl et al , 2012))  SENTENCE_END
SENTENCE_START Furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing (NLP)  SENTENCE_END
SENTENCE_START These include, but are not limited to, language modeling (Bengio et al , 2003), paraphrase detection (Socher et al , 2011) and word embedding extraction (Mikolov et al , 2013)  SENTENCE_END
SENTENCE_START In the field of statistical machine translation (SMT), deep neural networks have begun to show promising results  SENTENCE_END
SENTENCE_START (Schwenk, 2012) summarizes a successful usage of feedforward neural networks in the framework of phrase-based SMT system  SENTENCE_END
SENTENCE_START Along this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as a part of the conventional phrase-based SMT system  SENTENCE_END
SENTENCE_START The proposed neural network architecture, which we will refer to as an RNN Encoder-Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair  SENTENCE_END
SENTENCE_START The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence  SENTENCE_END
SENTENCE_START The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence  SENTENCE_END
SENTENCE_START Additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training  SENTENCE_END
SENTENCE_START The proposed RNN Encoder-Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French  SENTENCE_END
SENTENCE_START We train the model to learn the translation probability of an English phrase to a corresponding French phrase  SENTENCE_END
SENTENCE_START The model is then used as a part of a standard phrase-based SMT system by scoring each phrase pair in the phrase table  SENTENCE_END
SENTENCE_START The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder-Decoder improves the translation performance  SENTENCE_END
SENTENCE_START We qualitatively analyze the trained RNN Encoder-Decoder by comparing its phrase scores with those given by the existing translation model  SENTENCE_END
SENTENCE_START The qualitative analysis shows that the RNN Encoder-Decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance  SENTENCE_END
SENTENCE_START The further analysis of the model reveals that the RNN Encoder-Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase  SENTENCE_END
SENTENCE_START arXiv:1406 1078v3 [cs CL] 3 Sep 2014 2 RNN Encoder-Decoder 2 1 Preliminary: Recurrent Neural Networks A recurrent neural network (RNN) is a neural network that consists of a hidden state h and an optional output y which operates on a variable-length sequence x = (x1,   SENTENCE_END
SENTENCE_START , xT )  SENTENCE_END
SENTENCE_START At each time step t, the hidden state h t of the RNN is updated by h t = f h t-1 , xt , (1) where f is a non-linear activation function  SENTENCE_END
SENTENCE_START f may be as simple as an element-wise logistic sigmoid function and as complex as a long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997)  SENTENCE_END
SENTENCE_START An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence  SENTENCE_END
SENTENCE_START In that case, the output at each timestep t is the conditional distribution p(xt | xt-1,   SENTENCE_END
SENTENCE_START , x1)  SENTENCE_END
SENTENCE_START For example, a multinomial distribution (1-of-K coding) can be output using a softmax activation function p(xt,j = 1 | xt-1,   SENTENCE_END
SENTENCE_START , x1) = exp wjh t K j =1 exp wj h t , (2) for all possible symbols j = 1,   SENTENCE_END
SENTENCE_START , K, where wj are the rows of a weight matrix W  By combining these probabilities, we can compute the probability of the sequence x using p(x) = T t=1 p(xt | xt-1,   SENTENCE_END
SENTENCE_START , x1)  SENTENCE_END
SENTENCE_START (3) From this learned distribution, it is straightforward to sample a new sequence by iteratively sam pling a symbol at each time step  SENTENCE_END
SENTENCE_START 2 2 RNN Encoder-Decoder In this paper, we propose a novel neural network architecture that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector representation back into a variable-length sequence  SENTENCE_END
SENTENCE_START From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence, e g  SENTENCE_END
SENTENCE_START p(y1,   SENTENCE_END
SENTENCE_START , yT | x1,   SENTENCE_END
SENTENCE_START , xT ), where one x1 x2 xT yT' y2 y1 c Decoder Encoder Figure 1: An illustration of the proposed RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START should note that the input and output sequence lengths T and T may differ  SENTENCE_END
SENTENCE_START The encoder is an RNN that reads each symbol of an input sequence x sequentially  SENTENCE_END
SENTENCE_START As it reads each symbol, the hidden state of the RNN changes according to Eq  SENTENCE_END
SENTENCE_START (1)  SENTENCE_END
SENTENCE_START After reading the end of the sequence (marked by an end-of-sequence symbol), the hidden state of the RNN is a summary c of the whole input sequence  SENTENCE_END
SENTENCE_START The decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol yt given the hidden state h t   SENTENCE_END
SENTENCE_START However, unlike the RNN described in Sec  SENTENCE_END
SENTENCE_START 2 1, both yt and h t are also conditioned on yt-1 and on the summary c of the input sequence  SENTENCE_END
SENTENCE_START Hence, the hidden state of the decoder at time t is computed by, h t = f h t-1 , yt-1, c , and similarly, the conditional distribution of the next symbol is P(yt|yt-1, yt-2,   SENTENCE_END
SENTENCE_START , y1, c) = g h t , yt-1, c   SENTENCE_END
SENTENCE_START for given activation functions f and g (the latter must produce valid probabilities, e g  SENTENCE_END
SENTENCE_START with a softmax)  SENTENCE_END
SENTENCE_START See Fig  SENTENCE_END
SENTENCE_START 1 for a graphical depiction of the proposed model architecture  SENTENCE_END
SENTENCE_START The two components of the proposed RNN Encoder-Decoder are jointly trained to maximize the conditional log-likelihood max 1 N N n=1 log p(yn | xn), (4) where  is the set of the model parameters and each (xn, yn) is an (input sequence, output sequence) pair from the training set  SENTENCE_END
SENTENCE_START In our case, as the output of the decoder, starting from the input, is differentiable, we can use a gradient-based algorithm to estimate the model parameters  SENTENCE_END
SENTENCE_START Once the RNN Encoder-Decoder is trained, the model can be used in two ways  SENTENCE_END
SENTENCE_START One way is to use the model to generate a target sequence given an input sequence  SENTENCE_END
SENTENCE_START On the other hand, the model can be used to score a given pair of input and output sequences, where the score is simply a probability p(y | x) from Eqs  SENTENCE_END
SENTENCE_START (3) and (4)  SENTENCE_END
SENTENCE_START 2 3 Hidden Unit that Adaptively Remembers and Forgets In addition to a novel model architecture, we also propose a new type of hidden unit (f in Eq  SENTENCE_END
SENTENCE_START (1)) that has been motivated by the LSTM unit but is much simpler to compute and implement 1 Fig  SENTENCE_END
SENTENCE_START 2 shows the graphical depiction of the proposed hidden unit  SENTENCE_END
SENTENCE_START Let us describe how the activation of the j-th hidden unit is computed  SENTENCE_END
SENTENCE_START First, the reset gate rj is computed by rj =  [Wrx]j + Urh t-1 j , (5) where  is the logistic sigmoid function, and [  SENTENCE_END
SENTENCE_START ]j denotes the j-th element of a vector  SENTENCE_END
SENTENCE_START x and ht-1 are the input and the previous hidden state, respectively  SENTENCE_END
SENTENCE_START Wr and Ur are weight matrices which are learned  SENTENCE_END
SENTENCE_START Similarly, the update gate zj is computed by zj =  [Wzx]j + Uzh t-1 j   SENTENCE_END
SENTENCE_START (6) The actual activation of the proposed unit hj is then computed by h t j = zjh t-1 j + (1 - zj) h t j , (7) where h t j =  [Wx]j + U r h t-1 j   SENTENCE_END
SENTENCE_START (8) In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the previous hidden state and reset with the current input 1 The LSTM unit, which has shown impressive results in several applications such as speech recognition, has a memory cell and four gating units that adaptively control the information flow inside the unit, compared to only two gating units in the proposed hidden unit  SENTENCE_END
SENTENCE_START For details on LSTM networks, see, e g , (Graves, 2012)  SENTENCE_END
SENTENCE_START z r h h ~ x Figure 2: An illustration of the proposed hidden activation function  SENTENCE_END
SENTENCE_START The update gate z selects whether the hidden state is to be updated with a new hidden state h  The reset gate r decides whether the previous hidden state is ignored  SENTENCE_END
SENTENCE_START See Eqs  SENTENCE_END
SENTENCE_START (5)-(8) for the detailed equations of r, z, h and h   only  SENTENCE_END
SENTENCE_START This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation  SENTENCE_END
SENTENCE_START On the other hand, the update gate controls how much information from the previous hidden state will carry over to the current hidden state  SENTENCE_END
SENTENCE_START This acts similarly to the memory cell in the LSTM network and helps the RNN to remember long-term information  SENTENCE_END
SENTENCE_START Furthermore, this may be considered an adaptive variant of a leaky-integration unit (Bengio et al , 2013)  SENTENCE_END
SENTENCE_START As each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales  SENTENCE_END
SENTENCE_START Those units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active  SENTENCE_END
SENTENCE_START In our preliminary experiments, we found that it is crucial to use this new unit with gating units  SENTENCE_END
SENTENCE_START We were not able to get meaningful result with an oft-used tanh unit without any gating  SENTENCE_END
SENTENCE_START 3 Statistical Machine Translation In a commonly used statistical machine translation system (SMT), the goal of the system (decoder, specifically) is to find a translation f given a source sentence e, which maximizes p(f | e)  p(e | f)p(f), where the first term at the right hand side is called translation model and the latter language model (see, e g , (Koehn, 2005))  SENTENCE_END
SENTENCE_START In practice, however, most SMT systems model log p(f | e) as a log-linear model with additional features and corresponding weights: log p(f | e) = N n=1 wnfn(f, e) + log Z(e), (9) where fn and wn are the n-th feature and weight, respectively  SENTENCE_END
SENTENCE_START Z(e) is a normalization constant that does not depend on the weights  SENTENCE_END
SENTENCE_START The weights are often optimized to maximize the BLEU score on a development set  SENTENCE_END
SENTENCE_START In the phrase-based SMT framework introduced in (Koehn et al , 2003) and (Marcu and Wong, 2002), the translation model log p(e | f) is factorized into the translation probabilities of matching phrases in the source and target sentences 2 These probabilities are once again considered additional features in the log-linear model (see Eq  SENTENCE_END
SENTENCE_START (9)) and are weighted accordingly to maximize the BLEU score  SENTENCE_END
SENTENCE_START Since the neural net language model was proposed in (Bengio et al , 2003), neural networks have been used widely in SMT systems  SENTENCE_END
SENTENCE_START In many cases, neural networks have been used to rescore translation hypotheses (n-best lists) (see, e g , (Schwenk et al , 2006))  SENTENCE_END
SENTENCE_START Recently, however, there has been interest in training neural networks to score the translated sentence (or phrase pairs) using a representation of the source sentence as an additional input  SENTENCE_END
SENTENCE_START See, e g , (Schwenk, 2012), (Son et al , 2012) and (Zou et al , 2013)  SENTENCE_END
SENTENCE_START 3 1 Scoring Phrase Pairs with RNN Encoder-Decoder Here we propose to train the RNN Encoder-Decoder (see Sec  SENTENCE_END
SENTENCE_START 2 2) on a table of phrase pairs and use its scores as additional features in the log-linear model in Eq  SENTENCE_END
SENTENCE_START (9) when tuning the SMT decoder  SENTENCE_END
SENTENCE_START When we train the RNN Encoder-Decoder, we ignore the (normalized) frequencies of each phrase pair in the original corpora  SENTENCE_END
SENTENCE_START This measure was taken in order (1) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and (2) to ensure that the RNN Encoder-Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences  SENTENCE_END
SENTENCE_START One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase 2 Without loss of generality, from here on, we refer to p(e | f) for each phrase pair as a translation model as well pairs in the original corpus  SENTENCE_END
SENTENCE_START With a fixed capacity of the RNN Encoder-Decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities, i e , distinguishing between plausible and implausible translations, or learning the "manifold" (region of probability concentration) of plausible translations  SENTENCE_END
SENTENCE_START Once the RNN Encoder-Decoder is trained, we add a new score for each phrase pair to the existing phrase table  SENTENCE_END
SENTENCE_START This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation  SENTENCE_END
SENTENCE_START As Schwenk pointed out in (Schwenk, 2012), it is possible to completely replace the existing phrase table with the proposed RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START In that case, for a given source phrase, the RNN Encoder-Decoder will need to generate a list of (good) target phrases  SENTENCE_END
SENTENCE_START This requires, however, an expensive sampling procedure to be performed repeatedly  SENTENCE_END
SENTENCE_START In this paper, thus, we only consider rescoring the phrase pairs in the phrase table  SENTENCE_END
SENTENCE_START 3 2 Related Approaches: Neural Networks in Machine Translation Before presenting the empirical results, we discuss a number of recent works that have proposed to use neural networks in the context of SMT  SENTENCE_END
SENTENCE_START Schwenk in (Schwenk, 2012) proposed a similar approach of scoring phrase pairs  SENTENCE_END
SENTENCE_START Instead of the RNN-based neural network, he used a feedforward neural network that has fixed-size inputs (7 words in his case, with zero-padding for shorter phrases) and fixed-size outputs (7 words in the target language)  SENTENCE_END
SENTENCE_START When it is used specifically for scoring phrases for the SMT system, the maximum phrase length is often chosen to be small  SENTENCE_END
SENTENCE_START However, as the length of phrases increases or as we apply neural networks to other variable-length sequence data, it is important that the neural network can handle variable-length input and output  SENTENCE_END
SENTENCE_START The pro- posed RNN Encoder-Decoder is well-suited for these applications  SENTENCE_END
SENTENCE_START Similar to (Schwenk, 2012), Devlin et al  SENTENCE_END
SENTENCE_START (Devlin et al , 2014) proposed to use a feedforward neural network to model a translation model, however, by predicting one word in a target phrase at a time  SENTENCE_END
SENTENCE_START They reported an impressive improvement, but their approach still requires the maxi- mum length of the input phrase (or context words) to be fixed a priori  SENTENCE_END
SENTENCE_START Although it is not exactly a neural network they train, the authors of (Zou et al , 2013) proposed to learn a bilingual embedding of words/phrases  SENTENCE_END
SENTENCE_START They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system  SENTENCE_END
SENTENCE_START In (Chandar et al , 2014), a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase  SENTENCE_END
SENTENCE_START This is closely related to both the proposed RNN Encoder-Decoder and the model proposed in (Schwenk, 2012), except that their input representation of a phrase is a bag-of-words  SENTENCE_END
SENTENCE_START A similar approach of using bag-of-words representations was proposed in (Gao et al , 2013) as well  SENTENCE_END
SENTENCE_START Earlier, a similar encoder-decoder model using two recursive neural networks was proposed in (Socher et al , 2011), but their model was restricted to a monolingual setting, i e  SENTENCE_END
SENTENCE_START the model reconstructs an input sentence  SENTENCE_END
SENTENCE_START More recently, another encoder-decoder model using an RNN was proposed in (Auli et al , 2013), where the decoder is conditioned on a representation of either a source sentence or a source context  SENTENCE_END
SENTENCE_START One important difference between the proposed RNN Encoder-Decoder and the approaches in (Zou et al , 2013) and (Chandar et al , 2014) is that the order of the words in source and target phrases is taken into account  SENTENCE_END
SENTENCE_START The RNN Encoder-Decoder naturally distinguishes between sequences that have the same words but in a differ- ent order, whereas the aforementioned approaches effectively ignore order information  SENTENCE_END
SENTENCE_START The closest approach related to the proposed RNN Encoder-Decoder is the Recurrent Continuous Translation Model (Model 2) proposed in (Kalchbrenner and Blunsom, 2013)  SENTENCE_END
SENTENCE_START In their paper, they proposed a similar model that consists of an encoder and decoder  SENTENCE_END
SENTENCE_START The difference with our model is that they used a convolutional n-gram model (CGM) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder  SENTENCE_END
SENTENCE_START They, however, evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations  SENTENCE_END
SENTENCE_START 4 Experiments We evaluate our approach on the English/French translation task of the WMT'14 workshop  SENTENCE_END
SENTENCE_START 4 1 Data and Baseline System Large amounts of resources are available to build an English/French SMT system in the framework of the WMT'14 translation task  SENTENCE_END
SENTENCE_START The bilingual corpora include Europarl (61M words), news commentary (5 5M), UN (421M), and two crawled corpora of 90M and 780M words respectively  SENTENCE_END
SENTENCE_START The last two corpora are quite noisy  SENTENCE_END
SENTENCE_START To train the French language model, about 712M words of crawled newspaper material is available in addition to the target side of the bitexts  SENTENCE_END
SENTENCE_START All the word counts refer to French words after tokenization  SENTENCE_END
SENTENCE_START It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance, and results in extremely large models which are difficult to handle  SENTENCE_END
SENTENCE_START Instead, one should focus on the most relevant subset of the data for a given task  SENTENCE_END
SENTENCE_START We have done so by applying the data selection method proposed in (Moore and Lewis, 2010), and its extension to bi-texts (Axelrod et al , 2011)  SENTENCE_END
SENTENCE_START By these means we selected a subset of 418M words out of more than 2G words for language modeling and a subset of 348M out of 850M words for training the RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT, and newstest2014 as our test set  SENTENCE_END
SENTENCE_START Each set has more than 70 thousand words and a single reference translation  SENTENCE_END
SENTENCE_START For training the neural networks, including the proposed RNN Encoder-Decoder, we limited the source and target vocabulary to the most frequent 15,000 words for both English and French  SENTENCE_END
SENTENCE_START This covers approximately 93% of the dataset  SENTENCE_END
SENTENCE_START All the out-of-vocabulary words were mapped to a special token ([UNK])  SENTENCE_END
SENTENCE_START The baseline phrase-based SMT system was built using Moses with default settings  SENTENCE_END
SENTENCE_START This system achieves a BLEU score of 30 64 and 33 3 on the development and test sets, respectively (see Table 1)  SENTENCE_END
SENTENCE_START 4 1 1 RNN Encoder-Decoder The RNN Encoder-Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder  SENTENCE_END
SENTENCE_START The input matrix between each input symbol x t and the hidden unit is approximated with two lower-rank matrices, and the output matrix is approximated Models BLEU dev test Baseline 30 64 33 30 RNN 31 20 33 87 CSLM + RNN 31 48 34 64 CSLM + RNN + WP 31 50 34 54 Table 1: BLEU scores computed on the development and test sets using different combinations of approaches  SENTENCE_END
SENTENCE_START WP denotes a word penalty, where we penalizes the number of unknown words to neural networks  SENTENCE_END
SENTENCE_START similarly  SENTENCE_END
SENTENCE_START We used rank-100 matrices, equivalent to learning an embedding of dimension 100 for each word  SENTENCE_END
SENTENCE_START The activation function used for h in Eq  SENTENCE_END
SENTENCE_START (8) is a hyperbolic tangent function  SENTENCE_END
SENTENCE_START The computation from the hidden state in the decoder to the output is implemented as a deep neural network (Pascanu et al , 2014) with a single intermediate layer having 500 maxout units each pooling 2 inputs (Goodfellow et al , 2013)  SENTENCE_END
SENTENCE_START All the weight parameters in the RNN Encoder-Decoder were initialized by sampling from an isotropic zero-mean (white) Gaussian distribution with its standard deviation fixed to 0 01, except for the recurrent weight parameters  SENTENCE_END
SENTENCE_START For the recurrent weight matrices, we first sampled from a white Gaussian distribution and used its left singular vectors matrix, following (Saxe et al , 2014)  SENTENCE_END
SENTENCE_START We used Adadelta and stochastic gradient descent to train the RNN Encoder-Decoder with hyperparameters = 10-6 and  = 0 95 (Zeiler, 2012)  SENTENCE_END
SENTENCE_START At each update, we used 64 randomly selected phrase pairs from a phrase table (which was created from 348M words)  SENTENCE_END
SENTENCE_START The model was trained for approximately three days  SENTENCE_END
SENTENCE_START Details of the architecture used in the experiments are explained in more depth in the supplementary material  SENTENCE_END
SENTENCE_START 4 1 2 Neural Language Model In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder-Decoder, we also tried a more traditional approach of using a neural network for learning a target language model (CSLM) (Schwenk, 2007)  SENTENCE_END
SENTENCE_START Especially, the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder-Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT system add up or are redundant  SENTENCE_END
SENTENCE_START We trained the CSLM model on 7-grams from the target corpus  SENTENCE_END
SENTENCE_START Each input word was projected into the embedding space R512, and they were concatenated to form a 3072-dimensional vector  SENTENCE_END
SENTENCE_START The concatenated vector was fed through two rectified layers (of size 1536 and 1024) (Glorot et al , 2011)  SENTENCE_END
SENTENCE_START The output layer was a simple softmax layer (see Eq  SENTENCE_END
SENTENCE_START (2))  SENTENCE_END
SENTENCE_START All the weight parameters were initialized uniformly between -0 01 and 0 01, and the model was trained until the validation perplexity did not improve for 10 epochs  SENTENCE_END
SENTENCE_START After training, the language model achieved a perplexity of 45 80  SENTENCE_END
SENTENCE_START The validation set was a random selection of 0 1% of the corpus  SENTENCE_END
SENTENCE_START The model was used to score partial translations during the decoding process, which generally leads to higher gains in BLEU score than n-best list rescoring (Vaswani et al , 2013)  SENTENCE_END
SENTENCE_START To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stack-search performed by the decoder  SENTENCE_END
SENTENCE_START Only when the buffer is full, or a stack is about to be pruned, the n-grams are scored by the CSLM  SENTENCE_END
SENTENCE_START This allows us to perform fast matrix-matrix multiplication on GPU using Theano (Bergstra et al , 2010; Bastien et al , 2012)  SENTENCE_END
SENTENCE_START RNN Scores (log) TM Scores (log) Figure 3: The visualization of phrase pairs accord- ing to their scores (log-probabilities) by the RNN Encoder-Decoder and the translation model  SENTENCE_END
SENTENCE_START 4 2 Quantitative Analysis We tried the following combinations: 1  SENTENCE_END
SENTENCE_START Baseline configuration 2  SENTENCE_END
SENTENCE_START Baseline + RNN 3  SENTENCE_END
SENTENCE_START Baseline + CSLM + RNN 4  SENTENCE_END
SENTENCE_START Baseline + CSLM + RNN + Word penalty Source Translation Model RNN Encoder-Decoder Table 2: The top scoring target phrases for a small set of source phrases according to the translation model (direct translation probability) and by the RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START Source phrases were randomly selected from phrases with 4 or more words  SENTENCE_END
SENTENCE_START denotes an incomplete (partial) character  SENTENCE_END
SENTENCE_START r is a Cyrillic letter ghe  SENTENCE_END
SENTENCE_START The results are presented in Table 1  SENTENCE_END
SENTENCE_START As expected, adding features computed by neural networks consistently improves the performance over the baseline performance  SENTENCE_END
SENTENCE_START The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START This suggests that the contributions of the CSLM and the RNN Encoder-Decoder are not too correlated and that one can expect better results by improving each method independently  SENTENCE_END
SENTENCE_START Furthermore, we tried penalizing the number of words that are unknown to the neural networks (i e  SENTENCE_END
SENTENCE_START words which are not in the short-list)  SENTENCE_END
SENTENCE_START We do so by simply adding the number of unknown words as an additional feature the log-linear model in Eq  SENTENCE_END
SENTENCE_START (9) 3 However, in this case we 3 To understand the effect of the penalty, consider the set of all words in the 15,000 large shortlist, SL  SENTENCE_END
SENTENCE_START All words xi / SL are replaced by a special token [UNK] before being scored by the neural networks  SENTENCE_END
SENTENCE_START Hence, the conditional probability of any xi t / SL is actually given by the model as p (xt = [UNK] | x<t) = p (xt / SL | x<t) = x j t / SL p xj t | x<t  p xi t | x<t , where x<t is a shorthand notation for xt-1,   SENTENCE_END
SENTENCE_START , x1  SENTENCE_END
SENTENCE_START were not able to achieve better performance on the test set, but only on the development set  SENTENCE_END
SENTENCE_START 4 3 Qualitative Analysis In order to understand where the performance improvement comes from, we analyze the phrase pair scores computed by the RNN Encoder-Decoder against the corresponding p(f | e) from the translation model  SENTENCE_END
SENTENCE_START Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus, we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases  SENTENCE_END
SENTENCE_START Also, as we mentioned earlier in Sec  SENTENCE_END
SENTENCE_START 3 1, we further expect the RNN Encoder-Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus  SENTENCE_END
SENTENCE_START We focus on those pairs whose source phrase is long (more than 3 words per source phrase) and As a result, the probability of words not in the shortlist is always overestimated  SENTENCE_END
SENTENCE_START It is possible to address this issue by backing off to an existing model that contain non-shortlisted words (see (Schwenk, 2007)) In this paper, however, we opt for introducing a word penalty instead, which counteracts the word probability overestimation  SENTENCE_END
SENTENCE_START Source Samples from RNN Encoder-Decoder Table 3: Samples generated from the RNN Encoder-Decoder for each source phrase used in Table 2  SENTENCE_END
SENTENCE_START We show the top-5 target phrases out of 50 samples  SENTENCE_END
SENTENCE_START They are sorted by the RNN Encoder-Decoder scores  SENTENCE_END
SENTENCE_START Figure 4: 2-D embedding of the learned word representation  SENTENCE_END
SENTENCE_START The left one shows the full embedding space, while the right one shows a zoomed-in view of one region (color-coded)  SENTENCE_END
SENTENCE_START For more plots, see the supplementary material  SENTENCE_END
SENTENCE_START frequent  SENTENCE_END
SENTENCE_START For each such source phrase, we look at the target phrases that have been scored high either by the translation probability p(f | e) or by the RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START Similarly, we perform the same procedure with those pairs whose source phrase is long but rare in the corpus  SENTENCE_END
SENTENCE_START Table 2 lists the top-3 target phrases per source phrase favored either by the translation model or by the RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START The source phrases were randomly chosen among long ones having more than 4 or 5 words  SENTENCE_END
SENTENCE_START In most cases, the choices of the target phrases by the RNN Encoder-Decoder are closer to actual or literal translations  SENTENCE_END
SENTENCE_START We can observe that the RNN Encoder-Decoder prefers shorter phrases in general  SENTENCE_END
SENTENCE_START Interestingly, many phrase pairs were scored similarly by both the translation model and the RNN Encoder-Decoder, but there were as many other phrase pairs that were scored radically different (see Fig  SENTENCE_END
SENTENCE_START 3)  SENTENCE_END
SENTENCE_START This could arise from the proposed approach of training the RNN Encoder-Decoder on a set of unique phrase pairs, discouraging the RNN Encoder-Decoder from learning simply the frequencies of the phrase pairs from the corpus, as explained earlier  SENTENCE_END
SENTENCE_START Furthermore, in Table 3, we show for each of the source phrases in Table 2, the generated samples from the RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START For each source phrase, we generated 50 samples and show the top-five phrases accordingly to their scores  SENTENCE_END
SENTENCE_START We can see that the RNN Encoder-Decoder is able to propose well-formed target phrases without looking at the actual phrase table  SENTENCE_END
SENTENCE_START Importantly, the generated phrases do not overlap completely with the target phrases from the phrase table  SENTENCE_END
SENTENCE_START This encourages us to further investigate the possibility of replacing the whole or a part of the phrase table Figure 5: 2-D embedding of the learned phrase representation  SENTENCE_END
SENTENCE_START The top left one shows the full representation space (5000 randomly selected points), while the other three figures show the zoomed-in view of specific regions (color-coded)  SENTENCE_END
SENTENCE_START with the proposed RNN Encoder-Decoder in the future  SENTENCE_END
SENTENCE_START 4 4 Word and Phrase Representations Since the proposed RNN Encoder-Decoder is not specifically designed only for the task of machine translation, here we briefly look at the properties of the trained model  SENTENCE_END
SENTENCE_START It has been known for some time that continuous space language models using neural networks are able to learn semantically meaningful embeddings (See, e g , (Bengio et al , 2003; Mikolov et al , 2013))  SENTENCE_END
SENTENCE_START Since the proposed RNN Encoder-Decoder also projects to and maps back from a sequence of words into a continuous space vector, we expect to see a similar property with the proposed model as well  SENTENCE_END
SENTENCE_START The left plot in Fig  SENTENCE_END
SENTENCE_START 4 shows the 2-D embedding of the words using the word embedding matrix learned by the RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START The projection was done by the recently proposed Barnes-Hut-SNE (van der Maaten, 2013)  SENTENCE_END
SENTENCE_START We can clearly see that semantically similar words are clustered with each other (see the zoomed-in plots in Fig  SENTENCE_END
SENTENCE_START 4)  SENTENCE_END
SENTENCE_START The proposed RNN Encoder-Decoder naturally generates a continuous-space representation of a phrase  SENTENCE_END
SENTENCE_START The representation (c in Fig  SENTENCE_END
SENTENCE_START 1) in this case is a 1000-dimensional vector  SENTENCE_END
SENTENCE_START Similarly to the word representations, we visualize the representations of the phrases that consists of four or more words using the Barnes-Hut-SNE in Fig  SENTENCE_END
SENTENCE_START From the visualization, it is clear that the RNN Encoder-Decoder captures both semantic and syntactic structures of the phrases  SENTENCE_END
SENTENCE_START For instance, in the bottom-left plot, most of the phrases are about the duration of time, while those phrases that are syntactically similar are clustered together  SENTENCE_END
SENTENCE_START The bottom-right plot shows the cluster of phrases that are semantically similar (countries or regions)  SENTENCE_END
SENTENCE_START On the other hand, the top-right plot shows the phrases that are syntactically similar  SENTENCE_END
SENTENCE_START 5 Conclusion In this paper, we proposed a new neural network architecture, called an RNN Encoder-Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length  SENTENCE_END
SENTENCE_START The proposed RNN Encoder-Decoder is able to either score a pair of sequences (in terms of a conditional probability) or generate a target sequence given a source sequence  SENTENCE_END
SENTENCE_START Along with the new architecture, we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence  SENTENCE_END
SENTENCE_START We evaluated the proposed model with the task of statistical machine translation, where we used the RNN Encoder-Decoder to score each phrase pair in the phrase table  SENTENCE_END
SENTENCE_START Qualitatively, we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder-Decoder is able to propose well-formed target phrases  SENTENCE_END
SENTENCE_START The scores by the RNN Encoder-Decoder were found to improve the overall translation performance in terms of BLEU scores  SENTENCE_END
SENTENCE_START Also, we found that the contribution by the RNN Encoder-Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system, so that we can improve further the performance by using, for instance, the RNN Encoder-Decoder and the neural net language model together  SENTENCE_END
SENTENCE_START Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i e  SENTENCE_END
SENTENCE_START at the word level as well as phrase level  SENTENCE_END
SENTENCE_START This suggests that there may be more natural language related applications that may benefit from the proposed RNN Encoder-Decoder  SENTENCE_END
SENTENCE_START The proposed architecture has large potential for further improvement and analysis  SENTENCE_END
SENTENCE_START One approach that was not investigated here is to replace the whole, or a part of the phrase table by letting the RNN Encoder-Decoder propose target phrases  SENTENCE_END
SENTENCE_START Also, noting that the proposed model is not limited to being used with written language, it will be an important future research to apply the proposed architecture to other applications such as speech transcription  SENTENCE_END
